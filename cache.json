{"2023-03-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.08127v1","updated":"2023-03-14T17:57:06Z","published":"2023-03-14T17:57:06Z","title":"CB2: Collaborative Natural Language Interaction Research Platform","summary":"  CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n","authors":["Jacob Sharf","Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2303.08127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08117v1","updated":"2023-03-14T17:49:50Z","published":"2023-03-14T17:49:50Z","title":"Do Transformers Parse while Predicting the Masked Word?","summary":"  Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.\n","authors":["Haoyu Zhao","Abhishek Panigrahi","Rong Ge","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2303.08117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08114v1","updated":"2023-03-14T17:47:25Z","published":"2023-03-14T17:47:25Z","title":"Simfluence: Modeling the Influence of Individual Training Examples by\n  Simulating Training Runs","summary":"  Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.\n","authors":["Kelvin Guu","Albert Webson","Ellie Pavlick","Lucas Dixon","Ian Tenney","Tolga Bolukbasi"],"pdf_url":"https://arxiv.org/pdf/2303.08114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07247v2","updated":"2023-03-14T17:40:21Z","published":"2023-03-13T16:20:33Z","title":"Are Models Trained on Indian Legal Data Fair?","summary":"  Recent advances and applications of language technology and artificial\nintelligence have enabled much success across multiple domains like law,\nmedical and mental health. AI-based Language Models, like Judgement Prediction,\nhave recently been proposed for the legal sector. However, these models are\nstrife with encoded social biases picked up from the training data. While bias\nand fairness have been studied across NLP, most studies primarily locate\nthemselves within a Western context. In this work, we present an initial\ninvestigation of fairness from the Indian perspective in the legal domain. We\nhighlight the propagation of learnt algorithmic biases in the bail prediction\ntask for models trained on Hindi legal documents. We evaluate the fairness gap\nusing demographic parity and show that a decision tree model trained for the\nbail prediction task has an overall fairness disparity of 0.237 between input\nfeatures associated with Hindus and Muslims. Additionally, we highlight the\nneed for further research and studies in the avenues of fairness/bias in\napplying AI in the legal sector with a specific focus on the Indian context.\n","authors":["Sahil Girhepuje","Anmol Goel","Gokul S Krishnan","Shreya Goyal","Satyendra Pandey","Ponnurangam Kumaraguru","Balaraman Ravindran"],"pdf_url":"https://arxiv.org/pdf/2303.07247v2.pdf","comment":"Presented at the Symposium on AI and Law (SAIL) 2023"},{"id":"http://arxiv.org/abs/2208.11175v2","updated":"2023-03-14T17:06:58Z","published":"2022-08-23T20:03:27Z","title":"Ordinal analysis of lexical patterns","summary":"  Words are fundamental linguistic units that connect thoughts and things\nthrough meaning. However, words do not appear independently in a text sequence.\nThe existence of syntactic rules induces correlations among neighboring words.\nUsing an ordinal pattern approach, we present an analysis of lexical\nstatistical connections for 11 major languages. We find that the diverse\nmanners that languages utilize to express word relations give rise to unique\npattern structural distributions. Furthermore, fluctuations of these pattern\ndistributions for a given language can allow us to determine both the\nhistorical period when the text was written and its author. Taken together, our\nresults emphasize the relevance of ordinal time series analysis in linguistic\ntypology, historical linguistics and stylometry.\n","authors":["David Sanchez","Luciano Zunino","Juan De Gregorio","Raul Toral","Claudio Mirasso"],"pdf_url":"https://arxiv.org/pdf/2208.11175v2.pdf","comment":"9 pages, 12 figures, 2 tables; v2: the section on universality has\n  been removed because previous results were affected by spurious correlations.\n  Published version"},{"id":"http://arxiv.org/abs/2303.07142v2","updated":"2023-03-14T17:01:59Z","published":"2023-03-13T14:09:53Z","title":"Large Language Models in the Workplace: A Case Study on Prompt\n  Engineering for Job Type Classification","summary":"  This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n","authors":["Benjamin Clavié","Alexandru Ciceu","Frederick Naylor","Guillaume Soulié","Thomas Brightwell"],"pdf_url":"https://arxiv.org/pdf/2303.07142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08044v1","updated":"2023-03-14T16:23:23Z","published":"2023-03-14T16:23:23Z","title":"Happy-GLL: modular, reusable and complete top-down parsers for\n  parameterized nonterminals","summary":"  Parser generators and parser combinator libraries are the most popular tools\nfor producing parsers. Parser combinators use the host language to provide\nreusable components in the form of higher-order functions with parsers as\nparameters. Very few parser generators support this kind of reuse through\nabstraction and even fewer generate parsers that are as modular and reusable as\nthe parts of the grammar for which they are produced. This paper presents a\nstrategy for generating modular, reusable and complete top-down parsers from\nsyntax descriptions with parameterized nonterminals, based on the FUN-GLL\nvariant of the GLL algorithm.\n  The strategy is discussed and demonstrated as a novel back-end for the Happy\nparser generator. Happy grammars can contain `parameterized nonterminals' in\nwhich parameters abstract over grammar symbols, granting an abstraction\nmechanism to define reusable grammar operators. However, the existing Happy\nback-ends do not deliver on the full potential of parameterized nonterminals as\nparameterized nonterminals cannot be reused across grammars. Moreover, the\nparser generation process may fail to terminate or may result in exponentially\nlarge parsers generated in an exponential amount of time.\n  The GLL back-end presented in this paper implements parameterized\nnonterminals successfully by generating higher-order functions that resemble\nparser combinators, inheriting all the advantages of top-down parsing. The\nback-end is capable of generating parsers for the full class of context-free\ngrammars, generates parsers in linear time and generates parsers that find all\nderivations of the input string. To our knowledge, the presented GLL back-end\nmakes Happy the first parser generator that combines all these features.\n  This paper describes the translation procedure of the GLL back-end and\ncompares it to the LALR and GLR back-ends of Happy in several experiments.\n","authors":["L. Thomas van Binsbergen","Damian Frolich"],"pdf_url":"https://arxiv.org/pdf/2303.08044v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.08038v1","updated":"2023-03-14T16:17:55Z","published":"2023-03-14T16:17:55Z","title":"Progress Note Understanding -- Assessment and Plan Reasoning: Overview\n  of the 2022 N2C2 Track 3 Shared Task","summary":"  Daily progress notes are common types in the electronic health record (EHR)\nwhere healthcare providers document the patient's daily progress and treatment\nplans. The EHR is designed to document all the care provided to patients, but\nit also enables note bloat with extraneous information that distracts from the\ndiagnoses and treatment plans. Applications of natural language processing\n(NLP) in the EHR is a growing field with the majority of methods in information\nextraction. Few tasks use NLP methods for downstream diagnostic decision\nsupport. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3:\nProgress Note Understanding - Assessment and Plan Reasoning as one step towards\na new suite of tasks. The Assessment and Plan Reasoning task focuses on the\nmost critical components of progress notes, Assessment and Plan subsections\nwhere health problems and diagnoses are contained. The goal of the task was to\ndevelop and evaluate NLP systems that automatically predict causal relations\nbetween the overall status of the patient contained in the Assessment section\nand its relation to each component of the Plan section which contains the\ndiagnoses and treatment plans. The goal of the task was to identify and\nprioritize diagnoses as the first steps in diagnostic decision support to find\nthe most relevant information in long documents like daily progress notes. We\npresent the results of 2022 n2c2 Track 3 and provide a description of the data,\nevaluation, participation and system performance.\n","authors":["Yanjun Gao","Dmitriy Dligach","Timothy Miller","Matthew M Churpek","Ozlem Uzuner","Majid Afshar"],"pdf_url":"https://arxiv.org/pdf/2303.08038v1.pdf","comment":"To appear in Journal of Biomedical Informatics"},{"id":"http://arxiv.org/abs/2211.16198v2","updated":"2023-03-14T16:13:03Z","published":"2022-11-28T16:48:41Z","title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n","authors":["Vishaal Udandarao","Ankush Gupta","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2211.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08032v1","updated":"2023-03-14T16:11:47Z","published":"2023-03-14T16:11:47Z","title":"BODEGA: Benchmark for Adversarial Example Generation in Credibility\n  Assessment","summary":"  Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we introduce BODEGA: a benchmark for testing both victim models\nand attack methods on four misinformation detection tasks in an evaluation\nframework designed to simulate real use-cases of content moderation. We also\nsystematically test the robustness of popular text classifiers against\navailable attacking techniques and discover that, indeed, in some cases barely\nsignificant changes in input text can mislead the models. We openly share the\nBODEGA code and data in hope of enhancing the comparability and replicability\nof further research in this area.\n","authors":["Piotr Przybyła","Alexander Shvets","Horacio Saggion"],"pdf_url":"https://arxiv.org/pdf/2303.08032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08021v1","updated":"2023-03-14T16:04:13Z","published":"2023-03-14T16:04:13Z","title":"Optimizing Deep Learning Model Parameters with the Bees Algorithm for\n  Improved Medical Text Classification","summary":"  This paper introduces a novel mechanism to obtain the optimal parameters of a\ndeep learning model using the Bees Algorithm, which is a recent promising swarm\nintelligence algorithm. The optimization problem is to maximize the accuracy of\nclassifying ailments based on medical text given the initial hyper-parameters\nto be adjusted throughout a definite number of iterations. Experiments included\ntwo different datasets: English and Arabic. The highest accuracy achieved is\n99.63% on the English dataset using Long Short-Term Memory (LSTM) along with\nthe Bees Algorithm, and 88% on the Arabic dataset using AraBERT.\n","authors":["Mai A. Shaaban","Mariam Kashkash","Maryam Alghfeli","Adham Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2303.08021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07992v1","updated":"2023-03-14T15:46:28Z","published":"2023-03-14T15:46:28Z","title":"Evaluation of ChatGPT as a Question Answering System for Answering\n  Complex Questions","summary":"  ChatGPT is a powerful large language model (LLM) that has made remarkable\nprogress in natural language understanding. Nevertheless, the performance and\nlimitations of the model still need to be extensively evaluated. As ChatGPT\ncovers resources such as Wikipedia and supports natural language question\nanswering, it has garnered attention as a potential replacement for traditional\nknowledge based question answering (KBQA) models. Complex question answering is\na challenge task of KBQA, which comprehensively tests the ability of models in\nsemantic parsing and reasoning. To assess the performance of ChatGPT as a\nquestion answering system (QAS) using its own knowledge, we present a framework\nthat evaluates its ability to answer complex questions. Our approach involves\ncategorizing the potential features of complex questions and describing each\ntest question with multiple labels to identify combinatorial reasoning.\nFollowing the black-box testing specifications of CheckList proposed by Ribeiro\net.al, we develop an evaluation method to measure the functionality and\nreliability of ChatGPT in reasoning for answering complex questions. We use the\nproposed framework to evaluate the performance of ChatGPT in question answering\non 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual\ndatasets, with a total of approximately 190,000 test cases. We compare the\nevaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common\nlong-term problems in LLMs. The dataset and code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.\n","authors":["Yiming Tan","Dehai Min","Yu Li","Wenbo Li","Nan Hu","Yongrui Chen","Guilin Qi"],"pdf_url":"https://arxiv.org/pdf/2303.07992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07991v1","updated":"2023-03-14T15:45:35Z","published":"2023-03-14T15:45:35Z","title":"Finding the Needle in a Haystack: Unsupervised Rationale Extraction from\n  Long Text Classifiers","summary":"  Long-sequence transformers are designed to improve the representation of\nlonger texts by language models and their performance on downstream\ndocument-level tasks. However, not much is understood about the quality of\ntoken-level predictions in long-form models. We investigate the performance of\nsuch architectures in the context of document classification with unsupervised\nrationale extraction. We find standard soft attention methods to perform\nsignificantly worse when combined with the Longformer language model. We\npropose a compositional soft attention architecture that applies RoBERTa\nsentence-wise to extract plausible rationales at the token-level. We find this\nmethod to significantly outperform Longformer-driven baselines on sentiment\nclassification datasets, while also exhibiting significantly lower runtimes.\n","authors":["Kamil Bujel","Andrew Caines","Helen Yannakoudakis","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2303.07991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12278v2","updated":"2023-03-14T15:27:08Z","published":"2022-09-25T17:54:59Z","title":"Neural inhibition during speech planning contributes to contrastive\n  hyperarticulation","summary":"  Previous work has demonstrated that words are hyperarticulated on dimensions\nof speech that differentiate them from a minimal pair competitor. This\nphenomenon has been termed contrastive hyperarticulation (CH). We present a\ndynamic neural field (DNF) model of voice onset time (VOT) planning that\nderives CH from an inhibitory influence of the minimal pair competitor during\nplanning. We test some predictions of the model with a novel experiment\ninvestigating CH of voiceless stop consonant VOT in pseudowords. The results\ndemonstrate a CH effect in pseudowords, consistent with a basis for the effect\nin the real-time planning and production of speech. The scope and magnitude of\nCH in pseudowords was reduced compared to CH in real words, consistent with a\nrole for interactive activation between lexical and phonological levels of\nplanning. We discuss the potential of our model to unify an apparently\ndisparate set of phenomena, from CH to phonological neighborhood effects to\nphonetic trace effects in speech errors.\n","authors":["Michael C. Stern","Jason A. Shaw"],"pdf_url":"https://arxiv.org/pdf/2209.12278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07971v1","updated":"2023-03-14T15:24:05Z","published":"2023-03-14T15:24:05Z","title":"A Theory of Emergent In-Context Learning as Implicit Structure Induction","summary":"  Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.\n","authors":["Michael Hahn","Navin Goyal"],"pdf_url":"https://arxiv.org/pdf/2303.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07271v4","updated":"2023-03-14T14:53:37Z","published":"2022-06-15T03:18:56Z","title":"Human heuristics for AI-generated language are flawed","summary":"  Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems suggest words, complete\nsentences, or produce entire conversations. AI-generated language is often not\nidentified as such but presented as language written by humans, raising\nconcerns about novel forms of deception and manipulation. Here, we study how\nhumans discern whether verbal self-presentations, one of the most personal and\nconsequential forms of language, were generated by AI. In six experiments,\nparticipants (N = 4,600) were unable to detect self-presentations generated by\nstate-of-the-art AI language models in professional, hospitality, and dating\ncontexts. A computational analysis of language features shows that human\njudgments of AI-generated language are hindered by intuitive but flawed\nheuristics such as associating first-person pronouns, use of contractions, or\nfamily topics with human-written language. We experimentally demonstrate that\nthese heuristics make human judgment of AI-generated language predictable and\nmanipulable, allowing AI systems to produce text perceived as \"more human than\nhuman.\" We discuss solutions, such as AI accents, to reduce the deceptive\npotential of language generated by AI, limiting the subversion of human\nintuition.\n","authors":["Maurice Jakesch","Jeffrey Hancock","Mor Naaman"],"pdf_url":"https://arxiv.org/pdf/2206.07271v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03053v2","updated":"2023-03-14T14:31:05Z","published":"2022-11-06T07:53:19Z","title":"Suffix Retrieval-Augmented Language Modeling","summary":"  Causal language modeling (LM) uses word history to predict the next word.\nBERT, on the other hand, makes use of bi-directional word information in a\nsentence to predict words at masked positions. While BERT is effective in\nsequence encoding, it is non-causal by nature and is not designed for sequence\ngeneration. In this paper, we propose a novel language model, SUffix\nREtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual\neffect in an autoregressive manner. SUREALM employs an embedding retriever to\nsearch for training sentences in a data store that share similar word history\nduring sequence generation. In particular, the suffix portions of the retrieved\nsentences mimick the \"future\" context. We evaluated our proposed model on the\nDSTC9 spoken dialogue corpus and showed promising word perplexity reduction on\nthe validation and test set compared to competitive baselines.\n","authors":["Zecheng Wang","Yik-Cheung Tam"],"pdf_url":"https://arxiv.org/pdf/2211.03053v2.pdf","comment":"5 pages, 1 figure. Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2110.08895v4","updated":"2023-03-14T14:29:58Z","published":"2021-10-17T19:03:51Z","title":"DECAR: Deep Clustering for learning general-purpose Audio\n  Representations","summary":"  We introduce DECAR, a self-supervised pre-training approach for learning\ngeneral-purpose audio representations. Our system is based on clustering: it\nutilizes an offline clustering step to provide target labels that act as\npseudo-labels for solving a prediction task. We develop on top of recent\nadvances in self-supervised learning for computer vision and design a\nlightweight, easy-to-use self-supervised pre-training scheme. We pre-train\nDECAR embeddings on a balanced subset of the large-scale Audioset dataset and\ntransfer those representations to 9 downstream classification tasks, including\nspeech, music, animal sounds, and acoustic scenes. Furthermore, we conduct\nablation studies identifying key design choices and also make all our code and\npre-trained models publicly available.\n","authors":["Sreyan Ghosh","Sandesh V Katta","Ashish Seth","S. Umesh"],"pdf_url":"https://arxiv.org/pdf/2110.08895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07924v1","updated":"2023-03-14T14:10:16Z","published":"2023-03-14T14:10:16Z","title":"Improving Accented Speech Recognition with Multi-Domain Training","summary":"  Thanks to the rise of self-supervised learning, automatic speech recognition\n(ASR) systems now achieve near-human performance on a wide variety of datasets.\nHowever, they still lack generalization capability and are not robust to domain\nshifts like accent variations. In this work, we use speech audio representing\nfour different French accents to create fine-tuning datasets that improve the\nrobustness of pre-trained ASR models. By incorporating various accents in the\ntraining set, we obtain both in-domain and out-of-domain improvements. Our\nnumerical experiments show that we can reduce error rates by up to 25%\n(relative) on African and Belgian accents compared to single-domain training\nwhile keeping a good performance on standard French.\n","authors":["Lucas Maison","Yannick Estève"],"pdf_url":"https://arxiv.org/pdf/2303.07924v1.pdf","comment":"5 pages, 2 figures. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07914v1","updated":"2023-03-14T13:56:36Z","published":"2023-03-14T13:56:36Z","title":"Adapting Offline Speech Translation Models for Streaming with\n  Future-Aware Distillation and Inference","summary":"  A popular approach to streaming speech translation is to employ a single\noffline model with a \\textit{wait-$k$} policy to support different latency\nrequirements, which is simpler than training multiple online models with\ndifferent latency constraints. However, there is a mismatch problem in using a\nmodel trained with complete utterances for streaming inference with partial\ninput. We demonstrate that speech representations extracted at the end of a\nstreaming input are significantly different from those extracted from a\ncomplete utterance. To address this issue, we propose a new approach called\nFuture-Aware Streaming Translation (FAST) that adapts an offline ST model for\nstreaming input. FAST includes a Future-Aware Inference (FAI) strategy that\nincorporates future context through a trainable masked embedding, and a\nFuture-Aware Distillation (FAD) framework that transfers future context from an\napproximation of full speech to streaming input. Our experiments on the MuST-C\nEnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs\nbetween translation quality and latency than strong baselines. Extensive\nanalyses suggest that our methods effectively alleviate the aforementioned\nmismatch problem between offline training and online inference.\n","authors":["Biao Fu","Kai Fan","Minpeng Liao","Zhongqiang Huang","Boxing Chen","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2303.07914v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2302.08399v5","updated":"2023-03-14T13:47:26Z","published":"2023-02-16T16:18:03Z","title":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks","summary":"  Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n","authors":["Tomer Ullman"],"pdf_url":"https://arxiv.org/pdf/2302.08399v5.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07895v1","updated":"2023-03-14T13:28:39Z","published":"2023-03-14T13:28:39Z","title":"The Learnability of In-Context Learning","summary":"  In-context learning is a surprising and important phenomenon that emerged\nwhen modern language models were scaled to billions of learned parameters.\nWithout modifying a large language model's weights, it can be tuned to perform\nvarious downstream natural language tasks simply by including concatenated\ntraining examples of these tasks in its input. Though disruptive for many\npractical applications of large language models, this emergent learning\nparadigm is not well understood from a theoretical perspective. In this paper,\nwe propose a first-of-its-kind PAC based framework for in-context learnability,\nand use it to provide the first finite sample complexity results for the\nin-context learning setup. Our framework includes an initial pretraining phase,\nwhich fits a function to the pretraining distribution, and then a second\nin-context learning phase, which keeps this function constant and concatenates\ntraining examples of the downstream task in its input. We use our framework in\norder to prove that, under mild assumptions, when the pretraining distribution\nis a mixture of latent tasks (a model often considered for natural language\npretraining), these tasks can be efficiently learned via in-context learning,\neven though the model's weights are unchanged and the input significantly\ndiverges from the pretraining distribution. Our theoretical analysis reveals\nthat in this setting, in-context learning is more about identifying the task\nthan about learning it, a result which is in line with a series of recent\nempirical findings. We hope that the in-context learnability framework\npresented in this paper will facilitate future progress towards a deeper\nunderstanding of this important new learning paradigm.\n","authors":["Noam Wies","Yoav Levine","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2303.07895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07865v1","updated":"2023-03-14T12:56:47Z","published":"2023-03-14T12:56:47Z","title":"Geolocation Predicting of Tweets Using BERT-Based Models","summary":"  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context.\n","authors":["Kateryna Lutsai","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2303.07865v1.pdf","comment":"27 pages, 6 tables, 7 figures"},{"id":"http://arxiv.org/abs/2301.00656v2","updated":"2023-03-14T12:23:33Z","published":"2022-12-12T05:55:07Z","title":"TriNet: stabilizing self-supervised learning from complete or slow\n  collapse on ASR","summary":"  Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. TriNet learns the SSL latent embedding space and\nincorporates it to a higher level space for predicting pseudo target vectors\ngenerated by a frozen teacher. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 6.06% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.\n","authors":["Lixin Cao","Jun Wang","Ben Yang","Dan Su","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2301.00656v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07833v1","updated":"2023-03-14T12:15:52Z","published":"2023-03-14T12:15:52Z","title":"X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue\n  Generation","summary":"  In multi-turn dialogue generation, responses are not only related to the\ntopic and background of the context but also related to words and phrases in\nthe sentences of the context. However, currently widely used hierarchical\ndialog models solely rely on context representations from the utterance-level\nencoder, ignoring the sentence representations output by the word-level\nencoder. This inevitably results in a loss of information while decoding and\ngenerating. In this paper, we propose a new dialog model X-ReCoSa to tackle\nthis problem which aggregates multi-scale context information for hierarchical\ndialog models. Specifically, we divide the generation decoder into upper and\nlower parts, namely the intention part and the generation part. Firstly, the\nintention part takes context representations as input to generate the intention\nof the response. Then the generation part generates words depending on sentence\nrepresentations. Therefore, the hierarchical information has been fused into\nresponse generation. we conduct experiments on the English dataset DailyDialog.\nExperimental results exhibit that our method outperforms baseline models on\nboth automatic metric-based and human-based evaluations.\n","authors":["Danqin Wu"],"pdf_url":"https://arxiv.org/pdf/2303.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12529v2","updated":"2023-03-14T11:09:19Z","published":"2023-02-24T09:29:40Z","title":"Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph\n  Question Answering","summary":"  Knowledge graphs (KGs) have received increasing attention due to its wide\napplications on natural language processing. However, its use case on temporal\nquestion answering (QA) has not been well-explored. Most of existing methods\nare developed based on pre-trained language models, which might not be capable\nto learn \\emph{temporal-specific} presentations of entities in terms of\ntemporal KGQA task. To alleviate this problem, we propose a novel\n\\textbf{T}ime-aware \\textbf{M}ultiway \\textbf{A}daptive (\\textbf{TMA}) fusion\nnetwork. Inspired by the step-by-step reasoning behavior of humans. For each\ngiven question, TMA first extracts the relevant concepts from the KG, and then\nfeeds them into a multiway adaptive module to produce a\n\\emph{temporal-specific} representation of the question. This representation\ncan be incorporated with the pre-trained KG embedding to generate the final\nprediction. Empirical results verify that the proposed model achieves better\nperformance than the state-of-the-art models in the benchmark dataset. Notably,\nthe Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex\nquestions are absolutely improved by 24\\% and 10\\% compared to the\nbest-performing baseline. Furthermore, we also show that TMA employing an\nadaptive fusion mechanism can provide interpretability by analyzing the\nproportion of information in question representations.\n","authors":["Yonghao Liu","Di Liang","Fang Fang","Sirui Wang","Wei Wu","Rui Jiang"],"pdf_url":"https://arxiv.org/pdf/2302.12529v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.16264v2","updated":"2023-03-14T11:08:18Z","published":"2022-10-28T16:52:48Z","title":"Efficient Speech Translation with Dynamic Latent Perceivers","summary":"  Transformers have been the dominant architecture for Speech Translation in\nrecent years, achieving significant improvements in translation quality. Since\nspeech signals are longer than their textual counterparts, and due to the\nquadratic complexity of the Transformer, a down-sampling step is essential for\nits adoption in Speech Translation. Instead, in this research, we propose to\nease the complexity by using a Perceiver encoder to map the speech inputs to a\nfixed-length latent representation. Furthermore, we introduce a novel way of\ntraining Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent\nspaces without any additional computational overhead. Speech-to-Text Perceivers\nwith DLA can match the performance of Transformer baselines across three\nlanguage pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to\nDLA at inference, and can be flexibly deployed with various computational\nbudgets, without significant drops in translation quality.\n","authors":["Ioannis Tsiamas","Gerard I. Gállego","José A. R. Fonollosa","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2210.16264v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2301.00503v3","updated":"2023-03-14T11:01:26Z","published":"2023-01-02T02:10:18Z","title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay","summary":"  This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n","authors":["Yacheng He","Qianghuai Jia","Lin Yuan","Ruopeng Li","Yixin Ou","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00503v3.pdf","comment":"Accepted by WWW 2023 poster"},{"id":"http://arxiv.org/abs/2302.12530v2","updated":"2023-03-14T10:51:55Z","published":"2023-02-24T09:29:55Z","title":"Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts","summary":"  Transformer-based pre-trained models have achieved great improvements in\nsemantic matching. However, existing models still suffer from insufficient\nability to capture subtle differences. The modification, addition and deletion\nof words in sentence pairs may make it difficult for the model to predict their\nrelationship. To alleviate this problem, we propose a novel Dual Path Modeling\nFramework to enhance the model's ability to perceive subtle differences in\nsentence pairs by separately modeling affinity and difference semantics. Based\non dual-path modeling framework we design the Dual Path Modeling Network\n(DPM-Net) to recognize semantic relations. And we conduct extensive experiments\non 10 well-studied semantic matching and robustness test datasets, and the\nexperimental results show that our proposed method achieves consistent\nimprovements over baselines.\n","authors":["Chao Xue","Di Liang","Sirui Wang","Wei Wu","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.12530v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2202.02113v7","updated":"2023-03-14T10:33:15Z","published":"2022-02-04T12:52:32Z","title":"From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer","summary":"  Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n","authors":["Xin Xie","Ningyu Zhang","Zhoubo Li","Shumin Deng","Hui Chen","Feiyu Xiong","Mosha Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2202.02113v7.pdf","comment":"Accepted by WWW 2022 Poster"},{"id":"http://arxiv.org/abs/2110.03501v3","updated":"2023-03-14T10:30:51Z","published":"2021-10-07T14:37:06Z","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!","summary":"  Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npropose the generalizability of our pretrained language model from Anna\nKarenina Principle (AKP). We pretrain our model with different pairs of\nlanguage translations. Our results show language bias in solving symbolic\nmathematics tasks. Finally, we study the robustness of the fine-tuned model on\nsymbolic math tasks against distribution shift, and our approach generalizes\nbetter in distribution shift scenarios for the function integration.\n","authors":["Kimia Noorbakhsh","Modar Sulaiman","Mahdi Sharifi","Kallol Roy","Pooyan Jamshidi"],"pdf_url":"https://arxiv.org/pdf/2110.03501v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10852v5","updated":"2023-03-14T10:28:49Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Ningyu Zhang","Qiang Chen","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2205.10852v5.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2107.12220v2","updated":"2023-03-14T10:05:43Z","published":"2021-07-26T13:56:37Z","title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought","summary":"  When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2107.12220v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.07740v1","updated":"2023-03-14T09:36:42Z","published":"2023-03-14T09:36:42Z","title":"Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening","summary":"  Under the flourishing development in performance, current image-text\nretrieval methods suffer from $N$-related time complexity, which hinders their\napplication in practice. Targeting at efficiency improvement, this paper\npresents a simple and effective keyword-guided pre-screening framework for the\nimage-text retrieval. Specifically, we convert the image and text data into the\nkeywords and perform the keyword matching across modalities to exclude a large\nnumber of irrelevant gallery samples prior to the retrieval network. For the\nkeyword prediction, we transfer it into a multi-label classification problem\nand propose a multi-task learning scheme by appending the multi-label\nclassifiers to the image-text retrieval network to achieve a lightweight and\nhigh-performance keyword prediction. For the keyword matching, we introduce the\ninverted index in the search engine and create a win-win situation on both time\nand space complexities for the pre-screening. Extensive experiments on two\nwidely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of\nthe proposed framework. The proposed framework equipped with only two embedding\nlayers achieves $O(1)$ querying time complexity, while improving the retrieval\nefficiency and keeping its performance, when applied prior to the common\nimage-text retrieval methods. Our code will be released.\n","authors":["Min Cao","Yang Bai","Jingyao Wang","Ziqiang Cao","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07740v1.pdf","comment":"11 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.07726v1","updated":"2023-03-14T09:15:51Z","published":"2023-03-14T09:15:51Z","title":"Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme\n  Conversion","summary":"  Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework\nthat first transforms input sequences into character embeddings, obtains\nlinguistic information using language models, and then predicts the phonemes\nbased on global context about the entire input sequence. However, linguistic\nknowledge alone is often inadequate. Language models frequently encode overly\ngeneral structures of a sentence and fail to cover specific cases needed to use\nphonetic knowledge. Also, a handcrafted post-processing system is needed to\naddress the problems relevant to the tone of the characters. However, the\nsystem exhibits inconsistency in the segmentation of word boundaries which\nconsequently degrades the performance of the G2P system. To address these\nissues, we propose the Reinforcer that provides strong inductive bias for\nlanguage models by emphasizing the phonological information between neighboring\ncharacters to help disambiguate pronunciations. Experimental results show that\nthe Reinforcer boosts the cutting-edge architectures by a large margin. We also\ncombine the Reinforcer with a large-scale pre-trained model and demonstrate the\nvalidity of using neighboring context in knowledge transfer scenarios.\n","authors":["Jungjun Kim","Changjin Han","Gyuhyeon Nam","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07726v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07711v1","updated":"2023-03-14T08:52:58Z","published":"2023-03-14T08:52:58Z","title":"Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised\n  Style Extractor and Hierarchical Modeling in Speech Synthesis","summary":"  Cross-speaker style transfer in speech synthesis aims at transferring a style\nfrom source speaker to synthesized speech of a target speaker's timbre. In most\nprevious methods, the synthesized fine-grained prosody features often represent\nthe source speaker's average style, similar to the one-to-many problem(i.e.,\nmultiple prosody variations correspond to the same text). In response to this\nproblem, a strength-controlled semi-supervised style extractor is proposed to\ndisentangle the style from content and timbre, improving the representation and\ninterpretability of the global style embedding, which can alleviate the\none-to-many mapping and data imbalance problems in prosody prediction. A\nhierarchical prosody predictor is proposed to improve prosody modeling. We find\nthat better style transfer can be achieved by using the source speaker's\nprosody features that are easily predicted. Additionally, a\nspeaker-transfer-wise cycle consistency loss is proposed to assist the model in\nlearning unseen style-timbre combinations during the training phase.\nExperimental results show that the method outperforms the baseline. We provide\na website with audio samples.\n","authors":["Chunyu Qiang","Peng Yang","Hao Che","Ying Zhang","Xiaorui Wang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07711v1.pdf","comment":"Accepted by ICASSP2023"},{"id":"http://arxiv.org/abs/2211.14769v3","updated":"2023-03-14T08:21:31Z","published":"2022-11-27T09:01:31Z","title":"Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied\n  Agents under Federated Learning","summary":"  Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n","authors":["Yunchao Zhang","Zonglin Di","Kaiwen Zhou","Cihang Xie","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.14769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12462v2","updated":"2023-03-14T08:11:26Z","published":"2022-05-25T03:21:27Z","title":"Improving CTC-based ASR Models with Gated Interlayer Collaboration","summary":"  The CTC-based automatic speech recognition (ASR) models without the external\nlanguage model usually lack the capacity to model conditional dependencies and\ntextual interactions. In this paper, we present a Gated Interlayer\nCollaboration (GIC) mechanism to improve the performance of CTC-based models,\nwhich introduces textual information into the model and thus relaxes the\nconditional independence assumption of CTC-based models. Specifically, we\nconsider the weighted sum of token embeddings as the textual representation for\neach position, where the position-specific weights are the softmax probability\ndistribution constructed via inter-layer auxiliary CTC losses. The textual\nrepresentations are then fused with acoustic features by developing a gate\nunit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the\nproposed method outperforms several strong baselines.\n","authors":["Yuting Yang","Yuke Li","Binbin Du"],"pdf_url":"https://arxiv.org/pdf/2205.12462v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07689v1","updated":"2023-03-14T08:04:38Z","published":"2023-03-14T08:04:38Z","title":"Dual-Attention Model for Aspect-Level Sentiment Classification","summary":"  I propose a novel dual-attention model(DAM) for aspect-level sentiment\nclassification. Many methods have been proposed, such as support vector\nmachines for artificial design features, long short-term memory networks based\non attention mechanisms, and graph neural networks based on dependency parsing.\nWhile these methods all have decent performance, I think they all miss one\nimportant piece of syntactic information: dependency labels. Based on this\nidea, this paper proposes a model using dependency labels for the attention\nmechanism to do this task. We evaluate the proposed approach on three datasets:\nlaptop and restaurant are from SemEval 2014, and the last one is a twitter\ndataset. Experimental results show that the dual attention model has good\nperformance on all three datasets.\n","authors":["Mengfei Ye"],"pdf_url":"https://arxiv.org/pdf/2303.07689v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.07687v1","updated":"2023-03-14T08:01:21Z","published":"2023-03-14T08:01:21Z","title":"Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy","summary":"  Because of predicting all the target tokens in parallel, the\nnon-autoregressive models greatly improve the decoding efficiency of speech\nrecognition compared with traditional autoregressive models. In this work, we\npresent dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross\nEntropy (AXE), finding the monotonic alignment that minimizes the cross-entropy\nloss through dynamic programming, (2) Dynamic Rectification, creating new\ntraining samples by replacing some masks with model predicted tokens. The AXE\nignores the absolute position alignment between prediction and ground truth\nsentence and focuses on tokens matching in relative order. The dynamic\nrectification method makes the model capable of simulating the non-mask but\npossible wrong tokens, even if they have high confidence. Our experiments on\nWSJ dataset demonstrated that not only AXE loss but also the rectification\nmethod could improve the WER performance of Mask CTC.\n","authors":["Xulong Zhang","Haobin Tang","Jianzong Wang","Ning Cheng","Jian Luo","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.07687v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07682v1","updated":"2023-03-14T07:53:19Z","published":"2023-03-14T07:53:19Z","title":"QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis","summary":"  Recent expressive text to speech (TTS) models focus on synthesizing emotional\nspeech, but some fine-grained styles such as intonation are neglected. In this\npaper, we propose QI-TTS which aims to better transfer and control intonation\nto further deliver the speaker's questioning intention while transferring\nemotion from reference speech. We propose a multi-style extractor to extract\nstyle embedding from two different levels. While the sentence level represents\nemotion, the final syllable level represents intonation. For fine-grained\nintonation control, we use relative attributes to represent intonation\nintensity at the syllable level.Experiments have validated the effectiveness of\nQI-TTS for improving intonation expressiveness in emotional speech synthesis.\n","authors":["Haobin Tang","Xulong Zhang","Jianzong Wang","Ning Cheng","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2303.07682v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07678v1","updated":"2023-03-14T07:27:30Z","published":"2023-03-14T07:27:30Z","title":"Query2doc: Query Expansion with Large Language Models","summary":"  This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n","authors":["Liang Wang","Nan Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2303.07678v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2211.04215v2","updated":"2023-03-14T07:15:08Z","published":"2022-11-08T13:01:17Z","title":"Active Relation Discovery: Towards General and Label-aware Open Relation\n  Extraction","summary":"  Open Relation Extraction (OpenRE) aims to discover novel relations from open\ndomains. Previous OpenRE methods mainly suffer from two problems: (1)\nInsufficient capacity to discriminate between known and novel relations. When\nextending conventional test settings to a more general setting where test data\nmight also come from seen classes, existing approaches have a significant\nperformance decline. (2) Secondary labeling must be performed before practical\napplication. Existing methods cannot label human-readable and meaningful types\nfor novel relations, which is urgently required by the downstream tasks. To\naddress these issues, we propose the Active Relation Discovery (ARD) framework,\nwhich utilizes relational outlier detection for discriminating known and novel\nrelations and involves active learning for labeling novel relations. Extensive\nexperiments on three real-world datasets show that ARD significantly\noutperforms previous state-of-the-art methods on both conventional and our\nproposed general OpenRE settings. The source code and datasets will be\navailable for reproducibility.\n","authors":["Yangning Li","Yinghui Li","Xi Chen","Hai-Tao Zheng","Ying Shen","Hong-Gee Kim"],"pdf_url":"https://arxiv.org/pdf/2211.04215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07665v1","updated":"2023-03-14T07:10:03Z","published":"2023-03-14T07:10:03Z","title":"RenewNAT: Renewing Potential Translation for Non-Autoregressive\n  Transformer","summary":"  Non-autoregressive neural machine translation (NAT) models are proposed to\naccelerate the inference process while maintaining relatively high performance.\nHowever, existing NAT models are difficult to achieve the desired\nefficiency-quality trade-off. For one thing, fully NAT models with efficient\ninference perform inferior to their autoregressive counterparts. For another,\niterative NAT models can, though, achieve comparable performance while\ndiminishing the advantage of speed. In this paper, we propose RenewNAT, a\nflexible framework with high efficiency and effectiveness, to incorporate the\nmerits of fully and iterative NAT models. RenewNAT first generates the\npotential translation results and then renews them in a single pass. It can\nachieve significant performance improvements at the same expense as traditional\nNAT models (without introducing additional model parameters and decoding\nlatency). Experimental results on various translation benchmarks (e.g.,\n\\textbf{4} WMT) show that our framework consistently improves the performance\nof strong fully NAT methods (e.g., GLAT and DSLP) without additional speed\noverhead.\n","authors":["Pei Guo","Yisheng Xiao","Juntao Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07665v1.pdf","comment":"Accepted by AAAI23"},{"id":"http://arxiv.org/abs/2303.07650v1","updated":"2023-03-14T06:34:18Z","published":"2023-03-14T06:34:18Z","title":"Cross-lingual Alzheimer's Disease detection based on paralinguistic and\n  pre-trained features","summary":"  We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task,\nwhich aims to investigate which acoustic features can be generalized and\ntransferred across languages for Alzheimer's Disease (AD) prediction. The\nchallenge consists of two tasks: one is to classify the speech of AD patients\nand healthy individuals, and the other is to infer Mini Mental State\nExamination (MMSE) score based on speech only. The difficulty is mainly\nembodied in the mismatch of the dataset, in which the training set is in\nEnglish while the test set is in Greek. We extract paralinguistic features\nusing openSmile toolkit and acoustic features using XLSR-53. In addition, we\nextract linguistic features after transcribing the speech into text. These\nfeatures are used as indicators for AD detection in our method. Our method\nachieves an accuracy of 69.6% on the classification task and a root mean\nsquared error (RMSE) of 4.788 on the regression task. The results show that our\nproposed method is expected to achieve automatic multilingual Alzheimer's\nDisease detection through spontaneous speech.\n","authors":["Xuchu Chen","Yu Pu","Jinpeng Li","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07650v1.pdf","comment":"accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07624v1","updated":"2023-03-14T04:47:00Z","published":"2023-03-14T04:47:00Z","title":"I3D: Transformer architectures with input-dependent dynamic depth for\n  speech recognition","summary":"  Transformer-based end-to-end speech recognition has achieved great success.\nHowever, the large footprint and computational overhead make it difficult to\ndeploy these models in some real-world applications. Model compression\ntechniques can reduce the model size and speed up inference, but the compressed\nmodel has a fixed architecture which might be suboptimal. We propose a novel\nTransformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong\nperformance-efficiency trade-offs. With a similar number of layers at inference\ntime, I3D-based models outperform the vanilla Transformer and the static pruned\nmodel via iterative layer pruning. We also present interesting analysis on the\ngate probabilities and the input-dependency, which helps us better understand\ndeep encoders.\n","authors":["Yifan Peng","Jaesong Lee","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2303.07624v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2101.06397v2","updated":"2023-03-14T04:18:42Z","published":"2021-01-16T08:12:03Z","title":"To Understand Representation of Layer-aware Sequence Encoders as\n  Multi-order-graph","summary":"  In this paper, we propose an explanation of representation for self-attention\nnetwork (SAN) based neural sequence encoders, which regards the information\ncaptured by the model and the encoding of the model as graph structure and the\ngeneration of these graph structures respectively. The proposed explanation\napplies to existing works on SAN-based models and can explain the relationship\namong the ability to capture the structural or linguistic information, depth of\nmodel, and length of sentence, and can also be extended to other models such as\nrecurrent neural network based models. We also propose a revisited multigraph\ncalled Multi-order-Graph (MoG) based on our explanation to model the graph\nstructures in the SAN-based model as subgraphs in MoG and convert the encoding\nof SAN-based model to the generation of MoG. Based on our explanation, we\nfurther introduce a Graph-Transformer by enhancing the ability to capture\nmultiple subgraphs of different orders and focusing on subgraphs of high\norders. Experimental results on multiple neural machine translation tasks show\nthat the Graph-Transformer can yield effective performance improvement.\n","authors":["Sufeng Duan","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2101.06397v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2009.07489"},{"id":"http://arxiv.org/abs/2303.07616v1","updated":"2023-03-14T03:49:22Z","published":"2023-03-14T03:49:22Z","title":"The Life Cycle of Knowledge in Big Language Models: A Survey","summary":"  Knowledge plays a critical role in artificial intelligence. Recently, the\nextensive success of pre-trained language models (PLMs) has raised significant\nattention about how knowledge can be acquired, maintained, updated and used by\nlanguage models. Despite the enormous amount of related studies, there still\nlacks a unified view of how knowledge circulates within language models\nthroughout the learning, tuning, and application processes, which may prevent\nus from further understanding the connections between current progress or\nrealizing existing limitations. In this survey, we revisit PLMs as\nknowledge-based systems by dividing the life circle of knowledge in PLMs into\nfive critical periods, and investigating how knowledge circulates when it is\nbuilt, maintained and used. To this end, we systematically review existing\nstudies of each period of the knowledge life cycle, summarize the main\nchallenges and current limitations, and discuss future directions.\n","authors":["Boxi Cao","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2303.07616v1.pdf","comment":"paperlist: https://github.com/c-box/KnowledgeLifecycle"},{"id":"http://arxiv.org/abs/2303.07610v1","updated":"2023-03-14T03:13:02Z","published":"2023-03-14T03:13:02Z","title":"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on\n  Consistency with Human Preferences","summary":"  As a natural language assistant, ChatGPT is capable of performing various\ntasks, including but not limited to article generation, code completion, and\ndata analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\nlevel of accuracy and reliability in terms of content evaluation, exhibiting\nthe capability of mimicking human preferences. To further explore ChatGPT's\npotential in this regard, a study is conducted to assess its ability to rank\ncontent. In order to do so, a test set consisting of prompts is created,\ncovering a wide range of use cases, and five models are utilized to generate\ncorresponding responses. ChatGPT is then instructed to rank the responses\ngenerated by these models. The results on the test set show that ChatGPT's\nranking preferences are consistent with human to a certain extent. This\npreliminary experimental finding implies that ChatGPT's zero-shot ranking\ncapability could be used to reduce annotation pressure in a number of ranking\ntasks.\n","authors":["Yunjie Ji","Yan Gong","Yiping Peng","Chao Ni","Peiyan Sun","Dongyu Pan","Baochang Ma","Xiangang Li"],"pdf_url":"https://arxiv.org/pdf/2303.07610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07585v1","updated":"2023-03-14T02:11:24Z","published":"2023-03-14T02:11:24Z","title":"Input-length-shortening and text generation via attention values","summary":"  Identifying words that impact a task's performance more than others is a\nchallenge in natural language processing. Transformers models have recently\naddressed this issue by incorporating an attention mechanism that assigns\ngreater attention (i.e., relevance) scores to some words than others. Because\nof the attention mechanism's high computational cost, transformer models\nusually have an input-length limitation caused by hardware constraints. This\nlimitation applies to many transformers, including the well-known bidirectional\nencoder representations of the transformer (BERT) model. In this paper, we\nexamined BERT's attention assignment mechanism, focusing on two questions: (1)\nHow can attention be employed to reduce input length? (2) How can attention be\nused as a control mechanism for conditional text generation? We investigated\nthese questions in the context of a text classification task. We discovered\nthat BERT's early layers assign more critical attention scores for text\nclassification tasks compared to later layers. We demonstrated that the first\nlayer's attention sums could be used to filter tokens in a given sequence,\nconsiderably decreasing the input length while maintaining good test accuracy.\nWe also applied filtering, which uses a compute-efficient semantic similarities\nalgorithm, and discovered that retaining approximately 6\\% of the original\nsequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we\ncould generate data in a stable manner and indistinguishable from the original\none by only using a small percentage (10\\%) of the tokens with high attention\nscores according to BERT's first layer.\n","authors":["Neşet Özkan Tan","Alex Yuxuan Peng","Joshua Bensemann","Qiming Bao","Tim Hartill","Mark Gahegan","Michael Witbrock"],"pdf_url":"https://arxiv.org/pdf/2303.07585v1.pdf","comment":"7 pages, 4 figures. AAAI23-EMC2"},{"id":"http://arxiv.org/abs/2303.07576v1","updated":"2023-03-14T01:53:49Z","published":"2023-03-14T01:53:49Z","title":"Diffusion Models in NLP: A Survey","summary":"  Diffusion models have become a powerful family of deep generative models,\nwith record-breaking performance in many applications. This paper first gives\nan overview and derivation of the basic theory of diffusion models, then\nreviews the research results of diffusion models in the field of natural\nlanguage processing, from text generation, text-driven image generation and\nother four aspects, and analyzes and summarizes the relevant literature\nmaterials sorted out, and finally records the experience and feelings of this\ntopic literature review research.\n","authors":["Yuansong Zhu","Yu Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.04831v2","updated":"2023-03-14T01:53:07Z","published":"2022-01-13T08:25:53Z","title":"Knowledge Graph Augmented Network Towards Multiview Representation\n  Learning for Aspect-based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment\nanalysis. To better comprehend long complicated sentences and obtain accurate\naspect-specific information, linguistic and commonsense knowledge are generally\nrequired in this task. However, most current methods employ complicated and\ninefficient approaches to incorporate external knowledge, e.g., directly\nsearching the graph nodes. Additionally, the complementarity between external\nknowledge and linguistic information has not been thoroughly studied. To this\nend, we propose a knowledge graph augmented network KGAN, which aims to\neffectively incorporate external knowledge with explicitly syntactic and\ncontextual information. In particular, KGAN captures the sentiment feature\nrepresentations from multiple different perspectives, i.e., context-, syntax-\nand knowledge-based. First, KGAN learns the contextual and syntactic\nrepresentations in parallel to fully extract the semantic features. Then, KGAN\nintegrates the knowledge graphs into the embedding space, based on which the\naspect-specific knowledge representations are further obtained via an attention\nmechanism. Last, we propose a hierarchical fusion module to complement these\nmulti-view representations in a local-to-global manner. Extensive experiments\non five popular ABSA benchmarks demonstrate the effectiveness and robustness of\nour KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN\nachieves a new record of state-of-the-art performance among all datasets.\n","authors":["Qihuang Zhong","Liang Ding","Juhua Liu","Bo Du","Hua Jin","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2201.04831v2.pdf","comment":"Accepted by IEEE TKDE 2023"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.08138v1","updated":"2023-03-14T17:59:59Z","published":"2023-03-14T17:59:59Z","title":"Diversity-Aware Meta Visual Prompting","summary":"  We present Diversity-Aware Meta Visual Prompting~(DAM-VP), an efficient and\neffective prompting method for transferring pre-trained models to downstream\ntasks with frozen backbone. A challenging issue in visual prompting is that\nimage datasets sometimes have a large data diversity whereas a per-dataset\ngeneric prompt can hardly handle the complex distribution shift toward the\noriginal pretraining data distribution properly. To address this issue, we\npropose a dataset Diversity-Aware prompting strategy whose initialization is\nrealized by a Meta-prompt. Specifically, we cluster the downstream dataset into\nsmall homogeneity subsets in a diversity-adaptive way, with each subset has its\nown prompt optimized separately. Such a divide-and-conquer design reduces the\noptimization difficulty greatly and significantly boosts the prompting\nperformance. Furthermore, all the prompts are initialized with a meta-prompt,\nwhich is learned across several datasets. It is a bootstrapped paradigm, with\nthe key observation that the prompting knowledge learned from previous datasets\ncould help the prompt to converge faster and perform better on a new dataset.\nDuring inference, we dynamically select a proper prompt for each input, based\non the feature distance between the input and each subset. Through extensive\nexperiments, our DAM-VP demonstrates superior efficiency and effectiveness,\nclearly surpassing previous prompting methods in a series of downstream\ndatasets for different pretraining models. Our code is available at:\n\\url{https://github.com/shikiw/DAM-VP}.\n","authors":["Qidong Huang","Xiaoyi Dong","Dongdong Chen","Weiming Zhang","Feifei Wang","Gang Hua","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2303.08138v1.pdf","comment":"CVPR2023, code is available at https://github.com/shikiw/DAM-VP"},{"id":"http://arxiv.org/abs/2303.08137v1","updated":"2023-03-14T17:59:47Z","published":"2023-03-14T17:59:47Z","title":"LayoutDM: Discrete Diffusion Model for Controllable Layout Generation","summary":"  Controllable layout generation aims at synthesizing plausible arrangement of\nelement bounding boxes with optional constraints, such as type or position of a\nspecific element. In this work, we try to solve a broad range of layout\ngeneration tasks in a single model that is based on discrete state-space\ndiffusion models. Our model, named LayoutDM, naturally handles the structured\nlayout data in the discrete representation and learns to progressively infer a\nnoiseless layout from the initial input, where we model the layout corruption\nprocess by modality-wise discrete diffusion. For conditional generation, we\npropose to inject layout constraints in the form of masking or logit adjustment\nduring inference. We show in the experiments that our LayoutDM successfully\ngenerates high-quality layouts and outperforms both task-specific and\ntask-agnostic baselines on several layout tasks.\n","authors":["Naoto Inoue","Kotaro Kikuchi","Edgar Simo-Serra","Mayu Otani","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.08137v1.pdf","comment":"To be published in CVPR2023, project page:\n  https://cyberagentailab.github.io/layout-dm/"},{"id":"http://arxiv.org/abs/2303.08134v1","updated":"2023-03-14T17:59:02Z","published":"2023-03-14T17:59:02Z","title":"Parameter is Not All You Need: Starting from Non-Parametric Networks for\n  3D Point Cloud Analysis","summary":"  We present a Non-parametric Network for 3D point cloud analysis, Point-NN,\nwhich consists of purely non-learnable components: farthest point sampling\n(FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric\nfunctions. Surprisingly, it performs well on various 3D tasks, requiring no\nparameters or training, and even surpasses existing fully trained models.\nStarting from this basic non-parametric model, we propose two extensions.\nFirst, Point-NN can serve as a base architectural framework to construct\nParametric Networks by simply inserting linear layers on top. Given the\nsuperior non-parametric foundation, the derived Point-PN exhibits a high\nperformance-efficiency trade-off with only a few learnable parameters. Second,\nPoint-NN can be regarded as a plug-and-play module for the already trained 3D\nmodels during inference. Point-NN captures the complementary geometric\nknowledge and enhances existing methods for different 3D benchmarks without\nre-training. We hope our work may cast a light on the community for\nunderstanding 3D point clouds with non-parametric methods. Code is available at\nhttps://github.com/ZrrSkywalker/Point-NN.\n","authors":["Renrui Zhang","Liuhui Wang","Yali Wang","Peng Gao","Hongsheng Li","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2303.08134v1.pdf","comment":"Accepted by CVPR 2023. Code is available at\n  https://github.com/ZrrSkywalker/Point-NN"},{"id":"http://arxiv.org/abs/2303.08133v1","updated":"2023-03-14T17:59:01Z","published":"2023-03-14T17:59:01Z","title":"MeshDiffusion: Score-based Generative 3D Mesh Modeling","summary":"  We consider the task of generating realistic 3D shapes, which is useful for a\nvariety of applications such as automatic scene generation and physical\nsimulation. Compared to other 3D representations like voxels and point clouds,\nmeshes are more desirable in practice, because (1) they enable easy and\narbitrary manipulation of shapes for relighting and simulation, and (2) they\ncan fully leverage the power of modern graphics pipelines which are mostly\noptimized for meshes. Previous scalable methods for generating meshes typically\nrely on sub-optimal post-processing, and they tend to produce overly-smooth or\nnoisy surfaces without fine-grained geometric details. To overcome these\nshortcomings, we take advantage of the graph structure of meshes and use a\nsimple yet very effective generative modeling method to generate 3D meshes.\nSpecifically, we represent meshes with deformable tetrahedral grids, and then\ntrain a diffusion model on this direct parametrization. We demonstrate the\neffectiveness of our model on multiple generative tasks.\n","authors":["Zhen Liu","Yao Feng","Michael J. Black","Derek Nowrouzezahrai","Liam Paull","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08133v1.pdf","comment":"Published in ICLR 2023 (Spotlight, Notable-top-25%)"},{"id":"http://arxiv.org/abs/2303.08132v1","updated":"2023-03-14T17:58:44Z","published":"2023-03-14T17:58:44Z","title":"InstMove: Instance Motion for Object-centric Video Segmentation","summary":"  Despite significant efforts, cutting-edge video segmentation methods still\nremain sensitive to occlusion and rapid movement, due to their reliance on the\nappearance of objects in the form of object embeddings, which are vulnerable to\nthese disturbances. A common solution is to use optical flow to provide motion\ninformation, but essentially it only considers pixel-level motion, which still\nrelies on appearance similarity and hence is often inaccurate under occlusion\nand fast movement. In this work, we study the instance-level motion and present\nInstMove, which stands for Instance Motion for Object-centric Video\nSegmentation. In comparison to pixel-wise motion, InstMove mainly relies on\ninstance-level motion information that is free from image feature embeddings,\nand features physical interpretations, making it more accurate and robust\ntoward occlusion and fast-moving objects. To better fit in with the video\nsegmentation tasks, InstMove uses instance masks to model the physical presence\nof an object and learns the dynamic model through a memory network to predict\nits position and shape in the next frame. With only a few lines of code,\nInstMove can be integrated into current SOTA methods for three different video\nsegmentation tasks and boost their performance. Specifically, we improve the\nprevious arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and\n4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast-moving objects.\nThese results suggest that instance-level motion is robust and accurate, and\nhence serving as a powerful solution in complex scenarios for object-centric\nvideo segmentation.\n","authors":["Qihao Liu","Junfeng Wu","Yi Jiang","Xiang Bai","Alan Yuille","Song Bai"],"pdf_url":"https://arxiv.org/pdf/2303.08132v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08131v1","updated":"2023-03-14T17:58:34Z","published":"2023-03-14T17:58:34Z","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","summary":"  We present \\ourmodel{}, a simple Open-vocabulary Segmentation and Detection\nframework that jointly learns from different segmentation and detection\ndatasets. To bridge the gap of vocabulary and annotation granularity, we first\nintroduce a pre-trained text encoder to encode all the visual concepts in two\ntasks and learn a common semantic space for them. This gives us reasonably good\nresults compared with the counterparts trained on segmentation task only. To\nfurther reconcile them, we locate two discrepancies: $i$) task discrepancy --\nsegmentation requires extracting masks for both foreground objects and\nbackground stuff, while detection merely cares about the former; $ii$) data\ndiscrepancy -- box and mask annotations are with different spatial granularity,\nand thus not directly interchangeable. To address these issues, we propose a\ndecoupled decoding to reduce the interference between foreground/background and\na conditioned mask decoding to assist in generating masks for given boxes. To\nthis end, we develop a simple encoder-decoder model encompassing all three\ntechniques and train it jointly on COCO and Objects365. After pre-training, our\nmodel exhibits competitive or stronger zero-shot transferability for both\nsegmentation and detection. Specifically, \\ourmodel{} beats the\nstate-of-the-art method for open-vocabulary instance and panoptic segmentation\nacross 5 datasets, and outperforms previous work for open-vocabulary detection\non LVIS and ODinW under similar settings. When transferred to specific tasks,\nour model achieves new SoTA for panoptic segmentation on COCO and ADE20K, and\ninstance segmentation on ADE20K and Cityscapes.\n  Finally, we note that \\ourmodel{} is the first to explore the potential of\njoint training on segmentation and detection, and hope it can be received as a\nstrong baseline for developing a single model for both tasks in open world.\n","authors":["Hao Zhang","Feng Li","Xueyan Zou","Shilong Liu","Chunyuan Li","Jianfeng Gao","Jianwei Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08131v1.pdf","comment":"A Simple Framework for Open-Vocabulary Segmentation and Detection"},{"id":"http://arxiv.org/abs/2303.08129v1","updated":"2023-03-14T17:58:03Z","published":"2023-03-14T17:58:03Z","title":"PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D\n  Object Detection","summary":"  Masked Autoencoders learn strong visual representations and achieve\nstate-of-the-art results in several independent modalities, yet very few works\nhave addressed their capabilities in multi-modality settings. In this work, we\nfocus on point cloud and RGB image data, two modalities that are often\npresented together in the real world, and explore their meaningful\ninteractions. To improve upon the cross-modal synergy in existing works, we\npropose PiMAE, a self-supervised pre-training framework that promotes 3D and 2D\ninteraction through three aspects. Specifically, we first notice the importance\nof masking strategies between the two sources and utilize a projection module\nto complementarily align the mask and visible tokens of the two modalities.\nThen, we utilize a well-crafted two-branch MAE pipeline with a novel shared\ndecoder to promote cross-modality interaction in the mask tokens. Finally, we\ndesign a unique cross-modal reconstruction module to enhance representation\nlearning for both modalities. Through extensive experiments performed on\nlarge-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2), we\ndiscover it is nontrivial to interactively learn point-image features, where we\ngreatly improve multiple 3D detectors, 2D detectors, and few-shot classifiers\nby 2.9%, 6.7%, and 2.4%, respectively. Code is available at\nhttps://github.com/BLVLab/PiMAE.\n","authors":["Anthony Chen","Kevin Zhang","Renrui Zhang","Zihan Wang","Yuheng Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.08129v1.pdf","comment":"Accepted by CVPR2023. Code is available at\n  https://github.com/BLVLab/PiMAE"},{"id":"http://arxiv.org/abs/2303.08128v1","updated":"2023-03-14T17:57:47Z","published":"2023-03-14T17:57:47Z","title":"ViperGPT: Visual Inference via Python Execution for Reasoning","summary":"  Answering visual queries is a complex task that requires both visual\nprocessing and reasoning. End-to-end models, the dominant approach for this\ntask, do not explicitly differentiate between the two, limiting\ninterpretability and generalization. Learning modular programs presents a\npromising alternative, but has proven challenging due to the difficulty of\nlearning both the programs and modules simultaneously. We introduce ViperGPT, a\nframework that leverages code-generation models to compose vision-and-language\nmodels into subroutines to produce a result for any query. ViperGPT utilizes a\nprovided API to access the available modules, and composes them by generating\nPython code that is later executed. This simple approach requires no further\ntraining, and achieves state-of-the-art results across various complex visual\ntasks.\n","authors":["Dídac Surís","Sachit Menon","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2303.08128v1.pdf","comment":"Website: https://viper.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2303.08120v1","updated":"2023-03-14T17:52:29Z","published":"2023-03-14T17:52:29Z","title":"Blind Video Deflickering by Neural Filtering with a Flawed Atlas","summary":"  Many videos contain flickering artifacts. Common causes of flicker include\nvideo processing algorithms, video generation algorithms, and capturing videos\nunder specific situations. Prior work usually requires specific guidance such\nas the flickering frequency, manual annotations, or extra consistent videos to\nremove the flicker. In this work, we propose a general flicker removal\nframework that only receives a single flickering video as input without\nadditional guidance. Since it is blind to a specific flickering type or\nguidance, we name this \"blind deflickering.\" The core of our approach is\nutilizing the neural atlas in cooperation with a neural filtering strategy. The\nneural atlas is a unified representation for all frames in a video that\nprovides temporal consistency guidance but is flawed in many cases. To this\nend, a neural network is trained to mimic a filter to learn the consistent\nfeatures (e.g., color, brightness) and avoid introducing the artifacts in the\natlas. To validate our method, we construct a dataset that contains diverse\nreal-world flickering videos. Extensive experiments show that our method\nachieves satisfying deflickering performance and even outperforms baselines\nthat use extra guidance on a public benchmark.\n","authors":["Chenyang Lei","Xuanchi Ren","Zhaoxiang Zhang","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.08120v1.pdf","comment":"To appear in CVPR2023. Code:\n  github.com/ChenyangLEI/All-In-One-Deflicker Website:\n  chenyanglei.github.io/deflicker"},{"id":"http://arxiv.org/abs/2303.08113v1","updated":"2023-03-14T17:47:18Z","published":"2023-03-14T17:47:18Z","title":"Homeomorphic Image Registration via Conformal-Invariant Hyperelastic\n  Regularisation","summary":"  Deformable image registration is a fundamental task in medical image analysis\nand plays a crucial role in a wide range of clinical applications. Recently,\ndeep learning-based approaches have been widely studied for deformable medical\nimage registration and achieved promising results. However, existing deep\nlearning image registration techniques do not theoretically guarantee\ntopology-preserving transformations. This is a key property to preserve\nanatomical structures and achieve plausible transformations that can be used in\nreal clinical settings. We propose a novel framework for deformable image\nregistration. Firstly, we introduce a novel regulariser based on\nconformal-invariant properties in a nonlinear elasticity setting. Our\nregulariser enforces the deformation field to be smooth, invertible and\norientation-preserving. More importantly, we strictly guarantee topology\npreservation yielding to a clinical meaningful registration. Secondly, we boost\nthe performance of our regulariser through coordinate MLPs, where one can view\nthe to-be-registered images as continuously differentiable entities. We\ndemonstrate, through numerical and visual experiments, that our framework is\nable to outperform current techniques for image registration.\n","authors":["Jing Zou","Noémie Debroux","Lihao Liu","Jing Qin","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2303.08113v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.08096v1","updated":"2023-03-14T17:33:39Z","published":"2023-03-14T17:33:39Z","title":"MELON: NeRF with Unposed Images Using Equivalence Class Estimation","summary":"  Neural radiance fields enable novel-view synthesis and scene reconstruction\nwith photorealistic quality from a few images, but require known and accurate\ncamera poses. Conventional pose estimation algorithms fail on smooth or\nself-similar scenes, while methods performing inverse rendering from unposed\nviews require a rough initialization of the camera orientations. The main\ndifficulty of pose estimation lies in real-life objects being almost invariant\nunder certain transformations, making the photometric distance between rendered\nviews non-convex with respect to the camera parameters. Using an equivalence\nrelation that matches the distribution of local minima in camera space, we\nreduce this space to its quotient set, in which pose estimation becomes a more\nconvex problem. Using a neural-network to regularize pose estimation, we\ndemonstrate that our method - MELON - can reconstruct a neural radiance field\nfrom unposed images with state-of-the-art accuracy while requiring ten times\nfewer views than adversarial approaches.\n","authors":["Axel Levy","Mark Matthews","Matan Sela","Gordon Wetzstein","Dmitry Lagun"],"pdf_url":"https://arxiv.org/pdf/2303.08096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08085v1","updated":"2023-03-14T17:16:16Z","published":"2023-03-14T17:16:16Z","title":"Alias-Free Convnets: Fractional Shift Invariance via Polynomial\n  Activations","summary":"  Although CNNs are believed to be invariant to translations, recent works have\nshown this is not the case, due to aliasing effects that stem from downsampling\nlayers. The existing architectural solutions to prevent aliasing are partial\nsince they do not solve these effects, that originate in non-linearities. We\npropose an extended anti-aliasing method that tackles both downsampling and\nnon-linear layers, thus creating truly alias-free, shift-invariant CNNs. We\nshow that the presented model is invariant to integer as well as fractional\n(i.e., sub-pixel) translations, thus outperforming other shift-invariant\nmethods in terms of robustness to adversarial translations.\n","authors":["Hagay Michaeli","Tomer Michaeli","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2303.08085v1.pdf","comment":"https://github.com/hmichaeli/alias_free_convnets"},{"id":"http://arxiv.org/abs/2303.08084v1","updated":"2023-03-14T17:14:21Z","published":"2023-03-14T17:14:21Z","title":"Editing Implicit Assumptions in Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models often make implicit assumptions about the\nworld when generating images. While some assumptions are useful (e.g., the sky\nis blue), they can also be outdated, incorrect, or reflective of social biases\npresent in the training data. Thus, there is a need to control these\nassumptions without requiring explicit user input or costly re-training. In\nthis work, we aim to edit a given implicit assumption in a pre-trained\ndiffusion model. Our Text-to-Image Model Editing method, TIME for short,\nreceives a pair of inputs: a \"source\" under-specified prompt for which the\nmodel makes an implicit assumption (e.g., \"a pack of roses\"), and a\n\"destination\" prompt that describes the same setting, but with a specified\ndesired attribute (e.g., \"a pack of blue roses\"). TIME then updates the model's\ncross-attention layers, as these layers assign visual meaning to textual\ntokens. We edit the projection matrices in these layers such that the source\nprompt is projected close to the destination prompt. Our method is highly\nefficient, as it modifies a mere 2.2% of the model's parameters in under one\nsecond. To evaluate model editing approaches, we introduce TIMED (TIME\nDataset), containing 147 source and destination prompt pairs from various\ndomains. Our experiments (using Stable Diffusion) show that TIME is successful\nin model editing, generalizes well for related prompts unseen during editing,\nand imposes minimal effect on unrelated generations.\n","authors":["Hadas Orgad","Bahjat Kawar","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2303.08084v1.pdf","comment":"Project page: https://time-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2303.03052v2","updated":"2023-03-14T17:04:49Z","published":"2023-03-06T11:51:28Z","title":"Masked Images Are Counterfactual Samples for Robust Fine-tuning","summary":"  Deep learning models are challenged by the distribution shift between the\ntraining data and test data. Recently, the large models pre-trained on diverse\ndata demonstrate unprecedented robustness to various distribution shifts.\nHowever, fine-tuning on these models can lead to a trade-off between\nin-distribution (ID) performance and out-of-distribution (OOD) robustness.\nExisting methods for tackling this trade-off do not explicitly address the OOD\nrobustness problem. In this paper, based on causal analysis on the\naforementioned problems, we propose a novel fine-tuning method, which use\nmasked images as counterfactual samples that help improving the robustness of\nthe fine-tuning model. Specifically, we mask either the semantics-related or\nsemantics-unrelated patches of the images based on class activation map to\nbreak the spurious correlation, and refill the masked patches with patches from\nother images. The resulting counterfactual samples are used in feature-based\ndistillation with the pre-trained model. Extensive experiments verify that\nregularizing the fine-tuning with the proposed masked images can achieve a\nbetter trade-off between ID and OOD performance, surpassing previous methods on\nthe OOD performance. Our code will be publicly available.\n","authors":["Yao Xiao","Ziyi Tang","Pengxu Wei","Cong Liu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.03052v2.pdf","comment":"Accepted by CVPR 2023 (v2: improve the clarity)"},{"id":"http://arxiv.org/abs/2210.10272v2","updated":"2023-03-14T17:02:34Z","published":"2022-10-19T03:29:58Z","title":"Training set cleansing of backdoor poisoning by self-supervised\n  representation learning","summary":"  A backdoor or Trojan attack is an important type of data poisoning attack\nagainst deep neural network (DNN) classifiers, wherein the training dataset is\npoisoned with a small number of samples that each possess the backdoor pattern\n(usually a pattern that is either imperceptible or innocuous) and which are\nmislabeled to the attacker's target class. When trained on a backdoor-poisoned\ndataset, a DNN behaves normally on most benign test samples but makes incorrect\npredictions to the target class when the test sample has the backdoor pattern\nincorporated (i.e., contains a backdoor trigger). Here we focus on image\nclassification tasks and show that supervised training may build stronger\nassociation between the backdoor pattern and the associated target class than\nthat between normal features and the true class of origin. By contrast,\nself-supervised representation learning ignores the labels of samples and\nlearns a feature embedding based on images' semantic content. %We thus propose\nto use unsupervised representation learning to avoid emphasising\nbackdoor-poisoned training samples and learn a similar feature embedding for\nsamples of the same class. Using a feature embedding found by self-supervised\nrepresentation learning, a data cleansing method, which combines sample\nfiltering and re-labeling, is developed. Experiments on CIFAR-10 benchmark\ndatasets show that our method achieves state-of-the-art performance in\nmitigating backdoor attacks.\n","authors":["H. Wang","S. Karami","O. Dia","H. Ritter","E. Emamjomeh-Zadeh","J. Chen","Z. Xiang","D. J. Miller","G. Kesidis"],"pdf_url":"https://arxiv.org/pdf/2210.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03992v3","updated":"2023-03-14T16:59:18Z","published":"2023-02-08T11:01:19Z","title":"Convolutional Neural Networks Trained to Identify Words Provide a\n  Surprisingly Good Account of Visual Form Priming Effects","summary":"  A wide variety of orthographic coding schemes and models of visual word\nidentification have been developed to account for masked priming data that\nprovide a measure of orthographic similarity between letter strings. These\nmodels tend to include hand-coded orthographic representations with single unit\ncoding for specific forms of knowledge (e.g., units coding for a letter in a\ngiven position). Here we assess how well a range of these coding schemes and\nmodels account for the pattern of form priming effects taken from the Form\nPriming Project and compare these findings to results observed with 11 standard\ndeep neural network models (DNNs) developed in computer science. We find that\ndeep convolutional networks (CNNs) perform as well or better than the coding\nschemes and word recognition models, whereas transformer networks did less\nwell. The success of CNNs is remarkable as their architectures were not\ndeveloped to support word recognition (they were designed to perform well on\nobject recognition), they classify pixel images of words (rather than\nartificial encodings of letter strings), and their training was highly\nsimplified (not respecting many key aspects of human experience). In addition\nto these form priming effects, we find that the DNNs can account for visual\nsimilarity effects on priming that are beyond all current psychological models\nof priming. The findings add to the recent work of (Hannagan et al., 2021) and\nsuggest that CNNs should be given more attention in psychology as models of\nhuman visual word recognition.\n","authors":["Dong Yin","Valerio Biscione","Jeffrey Bowers"],"pdf_url":"https://arxiv.org/pdf/2302.03992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08063v1","updated":"2023-03-14T16:58:11Z","published":"2023-03-14T16:58:11Z","title":"Interpretable ODE-style Generative Diffusion Model via Force Field\n  Construction","summary":"  For a considerable time, researchers have focused on developing a method that\nestablishes a deep connection between the generative diffusion model and\nmathematical physics. Despite previous efforts, progress has been limited to\nthe pursuit of a single specialized method. In order to advance the\ninterpretability of diffusion models and explore new research directions, it is\nessential to establish a unified ODE-style generative diffusion model. Such a\nmodel should draw inspiration from physical models and possess a clear\ngeometric meaning. This paper aims to identify various physical models that are\nsuitable for constructing ODE-style generative diffusion models accurately from\na mathematical perspective. We then summarize these models into a unified\nmethod. Additionally, we perform a case study where we use the theoretical\nmodel identified by our method to develop a range of new diffusion model\nmethods, and conduct experiments. Our experiments on CIFAR-10 demonstrate the\neffectiveness of our approach. We have constructed a computational framework\nthat attains highly proficient results with regards to image generation speed,\nalongside an additional model that demonstrates exceptional performance in both\nInception score and FID score. These results underscore the significance of our\nmethod in advancing the field of diffusion models.\n","authors":["Weiyang Jin","Yongpei Zhu","Yuxi Peng"],"pdf_url":"https://arxiv.org/pdf/2303.08063v1.pdf","comment":"16pages, 13figures, 2tables"},{"id":"http://arxiv.org/abs/2303.04115v2","updated":"2023-03-14T16:57:30Z","published":"2023-03-07T18:28:39Z","title":"Predicted Embedding Power Regression for Large-Scale Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) inputs can compromise the performance and safety of\nreal world machine learning systems. While many methods exist for OOD detection\nand work well on small scale datasets with lower resolution and few classes,\nfew methods have been developed for large-scale OOD detection. Existing\nlarge-scale methods generally depend on maximum classification probability,\nsuch as the state-of-the-art grouped softmax method. In this work, we develop a\nnovel approach that calculates the probability of the predicted class label\nbased on label distributions learned during the training process. Our method\nperforms better than current state-of-the-art methods with only a negligible\nincrease in compute cost. We evaluate our method against contemporary methods\nacross $14$ datasets and achieve a statistically significant improvement with\nrespect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).\n","authors":["Hong Yang","William Gebhardt","Alexander G. Ororbia","Travis Desell"],"pdf_url":"https://arxiv.org/pdf/2303.04115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.11741v2","updated":"2023-03-14T16:57:14Z","published":"2022-09-21T21:17:56Z","title":"Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking\n  Neural Networks with Learnable Neuronal Dynamics","summary":"  Event-based cameras have recently shown great potential for high-speed motion\nestimation owing to their ability to capture temporally rich information\nasynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired\nevent-driven processing can efficiently handle such asynchronous data, while\nneuron models such as the leaky-integrate and fire (LIF) can keep track of the\nquintessential timing information contained in the inputs. SNNs achieve this by\nmaintaining a dynamic state in the neuron memory, retaining important\ninformation while forgetting redundant data over time. Thus, we posit that SNNs\nwould allow for better performance on sequential regression tasks compared to\nsimilarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult\nto train due to vanishing spikes at later layers. To that effect, we propose an\nadaptive fully-spiking framework with learnable neuronal dynamics to alleviate\nthe spike vanishing problem. We utilize surrogate gradient-based\nbackpropagation through time (BPTT) to train our deep SNNs from scratch. We\nvalidate our approach for the task of optical flow estimation on the\nMulti-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset.\nOur experiments on these datasets show an average reduction of 13% in average\nendpoint error (AEE) compared to state-of-the-art ANNs. We also explore several\ndown-scaled models and observe that our SNN models consistently outperform\nsimilarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the\nimportance of SNNs for smaller models and their suitability at the edge. In\nterms of efficiency, our SNNs offer substantial savings in network parameters\n(48.3x) and computational energy (10.2x) while attaining ~10% lower EPE\ncompared to the state-of-the-art ANN implementations.\n","authors":["Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2209.11741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08061v1","updated":"2023-03-14T16:54:59Z","published":"2023-03-14T16:54:59Z","title":"Point Cloud Diffusion Models for Automatic Implant Generation","summary":"  Advances in 3D printing of biocompatible materials make patient-specific\nimplants increasingly popular. The design of these implants is, however, still\na tedious and largely manual process. Existing approaches to automate implant\ngeneration are mainly based on 3D U-Net architectures on downsampled or\npatch-wise data, which can result in a loss of detail or contextual\ninformation. Following the recent success of Diffusion Probabilistic Models, we\npropose a novel approach for implant generation based on a combination of 3D\npoint cloud diffusion models and voxelization networks. Due to the stochastic\nsampling process in our diffusion model, we can propose an ensemble of\ndifferent implants per defect, from which the physicians can choose the most\nsuitable one. We evaluate our method on the SkullBreak and SkullFix datasets,\ngenerating high-quality implants and achieving competitive evaluation scores.\n","authors":["Paul Friedrich","Julia Wolleb","Florentin Bieder","Florian M. Thieringer","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2303.08061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02390v2","updated":"2023-03-14T16:54:14Z","published":"2022-10-05T17:05:56Z","title":"Bayesian Prompt Learning for Image-Language Model Generalization","summary":"  Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\n","authors":["Mohammad Mahdi Derakhshani","Enrique Sanchez","Adrian Bulat","Victor Guilherme Turrisi da Costa","Cees G. M. Snoek","Georgios Tzimiropoulos","Brais Martinez"],"pdf_url":"https://arxiv.org/pdf/2210.02390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08050v1","updated":"2023-03-14T16:32:24Z","published":"2023-03-14T16:32:24Z","title":"Subjective and Objective Quality Assessment for in-the-Wild Computer\n  Graphics Images","summary":"  Computer graphics images (CGIs) are artificially generated by means of\ncomputer programs and are widely perceived under various scenarios, such as\ngames, streaming media, etc. In practical, the quality of CGIs consistently\nsuffers from poor rendering during the production and inevitable compression\nartifacts during the transmission of multimedia applications. However, few\nworks have been dedicated to dealing with the challenge of computer graphics\nimages quality assessment (CGIQA). Most image quality assessment (IQA) metrics\nare developed for natural scene images (NSIs) and validated on the databases\nconsisting of NSIs with synthetic distortions, which are not suitable for\nin-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and\nCGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000\nCGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled\nlaboratory environment to obtain the accurate perceptual ratings of the CGIs.\nThen, we propose an effective deep learning-based no-reference (NR) IQA model\nby utilizing multi-stage feature fusion strategy and multi-stage channel\nattention mechanism. The major motivation of the proposed model is to make full\nuse of inter-channel information from low-level to high-level since CGIs have\napparent patterns as well as rich interactive semantic content. Experimental\nresults show that the proposed method outperforms all other state-of-the-art NR\nIQA methods on the constructed CGIQA-6k database and other CGIQA-related\ndatabases. The database along with the code will be released to facilitate\nfurther research.\n","authors":["Zicheng Zhang","Wei Sun","Tao Wang","Wei Lu","Quan Zhou","Jun he","Qiyuan Wang","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.08050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.09667v2","updated":"2023-03-14T16:26:17Z","published":"2023-01-23T19:09:36Z","title":"Improving Performance of Object Detection using the Mechanisms of Visual\n  Recognition in Humans","summary":"  Object recognition systems are usually trained and evaluated on high\nresolution images. However, in real world applications, it is common that the\nimages have low resolutions or have small sizes. In this study, we first track\nthe performance of the state-of-the-art deep object recognition network,\nFaster- RCNN, as a function of image resolution. The results reveals negative\neffects of low resolution images on recognition performance. They also show\nthat different spatial frequencies convey different information about the\nobjects in recognition process. It means multi-resolution recognition system\ncan provides better insight into optimal selection of features that results in\nbetter recognition of objects. This is similar to the mechanisms of the human\nvisual systems that are able to implement multi-scale representation of a\nvisual scene simultaneously. Then, we propose a multi-resolution object\nrecognition framework rather than a single-resolution network. The proposed\nframework is evaluated on the PASCAL VOC2007 database. The experimental results\nshow the performance of our adapted multi-resolution Faster-RCNN framework\noutperforms the single-resolution Faster-RCNN on input images with various\nresolutions with an increase in the mean Average Precision (mAP) of 9.14%\nacross all resolutions and 1.2% on the full-spectrum images. Furthermore, the\nproposed model yields robustness of the performance over a wide range of\nspatial frequencies.\n","authors":["Amir Ghasemi","Nasrin Bayat","Fatemeh Mottaghian","Akram Bayat"],"pdf_url":"https://arxiv.org/pdf/2301.09667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08035v1","updated":"2023-03-14T16:15:28Z","published":"2023-03-14T16:15:28Z","title":"ISimDL: Importance Sampling-Driven Acceleration of Fault Injection\n  Simulations for Evaluating the Robustness of Deep Learning","summary":"  Deep Learning (DL) systems have proliferated in many applications, requiring\nspecialized hardware accelerators and chips. In the nano-era, devices have\nbecome increasingly more susceptible to permanent and transient faults.\nTherefore, we need an efficient methodology for analyzing the resilience of\nadvanced DL systems against such faults, and understand how the faults in\nneural accelerator chips manifest as errors at the DL application level, where\nfaults can lead to undetectable and unrecoverable errors. Using fault\ninjection, we can perform resilience investigations of the DL system by\nmodifying neuron weights and outputs at the software-level, as if the hardware\nhad been affected by a transient fault. Existing fault models reduce the search\nspace, allowing faster analysis, but requiring a-priori knowledge on the model,\nand not allowing further analysis of the filtered-out search space. Therefore,\nwe propose ISimDL, a novel methodology that employs neuron sensitivity to\ngenerate importance sampling-based fault-scenarios. Without any a-priori\nknowledge of the model-under-test, ISimDL provides an equivalent reduction of\nthe search space as existing works, while allowing long simulations to cover\nall the possible faults, improving on existing model requirements. Our\nexperiments show that the importance sampling provides up to 15x higher\nprecision in selecting critical faults than the random uniform sampling,\nreaching such precision in less than 100 faults. Additionally, we showcase\nanother practical use-case for importance sampling for reliable DNN design,\nnamely Fault Aware Training (FAT). By using ISimDL to select the faults leading\nto errors, we can insert the faults during the DNN training process to harden\nthe DNN against such faults. Using importance sampling in FAT reduces the\noverhead required for finding faults that lead to a predetermined drop in\naccuracy by more than 12x.\n","authors":["Alessio Colucci","Andreas Steininger","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.08035v1.pdf","comment":"Under review at IJCNN2023"},{"id":"http://arxiv.org/abs/2209.07383v2","updated":"2023-03-14T16:15:21Z","published":"2022-09-15T15:47:31Z","title":"Visual Recognition with Deep Nearest Centroids","summary":"  We devise deep nearest centroids (DNC), a conceptually elegant yet\nsurprisingly effective network for large-scale visual recognition, by\nrevisiting Nearest Centroids, one of the most classic and simple classifiers.\nCurrent deep models learn the classifier in a fully parametric manner, ignoring\nthe latent data structure and lacking simplicity and explainability. DNC\ninstead conducts nonparametric, case-based reasoning; it utilizes sub-centroids\nof training samples to describe class distributions and clearly explains the\nclassification as the proximity of test data and the class sub-centroids in the\nfeature space. Due to the distance-based nature, the network output\ndimensionality is flexible, and all the learnable parameters are only for data\nembedding. That means all the knowledge learnt for ImageNet classification can\nbe completely transferred for pixel recognition learning, under the\n\"pre-training and fine-tuning\" paradigm. Apart from its nested simplicity and\nintuitive decision-making mechanism, DNC can even possess ad-hoc explainability\nwhen the sub-centroids are selected as actual training images that humans can\nview and inspect. Compared with parametric counterparts, DNC performs better on\nimage classification (CIFAR-10, ImageNet) and greatly boots pixel recognition\n(ADE20K, Cityscapes), with improved transparency and fewer learnable\nparameters, using various network architectures (ResNet, Swin) and segmentation\nmodels (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights\ninto related fields.\n","authors":["Wenguan Wang","Cheng Han","Tianfei Zhou","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2209.07383v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2211.16198v2","updated":"2023-03-14T16:13:03Z","published":"2022-11-28T16:48:41Z","title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n","authors":["Vishaal Udandarao","Ankush Gupta","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2211.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07506v3","updated":"2023-03-14T16:11:35Z","published":"2022-07-15T14:39:57Z","title":"Augmenting Softmax Information for Selective Classification with\n  Out-of-Distribution Data","summary":"  Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07506v3.pdf","comment":"ACCV 2022 (Best Paper Award)\n  https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html"},{"id":"http://arxiv.org/abs/2303.08029v1","updated":"2023-03-14T16:10:36Z","published":"2023-03-14T16:10:36Z","title":"Class-level Multiple Distributions Representation are Necessary for\n  Semantic Segmentation","summary":"  Existing approaches focus on using class-level features to improve semantic\nsegmentation performance. How to characterize the relationships of intra-class\npixels and inter-class pixels is the key to extract the discriminative\nrepresentative class-level features. In this paper, we introduce for the first\ntime to describe intra-class variations by multiple distributions. Then,\nmultiple distributions representation learning(\\textbf{MDRL}) is proposed to\naugment the pixel representations for semantic segmentation. Meanwhile, we\ndesign a class multiple distributions consistency strategy to construct\ndiscriminative multiple distribution representations of embedded pixels.\nMoreover, we put forward a multiple distribution semantic aggregation module to\naggregate multiple distributions of the corresponding class to enhance pixel\nsemantic information. Our approach can be seamlessly integrated into popular\nsegmentation frameworks FCN/PSPNet/CCNet and achieve 5.61\\%/1.75\\%/0.75\\% mIoU\nimprovements on ADE20K. Extensive experiments on the Cityscapes, ADE20K\ndatasets have proved that our method can bring significant performance\nimprovement.\n","authors":["Jianjian Yin","Zhichao Zheng","Yanhui Gu","Junsheng Zhou","Yi Chen"],"pdf_url":"https://arxiv.org/pdf/2303.08029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08010v1","updated":"2023-03-14T15:57:54Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07997v1","updated":"2023-03-14T15:48:47Z","published":"2023-03-14T15:48:47Z","title":"FingerSLAM: Closed-loop Unknown Object Localization and Reconstruction\n  from Visuo-tactile Feedback","summary":"  In this paper, we address the problem of using visuo-tactile feedback for\n6-DoF localization and 3D reconstruction of unknown in-hand objects. We propose\nFingerSLAM, a closed-loop factor graph-based pose estimator that combines local\ntactile sensing at finger-tip and global vision sensing from a wrist-mount\ncamera. FingerSLAM is constructed with two constituent pose estimators: a\nmulti-pass refined tactile-based pose estimator that captures movements from\ndetailed local textures, and a single-pass vision-based pose estimator that\npredicts from a global view of the object. We also design a loop closure\nmechanism that actively matches current vision and tactile images to previously\nstored key-frames to reduce accumulated error. FingerSLAM incorporates the two\nsensing modalities of tactile and vision, as well as the loop closure mechanism\nwith a factor graph-based optimization framework. Such a framework produces an\noptimized pose estimation solution that is more accurate than the standalone\nestimators. The estimated poses are then used to reconstruct the shape of the\nunknown object incrementally by stitching the local point clouds recovered from\ntactile images. We train our system on real-world data collected with 20\nobjects. We demonstrate reliable visuo-tactile pose estimation and shape\nreconstruction through quantitative and qualitative real-world evaluations on 6\nobjects that are unseen during training.\n","authors":["Jialiang Zhao","Maria Bauza","Edward H. Adelson"],"pdf_url":"https://arxiv.org/pdf/2303.07997v1.pdf","comment":"Submitted and accepted to 2023 IEEE International Conference on\n  Robotics and Automation (ICRA 2023)"},{"id":"http://arxiv.org/abs/2303.07989v1","updated":"2023-03-14T15:44:45Z","published":"2023-03-14T15:44:45Z","title":"A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing","summary":"  Air-writing refers to virtually writing linguistic characters through hand\ngestures in three-dimensional space with six degrees of freedom. This paper\nproposes a generic video camera-aided convolutional neural network (CNN) based\nair-writing framework. Gestures are performed using a marker of fixed color in\nfront of a generic video camera, followed by color-based segmentation to\nidentify the marker and track the trajectory of the marker tip. A pre-trained\nCNN is then used to classify the gesture. The recognition accuracy is further\nimproved using transfer learning with the newly acquired data. The performance\nof the system varies significantly on the illumination condition due to\ncolor-based segmentation. In a less fluctuating illumination condition, the\nsystem is able to recognize isolated unistroke numerals of multiple languages.\nThe proposed framework has achieved 97.7%, 95.4% and 93.7% recognition rates in\nperson independent evaluations on English, Bengali and Devanagari numerals,\nrespectively.\n","authors":["Prasun Roy","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2303.07989v1.pdf","comment":"Accepted in The International Conference on Frontiers of Handwriting\n  Recognition (ICFHR) 2018"},{"id":"http://arxiv.org/abs/2010.12669v2","updated":"2023-03-14T15:20:15Z","published":"2020-10-23T21:07:40Z","title":"Position and Rotation Invariant Sign Language Recognition from 3D Kinect\n  Data with Recurrent Neural Networks","summary":"  Sign language is a gesture-based symbolic communication medium among speech\nand hearing impaired people. It also serves as a communication bridge between\nnon-impaired and impaired populations. Unfortunately, in most situations, a\nnon-impaired person is not well conversant in such symbolic languages\nrestricting the natural information flow between these two categories.\nTherefore, an automated translation mechanism that seamlessly translates sign\nlanguage into natural language can be highly advantageous. In this paper, we\nattempt to perform recognition of 30 basic Indian sign gestures. Gestures are\nrepresented as temporal sequences of 3D maps (RGB + depth), each consisting of\n3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent\nneural network (RNN) is employed as the classifier. To improve the classifier's\nperformance, we use geometric transformation for the alignment correction of\ndepth frames. In our experiments, the model achieves 84.81% accuracy.\n","authors":["Prasun Roy","Saumik Bhattacharya","Partha Pratim Roy","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2010.12669v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.07963v1","updated":"2023-03-14T15:07:51Z","published":"2023-03-14T15:07:51Z","title":"RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning","summary":"  This paper introduces a new method for 3D point cloud registration based on\ndeep learning. The architecture is composed of three distinct blocs: (i) an\nencoder composed of a convolutional graph-based descriptor that encodes the\nimmediate neighbourhood of each point and an attention mechanism that encodes\nthe variations of the surface normals. Such descriptors are refined by\nhighlighting attention between the points of the same set and then between the\npoints of the two sets. (ii) a matching process that estimates a matrix of\ncorrespondences using the Sinkhorn algorithm. (iii) Finally, the rigid\ntransformation between the two point clouds is calculated by RANSAC using the\nKc best scores from the correspondence matrix. We conduct experiments on the\nModelNet40 dataset, and our proposed architecture shows very promising results,\noutperforming state-of-the-art methods in most of the simulated configurations,\nincluding partial overlap and data augmentation with Gaussian noise.\n","authors":["Karim Slimani","Brahim Tamadazte","Catherine Achard"],"pdf_url":"https://arxiv.org/pdf/2303.07963v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2206.04636v3","updated":"2023-03-14T15:07:16Z","published":"2022-06-09T17:34:39Z","title":"Spatial Entropy as an Inductive Bias for Vision Transformers","summary":"  Recent work on Vision Transformers (VTs) showed that introducing a local\ninductive bias in the VT architecture helps reducing the number of samples\nnecessary for training. However, the architecture modifications lead to a loss\nof generality of the Transformer backbone, partially contradicting the push\ntowards the development of uniform architectures, shared, e.g., by both the\nComputer Vision and the Natural Language Processing areas. In this work, we\npropose a different and complementary direction, in which a local bias is\nintroduced using an auxiliary self-supervised task, performed jointly with\nstandard supervised training. Specifically, we exploit the observation that the\nattention maps of VTs, when trained with self-supervision, can contain a\nsemantic segmentation structure which does not spontaneously emerge when\ntraining is supervised. Thus, we explicitly encourage the emergence of this\nspatial clustering as a form of training regularization. In more detail, we\nexploit the assumption that, in a given image, objects usually correspond to\nfew connected regions, and we propose a spatial formulation of the information\nentropy to quantify this object-based inductive bias. By minimizing the\nproposed spatial entropy, we include an additional self-supervised signal\nduring training. Using extensive experiments, we show that the proposed\nregularization leads to equivalent or better results than other VT proposals\nwhich include a local bias by changing the basic Transformer architecture, and\nit can drastically boost the VT final accuracy when using small-medium training\nsets. The code is available at https://github.com/helia95/SAR.\n","authors":["Elia Peruzzo","Enver Sangineto","Yahui Liu","Marco De Nadai","Wei Bi","Bruno Lepri","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2206.04636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00841v3","updated":"2023-03-14T15:04:33Z","published":"2022-10-03T11:57:30Z","title":"Smooth image-to-image translations with latent space interpolations","summary":"  Multi-domain image-to-image (I2I) translations can transform a source image\naccording to the style of a target domain. One important, desired\ncharacteristic of these transformations, is their graduality, which corresponds\nto a smooth change between the source and the target image when their\nrespective latent-space representations are linearly interpolated. However,\nstate-of-the-art methods usually perform poorly when evaluated using\ninter-domain interpolations, often producing abrupt changes in the appearance\nor non-realistic intermediate images. In this paper, we argue that one of the\nmain reasons behind this problem is the lack of sufficient inter-domain\ntraining data and we propose two different regularization methods to alleviate\nthis issue: a new shrinkage loss, which compacts the latent space, and a Mixup\ndata-augmentation strategy, which flattens the style representations between\ndomains. We also propose a new metric to quantitatively evaluate the degree of\nthe interpolation smoothness, an aspect which is not sufficiently covered by\nthe existing I2I translation metrics. Using both our proposed metric and\nstandard evaluation protocols, we show that our regularization techniques can\nimprove the state-of-the-art multi-domain I2I translations by a large margin.\nOur code will be made publicly available upon the acceptance of this article.\n","authors":["Yahui Liu","Enver Sangineto","Yajing Chen","Linchao Bao","Haoxian Zhang","Nicu Sebe","Bruno Lepri","Marco De Nadai"],"pdf_url":"https://arxiv.org/pdf/2210.00841v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06274v2","updated":"2023-03-14T14:53:19Z","published":"2023-03-11T01:21:13Z","title":"CoNIC Challenge: Pushing the Frontiers of Nuclear Detection,\n  Segmentation, Classification and Counting","summary":"  Nuclear detection, segmentation and morphometric profiling are essential in\nhelping us further understand the relationship between histology and patient\noutcome. To drive innovation in this area, we setup a community-wide challenge\nusing the largest available dataset of its kind to assess nuclear segmentation\nand cellular composition. Our challenge, named CoNIC, stimulated the\ndevelopment of reproducible algorithms for cellular recognition with real-time\nresult inspection on public leaderboards. We conducted an extensive\npost-challenge analysis based on the top-performing models using 1,658\nwhole-slide images of colon tissue. With around 700 million detected nuclei per\nmodel, associated features were used for dysplasia grading and survival\nanalysis, where we demonstrated that the challenge's improvement over the\nprevious state-of-the-art led to significant boosts in downstream performance.\nOur findings also suggest that eosinophils and neutrophils play an important\nrole in the tumour microevironment. We release challenge models and WSI-level\nresults to foster the development of further methods for biomarker discovery.\n","authors":["Simon Graham","Quoc Dang Vu","Mostafa Jahanifar","Martin Weigert","Uwe Schmidt","Wenhua Zhang","Jun Zhang","Sen Yang","Jinxi Xiang","Xiyue Wang","Josef Lorenz Rumberger","Elias Baumann","Peter Hirsch","Lihao Liu","Chenyang Hong","Angelica I. Aviles-Rivero","Ayushi Jain","Heeyoung Ahn","Yiyu Hong","Hussam Azzuni","Min Xu","Mohammad Yaqub","Marie-Claire Blache","Benoît Piégu","Bertrand Vernay","Tim Scherr","Moritz Böhland","Katharina Löffler","Jiachen Li","Weiqin Ying","Chixin Wang","Dagmar Kainmueller","Carola-Bibiane Schönlieb","Shuolin Liu","Dhairya Talsania","Yughender Meda","Prakash Mishra","Muhammad Ridzuan","Oliver Neumann","Marcel P. Schilling","Markus Reischl","Ralf Mikut","Banban Huang","Hsiang-Chin Chien","Ching-Ping Wang","Chia-Yen Lee","Hong-Kun Lin","Zaiyi Liu","Xipeng Pan","Chu Han","Jijun Cheng","Muhammad Dawood","Srijay Deshpande","Raja Muhammad Saad Bashir","Adam Shephard","Pedro Costa","João D. Nunes","Aurélio Campilho","Jaime S. Cardoso","Hrishikesh P S","Densen Puthussery","Devika R G","Jiji C V","Ye Zhang","Zijie Fang","Zhifan Lin","Yongbing Zhang","Chunhui Lin","Liukun Zhang","Lijian Mao","Min Wu","Vi Thi-Tuong Vo","Soo-Hyung Kim","Taebum Lee","Satoshi Kondo","Satoshi Kasai","Pranay Dumbhare","Vedant Phuse","Yash Dubey","Ankush Jamthikar","Trinh Thi Le Vuong","Jin Tae Kwak","Dorsa Ziaei","Hyun Jung","Tianyi Miao","David Snead","Shan E Ahmed Raza","Fayyaz Minhas","Nasir M. Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2303.06274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07951v1","updated":"2023-03-14T14:49:52Z","published":"2023-03-14T14:49:52Z","title":"MetaMixer: A Regularization Strategy for Online Knowledge Distillation","summary":"  Online knowledge distillation (KD) has received increasing attention in\nrecent years. However, while most existing online KD methods focus on\ndeveloping complicated model structures and training strategies to improve the\ndistillation of high-level knowledge like probability distribution, the effects\nof the multi-level knowledge in the online KD are greatly overlooked,\nespecially the low-level knowledge. Thus, to provide a novel viewpoint to\nonline KD, we propose MetaMixer, a regularization strategy that can strengthen\nthe distillation by combining the low-level knowledge that impacts the\nlocalization capability of the networks, and high-level knowledge that focuses\non the whole image. Experiments under different conditions show that MetaMixer\ncan achieve significant performance gains over state-of-the-art methods.\n","authors":["Maorong Wang","Ling Xiao","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2303.07951v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.07945v1","updated":"2023-03-14T14:35:59Z","published":"2023-03-14T14:35:59Z","title":"Edit-A-Video: Single Video Editing with Object-Aware Consistency","summary":"  Despite the fact that text-to-video (TTV) model has recently achieved\nremarkable success, there have been few approaches on TTV for its extension to\nvideo editing. Motivated by approaches on TTV models adapting from\ndiffusion-based text-to-image (TTI) models, we suggest the video editing\nframework given only a pretrained TTI model and a single <text, video> pair,\nwhich we term Edit-A-Video. The framework consists of two stages: (1) inflating\nthe 2D model into the 3D model by appending temporal modules and tuning on the\nsource video (2) inverting the source video into the noise and editing with\ntarget text prompt and attention map injection. Each stage enables the temporal\nmodeling and preservation of semantic attributes of the source video. One of\nthe key challenges for video editing include a background inconsistency\nproblem, where the regions not included for the edit suffer from undesirable\nand inconsistent temporal alterations. To mitigate this issue, we also\nintroduce a novel mask blending method, termed as sparse-causal blending (SC\nBlending). We improve previous mask blending methods to reflect the temporal\nconsistency so that the area where the editing is applied exhibits smooth\ntransition while also achieving spatio-temporal consistency of the unedited\nregions. We present extensive experimental results over various types of text\nand videos, and demonstrate the superiority of the proposed method compared to\nbaselines in terms of background consistency, text alignment, and video editing\nquality.\n","authors":["Chaehun Shin","Heeseung Kim","Che Hyun Lee","Sang-gil Lee","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.07945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07944v1","updated":"2023-03-14T14:34:51Z","published":"2023-03-14T14:34:51Z","title":"Non-Contrastive Unsupervised Learning of Physiological Signals from\n  Video","summary":"  Subtle periodic signals such as blood volume pulse and respiration can be\nextracted from RGB video, enabling remote health monitoring at low cost.\nAdvancements in remote pulse estimation -- or remote photoplethysmography\n(rPPG) -- are currently driven by deep learning solutions. However, modern\napproaches are trained and evaluated on benchmark datasets with associated\nground truth from contact-PPG sensors. We present the first non-contrastive\nunsupervised learning framework for signal regression to break free from the\nconstraints of labelled video data. With minimal assumptions of periodicity and\nfinite bandwidth, our approach is capable of discovering the blood volume pulse\ndirectly from unlabelled videos. We find that encouraging sparse power spectra\nwithin normal physiological bandlimits and variance over batches of power\nspectra is sufficient for learning visual features of periodic signals. We\nperform the first experiments utilizing unlabelled video data not specifically\ncreated for rPPG to train robust pulse rate estimators. Given the limited\ninductive biases and impressive empirical results, the approach is\ntheoretically capable of discovering other periodic signals from video,\nenabling multiple physiological measurements without the need for ground truth\nsignals. Codes to fully reproduce the experiments are made available along with\nthe paper.\n","authors":["Jeremy Speth","Nathan Vance","Patrick Flynn","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2303.07944v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07938v1","updated":"2023-03-14T14:25:29Z","published":"2023-03-14T14:25:29Z","title":"Controllable Mesh Generation Through Sparse Latent Point Diffusion\n  Models","summary":"  Mesh generation is of great value in various applications involving computer\ngraphics and virtual content, yet designing generative models for meshes is\nchallenging due to their irregular data structure and inconsistent topology of\nmeshes in the same category. In this work, we design a novel sparse latent\npoint diffusion model for mesh generation. Our key insight is to regard point\nclouds as an intermediate representation of meshes, and model the distribution\nof point clouds instead. While meshes can be generated from point clouds via\ntechniques like Shape as Points (SAP), the challenges of directly generating\nmeshes can be effectively avoided. To boost the efficiency and controllability\nof our mesh generation method, we propose to further encode point clouds to a\nset of sparse latent points with point-wise semantic meaningful features, where\ntwo DDPMs are trained in the space of sparse latent points to respectively\nmodel the distribution of the latent point positions and features at these\nlatent points. We find that sampling in this latent space is faster than\ndirectly sampling dense point clouds. Moreover, the sparse latent points also\nenable us to explicitly control both the overall structures and local details\nof the generated meshes. Extensive experiments are conducted on the ShapeNet\ndataset, where our proposed sparse latent point diffusion model achieves\nsuperior performance in terms of generation quality and controllability when\ncompared to existing methods.\n","authors":["Zhaoyang Lyu","Jinyi Wang","Yuwei An","Ya Zhang","Dahua Lin","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2303.07938v1.pdf","comment":"Accepted to CVPR 2023. Project page is at https://slide-3d.github.io"},{"id":"http://arxiv.org/abs/2303.02489v2","updated":"2023-03-14T14:25:04Z","published":"2023-03-04T19:53:00Z","title":"CapDet: Unifying Dense Captioning and Open-World Detection Pretraining","summary":"  Benefiting from large-scale vision-language pre-training on image-text pairs,\nopen-world detection methods have shown superior generalization ability under\nthe zero-shot or few-shot detection settings. However, a pre-defined category\nspace is still required during the inference stage of existing methods and only\nthe objects belonging to that space will be predicted. To introduce a \"real\"\nopen-world detector, in this paper, we propose a novel method named CapDet to\neither predict under a given category list or directly generate the category of\npredicted bounding boxes. Specifically, we unify the open-world detection and\ndense caption tasks into a single yet effective framework by introducing an\nadditional dense captioning head to generate the region-grounded captions.\nBesides, adding the captioning task will in turn benefit the generalization of\ndetection performance since the captioning dataset covers more concepts.\nExperiment results show that by unifying the dense caption task, our CapDet has\nobtained significant performance improvements (e.g., +2.1% mAP on LVIS rare\nclasses) over the baseline method on LVIS (1203 classes). Besides, our CapDet\nalso achieves state-of-the-art performance on dense captioning tasks, e.g.,\n15.44% mAP on VG V1.2 and 13.98% on the VG-COCO dataset.\n","authors":["Yanxin Long","Youpeng Wen","Jianhua Han","Hang Xu","Pengzhen Ren","Wei Zhang","Shen Zhao","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2303.02489v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07937v1","updated":"2023-03-14T14:24:31Z","published":"2023-03-14T14:24:31Z","title":"Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D\n  Generation","summary":"  Text-to-3D generation has shown rapid progress in recent days with the advent\nof score distillation, a methodology of using pretrained text-to-2D diffusion\nmodels to optimize neural radiance field (NeRF) in the zero-shot setting.\nHowever, the lack of 3D awareness in the 2D diffusion models destabilizes score\ndistillation-based methods from reconstructing a plausible 3D scene. To address\nthis issue, we propose \\ours, a novel framework that incorporates 3D awareness\ninto pretrained 2D diffusion models, enhancing the robustness and 3D\nconsistency of score distillation-based methods. We realize this by first\nconstructing a coarse 3D structure of a given text prompt and then utilizing\nprojected, view-specific depth map as a condition for the diffusion model.\nAdditionally, we introduce a training strategy that enables the 2D diffusion\nmodel learns to handle the errors and sparsity within the coarse 3D structure\nfor robust generation, as well as a method for ensuring semantic consistency\nthroughout all viewpoints of the scene. Our framework surpasses the limitations\nof prior arts, and has significant implications for 3D consistent generation of\n2D diffusion models.\n","authors":["Junyoung Seo","Wooseok Jang","Min-Seop Kwak","Jaehoon Ko","Hyeonsu Kim","Junho Kim","Jin-Hwa Kim","Jiyoung Lee","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.07937v1.pdf","comment":"Project page https://ku-cvlab.github.io/3DFuse/"},{"id":"http://arxiv.org/abs/2303.07929v1","updated":"2023-03-14T14:12:33Z","published":"2023-03-14T14:12:33Z","title":"DAA: A Delta Age AdaIN operation for age estimation via binary code\n  transformer","summary":"  Naked eye recognition of age is usually based on comparison with the age of\nothers. However, this idea is ignored by computer tasks because it is difficult\nto obtain representative contrast images of each age. Inspired by the transfer\nlearning, we designed the Delta Age AdaIN (DAA) operation to obtain the feature\ndifference with each age, which obtains the style map of each age through the\nlearned values representing the mean and standard deviation. We let the input\nof transfer learning as the binary code of age natural number to obtain\ncontinuous age feature information. The learned two groups of values in Binary\ncode mapping are corresponding to the mean and standard deviation of the\ncomparison ages. In summary, our method consists of four parts: FaceEncoder,\nDAA operation, Binary code mapping, and AgeDecoder modules. After getting the\ndelta age via AgeDecoder, we take the average value of all comparison ages and\ndelta ages as the predicted age. Compared with state-of-the-art methods, our\nmethod achieves better performance with fewer parameters on multiple facial age\ndatasets.\n","authors":["Ping Chen","Xingpeng Zhang","Ye Li","Ju Tao","Bin Xiao","Bing Wang","Zongjie Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07929v1.pdf","comment":"Accepted by CVPR2023; 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.07910v1","updated":"2023-03-14T13:50:31Z","published":"2023-03-14T13:50:31Z","title":"Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm","summary":"  Parameter-Efficient Transfer Learning (PETL) aims at efficiently adapting\nlarge models pre-trained on massive data to downstream tasks with limited\ntask-specific data. In view of the practicality of PETL, previous works focus\non tuning a small set of parameters for each downstream task in an end-to-end\nmanner while rarely considering the task distribution shift issue between the\npre-training task and the downstream task. This paper proposes a novel\ntwo-stage paradigm, where the pre-trained model is first aligned to the target\ndistribution. Then the task-relevant information is leveraged for effective\nadaptation. Specifically, the first stage narrows the task distribution shift\nby tuning the scale and shift in the LayerNorm layers. In the second stage, to\nefficiently learn the task-relevant information, we propose a Taylor\nexpansion-based importance score to identify task-relevant channels for the\ndownstream task and then only tune such a small portion of channels, making the\nadaptation to be parameter-efficient. Overall, we present a promising new\ndirection for PETL, and the proposed paradigm achieves state-of-the-art\nperformance on the average accuracy of 19 downstream tasks.\n","authors":["Hengyuan Zhao","Hao Luo","Yuyang Zhao","Pichao Wang","Fan Wang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2303.07910v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.07909v1","updated":"2023-03-14T13:49:54Z","published":"2023-03-14T13:49:54Z","title":"Text-to-image Diffusion Model in Generative AI: A Survey","summary":"  This survey reviews text-to-image diffusion models in the context that\ndiffusion models have emerged to be popular for a wide range of generative\ntasks. As a self-contained work, this survey starts with a brief introduction\nof how a basic diffusion model works for image synthesis, followed by how\ncondition or guidance improves learning. Based on that, we present a review of\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\ntext-to-image. We further summarize applications beyond text-to-image\ngeneration: text-guided creative generation and text-guided image editing.\nBeyond the progress made so far, we discuss existing challenges and promising\nfuture directions.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Mengchun Zhang","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.07909v1.pdf","comment":"First survey on the recent progress of text-to-image generation based\n  on the diffusion model (under progress)"},{"id":"http://arxiv.org/abs/2303.07898v1","updated":"2023-03-14T13:36:36Z","published":"2023-03-14T13:36:36Z","title":"AutoEnsemble: Automated Ensemble Search Framework for Semantic\n  Segmentation Using Image Labels","summary":"  A key bottleneck of employing state-of-the-art semantic segmentation networks\nin the real world is the availability of training labels. Standard semantic\nsegmentation networks require massive pixel-wise annotated labels to reach\nstate-of-the-art prediction quality. Hence, several works focus on semantic\nsegmentation networks trained with only image-level annotations. However, when\nscrutinizing the state-of-the-art results in more detail, we notice that\nalthough they are very close to each other on average prediction quality,\ndifferent approaches perform better in different classes while providing low\nquality in others. To address this problem, we propose a novel framework,\nAutoEnsemble, which employs an ensemble of the \"pseudo-labels\" for a given set\nof different segmentation techniques on a class-wise level. Pseudo-labels are\nthe pixel-wise predictions of the image-level semantic segmentation frameworks\nused to train the final segmentation model. Our pseudo-labels seamlessly\ncombine the strong points of multiple segmentation techniques approaches to\nreach superior prediction quality. We reach up to 2.4% improvement over\nAutoEnsemble's components. An exhaustive analysis was performed to demonstrate\nAutoEnsemble's effectiveness over state-of-the-art frameworks for image-level\nsemantic segmentation.\n","authors":["Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07898v1.pdf","comment":"This paper is submitted to a IEEE conference for peer review\n  publication"},{"id":"http://arxiv.org/abs/2303.07896v1","updated":"2023-03-14T13:31:05Z","published":"2023-03-14T13:31:05Z","title":"Automated Ensemble Search Framework for Semantic Segmentation Using\n  Medical Imaging Labels","summary":"  Reliable classification and detection of certain medical conditions, in\nimages, with state-of-the-art semantic segmentation networks, require vast\namounts of pixel-wise annotation. However, the public availability of such\ndatasets is minimal. Therefore, semantic segmentation with image-level labels\npresents a promising alternative to this problem. Nevertheless, very few works\nhave focused on evaluating this technique and its applicability to the medical\nsector. Due to their complexity and the small number of training examples in\nmedical datasets, classifier-based weakly supervised networks like class\nactivation maps (CAMs) struggle to extract useful information from them.\nHowever, most state-of-the-art approaches rely on them to achieve their\nimprovements. Therefore, we propose a framework that can still utilize the\nlow-quality CAM predictions of complicated datasets to improve the accuracy of\nour results. Our framework achieves that by first utilizing lower threshold\nCAMs to cover the target object with high certainty; second, by combining\nmultiple low-threshold CAMs that even out their errors while highlighting the\ntarget object. We performed exhaustive experiments on the popular multi-modal\nBRATS and prostate DECATHLON segmentation challenge datasets. Using the\nproposed framework, we have demonstrated an improved dice score of up to 8% on\nBRATS and 6% on DECATHLON datasets compared to the previous state-of-the-art.\n","authors":["Erik Ostrowski","Bharath Srinivas Prabakaran","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07892v1","updated":"2023-03-14T13:25:55Z","published":"2023-03-14T13:25:55Z","title":"Image Label based Semantic Segmentation Framework using Object\n  Perimeters","summary":"  Achieving high-quality semantic segmentation predictions using only\nimage-level labels enables a new level of real-world applicability. Although\nstate-of-the-art networks deliver reliable predictions, the amount of\nhandcrafted pixel-wise annotations to enable these results are not feasible in\nmany real-world applications. Hence, several works have already targeted this\nbottleneck, using classifier-based networks like Class Activation Maps (CAMs)\nas a base. Addressing CAM's weaknesses of fuzzy borders and incomplete\npredictions, state-of-the-art approaches rely only on adding regulations to the\nclassifier loss or using pixel-similarity-based refinement after the fact. We\npropose a framework that introduces an additional module using object\nperimeters for improved saliency. We define object perimeter information as the\nline separating the object and background. Our new PerimeterFit module will be\napplied to pre-refine the CAM predictions before using the\npixel-similarity-based network. In this way, our PerimeterFit increases the\nquality of the CAM prediction while simultaneously improving the false negative\nrate. We investigated a wide range of state-of-the-art unsupervised semantic\nsegmentation networks and edge detection techniques to create useful perimeter\nmaps, which enable our framework to predict object locations with sharper\nperimeters. We achieved up to 1.5\\% improvement over frameworks without our\nPerimeterFit module. We conduct an exhaustive analysis to illustrate that our\nframework enhances existing state-of-the-art frameworks for image-level-based\nsemantic segmentation. The framework is open-source and accessible online at\nhttps://github.com/ErikOstrowski/Perimeter-based-Semantic-Segmentation.\n","authors":["Erik Ostrowski","Bharath Srinivas Prabakaran","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03595v2","updated":"2023-03-14T13:15:04Z","published":"2023-03-07T02:00:34Z","title":"LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global\n  Cross-Modal Fusion","summary":"  LiDAR-camera fusion methods have shown impressive performance in 3D object\ndetection. Recent advanced multi-modal methods mainly perform global fusion,\nwhere image features and point cloud features are fused across the whole scene.\nSuch practice lacks fine-grained region-level information, yielding suboptimal\nfusion performance. In this paper, we present the novel Local-to-Global fusion\nnetwork (LoGoNet), which performs LiDAR-camera fusion at both local and global\nlevels. Concretely, the Global Fusion (GoF) of LoGoNet is built upon previous\nliterature, while we exclusively use point centroids to more precisely\nrepresent the position of voxel features, thus achieving better cross-modal\nalignment. As to the Local Fusion (LoF), we first divide each proposal into\nuniform grids and then project these grid centers to the images. The image\nfeatures around the projected grid points are sampled to be fused with\nposition-decorated point cloud features, maximally utilizing the rich\ncontextual information around the proposals. The Feature Dynamic Aggregation\n(FDA) module is further proposed to achieve information interaction between\nthese locally and globally fused features, thus producing more informative\nmulti-modal features. Extensive experiments on both Waymo Open Dataset (WOD)\nand KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D\ndetection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection\nleaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy\nthat, for the first time, the detection performance on three classes surpasses\n80 APH (L2) simultaneously. Code will be available at\n\\url{https://github.com/sankin97/LoGoNet}.\n","authors":["Xin Li","Tao Ma","Yuenan Hou","Botian Shi","Yuchen Yang","Youquan Liu","Xingjiao Wu","Qin Chen","Yikang Li","Yu Qiao","Liang He"],"pdf_url":"https://arxiv.org/pdf/2303.03595v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.16762v3","updated":"2023-03-14T13:07:50Z","published":"2022-11-30T06:02:01Z","title":"GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided\n  Distance Representation","summary":"  We present a learning-based method, namely GeoUDF,to tackle the long-standing\nand challenging problem of reconstructing a discrete surface from a sparse\npoint cloud.To be specific, we propose a geometry-guided learning method for\nUDF and its gradient estimation that explicitly formulates the unsigned\ndistance of a query point as the learnable affine averaging of its distances to\nthe tangent planes of neighboring points on the surface. Besides,we model the\nlocal geometric structure of the input point clouds by explicitly learning a\nquadratic polynomial for each point. This not only facilitates upsampling the\ninput sparse point cloud but also naturally induces unoriented normal, which\nfurther augments UDF estimation. Finally, to extract triangle meshes from the\npredicted UDF we propose a customized edge-based marching cube module. We\nconduct extensive experiments and ablation studies to demonstrate the\nsignificant advantages of our method over state-of-the-art methods in terms of\nreconstruction accuracy, efficiency, and generality. The source code is\npublicly available at https://github.com/rsy6318/GeoUDF.\n","authors":["Siyu Ren","Junhui Hou","Xiaodong Chen","Ying He","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2211.16762v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03729v2","updated":"2023-03-14T13:07:09Z","published":"2023-03-07T08:37:48Z","title":"Learning Discriminative Representations for Skeleton Based Action\n  Recognition","summary":"  Human action recognition aims at classifying the category of human action\nfrom a segment of a video. Recently, people have dived into designing GCN-based\nmodels to extract features from skeletons for performing this task, because\nskeleton representations are much more efficient and robust than other\nmodalities such as RGB frames. However, when employing the skeleton data, some\nimportant clues like related items are also discarded. It results in some\nambiguous actions that are hard to be distinguished and tend to be\nmisclassified. To alleviate this problem, we propose an auxiliary feature\nrefinement head (FR Head), which consists of spatial-temporal decoupling and\ncontrastive feature refinement, to obtain discriminative representations of\nskeletons. Ambiguous samples are dynamically discovered and calibrated in the\nfeature space. Furthermore, FR Head could be imposed on different stages of\nGCNs to build a multi-level refinement for stronger supervision. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.\nOur proposed models obtain competitive results from state-of-the-art methods\nand can help to discriminate those ambiguous samples. Codes are available at\nhttps://github.com/zhysora/FR-Head.\n","authors":["Huanyu Zhou","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03729v2.pdf","comment":"Accepted by CVPR2023. 10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.07868v1","updated":"2023-03-14T13:01:25Z","published":"2023-03-14T13:01:25Z","title":"DynaMask: Dynamic Mask Selection for Instance Segmentation","summary":"  The representative instance segmentation methods mostly segment different\nobject instances with a mask of the fixed resolution, e.g., 28*28 grid.\nHowever, a low-resolution mask loses rich details, while a high-resolution mask\nincurs quadratic computation overhead. It is a challenging task to predict the\noptimal binary mask for each instance. In this paper, we propose to dynamically\nselect suitable masks for different object proposals. First, a dual-level\nFeature Pyramid Network (FPN) with adaptive feature aggregation is developed to\ngradually increase the mask grid resolution, ensuring high-quality segmentation\nof objects. Specifically, an efficient region-level top-down path (r-FPN) is\nintroduced to incorporate complementary contextual and detailed information\nfrom different stages of image-level FPN (i-FPN). Then, to alleviate the\nincrease of computation and memory costs caused by using large masks, we\ndevelop a Mask Switch Module (MSM) with negligible computational cost to select\nthe most suitable mask resolution for each instance, achieving high efficiency\nwhile maintaining high segmentation accuracy. Without bells and whistles, the\nproposed method, namely DynaMask, brings consistent and noticeable performance\nimprovements over other state-of-the-arts at a moderate computation overhead.\nThe source code: https://github.com/lslrh/DynaMask.\n","authors":["Ruihuang Li","Chenhang He","Shuai Li","Yabin Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07868v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2211.16927v2","updated":"2023-03-14T12:56:57Z","published":"2022-11-30T11:57:45Z","title":"3D GAN Inversion with Facial Symmetry Prior","summary":"  Recently, a surge of high-quality 3D-aware GANs have been proposed, which\nleverage the generative power of neural rendering. It is natural to associate\n3D GANs with GAN inversion methods to project a real image into the generator's\nlatent space, allowing free-view consistent synthesis and editing, referred as\n3D GAN inversion. Although with the facial prior preserved in pre-trained 3D\nGANs, reconstructing a 3D portrait with only one monocular image is still an\nill-pose problem. The straightforward application of 2D GAN inversion methods\nfocuses on texture similarity only while ignoring the correctness of 3D\ngeometry shapes. It may raise geometry collapse effects, especially when\nreconstructing a side face under an extreme pose. Besides, the synthetic\nresults in novel views are prone to be blurry. In this work, we propose a novel\nmethod to promote 3D GAN inversion by introducing facial symmetry prior. We\ndesign a pipeline and constraints to make full use of the pseudo auxiliary view\nobtained via image flipping, which helps obtain a robust and reasonable\ngeometry shape during the inversion process. To enhance texture fidelity in\nunobserved viewpoints, pseudo labels from depth-guided 3D warping can provide\nextra supervision. We design constraints aimed at filtering out conflict areas\nfor optimization in asymmetric situations. Comprehensive quantitative and\nqualitative evaluations on image reconstruction and editing demonstrate the\nsuperiority of our method.\n","authors":["Fei Yin","Yong Zhang","Xuan Wang","Tengfei Wang","Xiaoyu Li","Yuan Gong","Yanbo Fan","Xiaodong Cun","Ying Shan","Cengiz Oztireli","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2211.16927v2.pdf","comment":"Project Page is at https://feiiyin.github.io/SPI/"},{"id":"http://arxiv.org/abs/2303.07863v1","updated":"2023-03-14T12:53:27Z","published":"2023-03-14T12:53:27Z","title":"You Can Ground Earlier than See: An Effective and Efficient Pipeline for\n  Temporal Sentence Grounding in Compressed Videos","summary":"  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.\n","authors":["Xiang Fang","Daizong Liu","Pan Zhou","Guoshun Nan"],"pdf_url":"https://arxiv.org/pdf/2303.07863v1.pdf","comment":"Accepted by CVPR-23"},{"id":"http://arxiv.org/abs/2303.07853v1","updated":"2023-03-14T12:46:52Z","published":"2023-03-14T12:46:52Z","title":"BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised\n  Semantic Segmentation of Medical Images","summary":"  Weakly Supervised Semantic Segmentation (WSSS) with only image-level\nsupervision is a promising approach to deal with the need for Segmentation\nnetworks, especially for generating a large number of pixel-wise masks in a\ngiven dataset. However, most state-of-the-art image-level WSSS techniques lack\nan understanding of the geometric features embedded in the images since the\nnetwork cannot derive any object boundary information from just image-level\nlabels. We define a boundary here as the line separating an object and its\nbackground, or two different objects. To address this drawback, we propose our\nnovel BoundaryCAM framework, which deploys state-of-the-art class activation\nmaps combined with various post-processing techniques in order to achieve\nfine-grained higher-accuracy segmentation masks. To achieve this, we\ninvestigate a state-of-the-art unsupervised semantic segmentation network that\ncan be used to construct a boundary map, which enables BoundaryCAM to predict\nobject locations with sharper boundaries. By applying our method to WSSS\npredictions, we were able to achieve up to 10% improvements even to the benefit\nof the current state-of-the-art WSSS methods for medical imaging. The framework\nis open-source and accessible online at\nhttps://github.com/bharathprabakaran/BoundaryCAM.\n","authors":["Bharath Srinivas Prabakaran","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07852v1","updated":"2023-03-14T12:46:48Z","published":"2023-03-14T12:46:48Z","title":"FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network\n  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features","summary":"  Ultrasound imaging is one of the most prominent technologies to evaluate the\ngrowth, progression, and overall health of a fetus during its gestation.\nHowever, the interpretation of the data obtained from such studies is best left\nto expert physicians and technicians who are trained and well-versed in\nanalyzing such images. To improve the clinical workflow and potentially develop\nan at-home ultrasound-based fetal monitoring platform, we present a novel fetus\nphantom ultrasound dataset, FPUS23, which can be used to identify (1) the\ncorrect diagnostic planes for estimating fetal biometric values, (2) fetus\norientation, (3) their anatomical features, and (4) bounding boxes of the fetus\nphantom anatomies at 23 weeks gestation. The entire dataset is composed of\n15,728 images, which are used to train four different Deep Neural Network\nmodels, built upon a ResNet34 backbone, for detecting aforementioned fetus\nfeatures and use-cases. We have also evaluated the models trained using our\nFPUS23 dataset, to show that the information learned by these models can be\nused to substantially increase the accuracy on real-world ultrasound fetus\ndatasets. We make the FPUS23 dataset and the pre-trained models publicly\naccessible at https://github.com/bharathprabakaran/FPUS23, which will further\nfacilitate future research on fetal ultrasound imaging and analysis.\n","authors":["Bharath Srinivas Prabakaran","Paul Hamelmann","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07849v1","updated":"2023-03-14T12:41:56Z","published":"2023-03-14T12:41:56Z","title":"Implicit Stacked Autoregressive Model for Video Prediction","summary":"  Future frame prediction has been approached through two primary methods:\nautoregressive and non-autoregressive. Autoregressive methods rely on the\nMarkov assumption and can achieve high accuracy in the early stages of\nprediction when errors are not yet accumulated. However, their performance\ntends to decline as the number of time steps increases. In contrast,\nnon-autoregressive methods can achieve relatively high performance but lack\ncorrelation between predictions for each time step. In this paper, we propose\nan Implicit Stacked Autoregressive Model for Video Prediction (IAM4VP), which\nis an implicit video prediction model that applies a stacked autoregressive\nmethod. Like non-autoregressive methods, stacked autoregressive methods use the\nsame observed frame to estimate all future frames. However, they use their own\npredictions as input, similar to autoregressive methods. As the number of time\nsteps increases, predictions are sequentially stacked in the queue. To evaluate\nthe effectiveness of IAM4VP, we conducted experiments on three common future\nframe prediction benchmark datasets and weather\\&climate prediction benchmark\ndatasets. The results demonstrate that our proposed model achieves\nstate-of-the-art performance.\n","authors":["Minseok Seo","Hakjin Lee","Doyi Kim","Junghoon Seo"],"pdf_url":"https://arxiv.org/pdf/2303.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05183v2","updated":"2023-03-14T12:41:01Z","published":"2023-03-09T11:21:59Z","title":"Blind2Sound: Self-Supervised Image Denoising without Residual Noise","summary":"  Self-supervised blind denoising for Poisson-Gaussian noise remains a\nchallenging task. Pseudo-supervised pairs constructed from single noisy images\nre-corrupt the signal and degrade the performance. The visible blindspots solve\nthe information loss in masked inputs. However, without explicitly noise\nsensing, mean square error as an objective function cannot adjust denoising\nintensities for dynamic noise levels, leading to noticeable residual noise. In\nthis paper, we propose Blind2Sound, a simple yet effective approach to overcome\nresidual noise in denoised images. The proposed adaptive re-visible loss senses\nnoise levels and performs personalized denoising without noise residues while\nretaining the signal lossless. The theoretical analysis of intermediate medium\ngradients guarantees stable training, while the Cramer Gaussian loss acts as a\nregularization to facilitate the accurate perception of noise levels and\nimprove the performance of the denoiser. Experiments on synthetic and\nreal-world datasets show the superior performance of our method, especially for\nsingle-channel images.\n","authors":["Zejin Wang","Jiazheng Liu","Hao Zhai","Hua Han"],"pdf_url":"https://arxiv.org/pdf/2303.05183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07840v1","updated":"2023-03-14T12:26:48Z","published":"2023-03-14T12:26:48Z","title":"Precise Facial Landmark Detection by Reference Heatmap Transformer","summary":"  Most facial landmark detection methods predict landmarks by mapping the input\nfacial appearance features to landmark heatmaps and have achieved promising\nresults. However, when the face image is suffering from large poses, heavy\nocclusions and complicated illuminations, they cannot learn discriminative\nfeature representations and effective facial shape constraints, nor can they\naccurately predict the value of each element in the landmark heatmap, limiting\ntheir detection accuracy. To address this problem, we propose a novel Reference\nHeatmap Transformer (RHT) by introducing reference heatmap information for more\nprecise facial landmark detection. The proposed RHT consists of a Soft\nTransformation Module (STM) and a Hard Transformation Module (HTM), which can\ncooperate with each other to encourage the accurate transformation of the\nreference heatmap information and facial shape constraints. Then, a Multi-Scale\nFeature Fusion Module (MSFFM) is proposed to fuse the transformed heatmap\nfeatures and the semantic features learned from the original face images to\nenhance feature representations for producing more accurate target heatmaps. To\nthe best of our knowledge, this is the first study to explore how to enhance\nfacial landmark detection by transforming the reference heatmap information.\nThe experimental results from challenging benchmark datasets demonstrate that\nour proposed method outperforms the state-of-the-art methods in the literature.\n","authors":["Jun Wan","Jun Liu","Jie Zhou","Zhihui Lai","Linlin Shen","Hang Sun","Ping Xiong","Wenwen Min"],"pdf_url":"https://arxiv.org/pdf/2303.07840v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing, March 2023"},{"id":"http://arxiv.org/abs/2302.02314v2","updated":"2023-03-14T12:12:00Z","published":"2023-02-05T06:27:45Z","title":"CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image\n  Classification","summary":"  Most computer vision models are developed based on either convolutional\nneural network (CNN) or transformer, while the former (latter) method captures\nlocal (global) features. To relieve model performance limitations due to the\nlack of global (local) features, we develop a novel classification network CECT\nby controllable ensemble CNN and transformer. CECT is composed of a\nconvolutional encoder block, a transposed-convolutional decoder block, and a\ntransformer classification block. Different from conventional CNN- or\ntransformer-based methods, our CECT can capture features at both multi-local\nand global scales. Besides, the contribution of local features at different\nscales can be controlled with the proposed ensemble coefficients. We evaluate\nCECT on two public COVID-19 datasets and it outperforms existing\nstate-of-the-art methods on all evaluation metrics. With remarkable feature\ncapture ability, we believe CECT can be extended to other medical image\nclassification scenarios as a diagnosis assistant.\n","authors":["Zhaoshan Liu","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2302.02314v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.09100v2","updated":"2023-03-14T12:08:11Z","published":"2022-12-18T14:56:22Z","title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input\n  Images","summary":"  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset will be made public with the code and\nmodels on the project website https://abdullahamdi.com/sparf/ .\n","authors":["Abdullah Hamdi","Bernard Ghanem","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2212.09100v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.07831v1","updated":"2023-03-14T12:07:48Z","published":"2023-03-14T12:07:48Z","title":"Quaternion Orthogonal Transformer for Facial Expression Recognition in\n  the Wild","summary":"  Facial expression recognition (FER) is a challenging topic in artificial\nintelligence. Recently, many researchers have attempted to introduce Vision\nTransformer (ViT) to the FER task. However, ViT cannot fully utilize emotional\nfeatures extracted from raw images and requires a lot of computing resources.\nTo overcome these problems, we propose a quaternion orthogonal transformer\n(QOT) for FER. Firstly, to reduce redundancy among features extracted from\npre-trained ResNet-50, we use the orthogonal loss to decompose and compact\nthese features into three sets of orthogonal sub-features. Secondly, three\northogonal sub-features are integrated into a quaternion matrix, which\nmaintains the correlations between different orthogonal components. Finally, we\ndevelop a quaternion vision transformer (Q-ViT) for feature classification. The\nQ-ViT adopts quaternion operations instead of the original operations in ViT,\nwhich improves the final accuracies with fewer parameters. Experimental results\non three in-the-wild FER datasets show that the proposed QOT outperforms\nseveral state-of-the-art models and reduces the computations.\n","authors":["Yu Zhou","Liyuan Guo","Lianghai Jin"],"pdf_url":"https://arxiv.org/pdf/2303.07831v1.pdf","comment":"This paper has been accepted to ICASSP2023"},{"id":"http://arxiv.org/abs/2303.07820v1","updated":"2023-03-14T11:53:12Z","published":"2023-03-14T11:53:12Z","title":"Adaptive Rotated Convolution for Rotated Object Detection","summary":"  Rotated object detection aims to identify and locate objects in images with\narbitrary orientation. In this scenario, the oriented directions of objects\nvary considerably across different images, while multiple orientations of\nobjects exist within an image. This intrinsic characteristic makes it\nchallenging for standard backbone networks to extract high-quality features of\nthese arbitrarily orientated objects. In this paper, we present Adaptive\nRotated Convolution (ARC) module to handle the aforementioned challenges. In\nour ARC module, the convolution kernels rotate adaptively to extract object\nfeatures with varying orientations in different images, and an efficient\nconditional computation mechanism is introduced to accommodate the large\norientation variations of objects within an image. The two designs work\nseamlessly in rotated object detection problem. Moreover, ARC can conveniently\nserve as a plug-and-play module in various vision backbones to boost their\nrepresentation ability to detect oriented objects accurately. Experiments on\ncommonly used benchmarks (DOTA and HRSC2016) demonstrate that equipped with our\nproposed ARC module in the backbone network, the performance of multiple\npopular oriented object detectors is significantly improved (e.g. +3.03% mAP on\nRotated RetinaNet and +4.16% on CFA). Combined with the highly competitive\nmethod Oriented R-CNN, the proposed approach achieves state-of-the-art\nperformance on the DOTA dataset with 81.77% mAP.\n","authors":["Yifan Pu","Yiru Wang","Zhuofan Xia","Yizeng Han","Yulin Wang","Weihao Gan","Zidong Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2303.07820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12039v2","updated":"2023-03-14T11:48:41Z","published":"2022-11-22T06:21:31Z","title":"Accelerating Diffusion Sampling with Classifier-based Feature\n  Distillation","summary":"  Although diffusion model has shown great potential for generating higher\nquality images than GANs, slow sampling speed hinders its wide application in\npractice. Progressive distillation is thus proposed for fast sampling by\nprogressively aligning output images of $N$-step teacher sampler with\n$N/2$-step student sampler. In this paper, we argue that this\ndistillation-based accelerating method can be further improved, especially for\nfew-step samplers, with our proposed \\textbf{C}lassifier-based \\textbf{F}eature\n\\textbf{D}istillation (CFD). Instead of aligning output images, we distill\nteacher's sharpened feature distribution into the student with a\ndataset-independent classifier, making the student focus on those important\nfeatures to improve performance. We also introduce a dataset-oriented loss to\nfurther optimize the model. Experiments on CIFAR-10 show the superiority of our\nmethod in achieving high quality and fast sampling. Code is provided at\n\\url{https://github.com/zju-SWJ/RCFD}.\n","authors":["Wujie Sun","Defang Chen","Can Wang","Deshi Ye","Yan Feng","Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2211.12039v2.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2303.07815v1","updated":"2023-03-14T11:46:04Z","published":"2023-03-14T11:46:04Z","title":"MobileVOS: Real-Time Video Object Segmentation Contrastive Learning\n  meets Knowledge Distillation","summary":"  This paper tackles the problem of semi-supervised video object segmentation\non resource-constrained devices, such as mobile phones. We formulate this\nproblem as a distillation task, whereby we demonstrate that small\nspace-time-memory networks with finite memory can achieve competitive results\nwith state of the art, but at a fraction of the computational cost (32\nmilliseconds per frame on a Samsung Galaxy S22). Specifically, we provide a\ntheoretically grounded framework that unifies knowledge distillation with\nsupervised contrastive representation learning. These models are able to\njointly benefit from both pixel-wise contrastive learning and distillation from\na pre-trained teacher. We validate this loss by achieving competitive J&F to\nstate of the art on both the standard DAVIS and YouTube benchmarks, despite\nrunning up to 5x faster, and with 32x fewer parameters.\n","authors":["Roy Miles","Mehmet Kerim Yucel","Bruno Manganelli","Albert Saa-Garriga"],"pdf_url":"https://arxiv.org/pdf/2303.07815v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07814v1","updated":"2023-03-14T11:44:58Z","published":"2023-03-14T11:44:58Z","title":"Kinematic Data-Based Action Segmentation for Surgical Applications","summary":"  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nIn the context of surgical procedures, action segmentation is critical for\nworkflow analysis algorithms. This work presents two contributions related to\naction segmentation on kinematic data. Firstly, we introduce two multi-stage\narchitectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Horizontal-Flip, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieve state-of-the-art\nperformance on all benchmark datasets and establish a strong baseline for the\nBRS dataset.\n","authors":["Adam Goldbraikh","Omer Shubi","Or Rubin","Carla M Pugh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2303.07814v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2110.01303v3","updated":"2023-03-14T11:40:35Z","published":"2021-10-04T10:19:53Z","title":"Incremental Class Learning using Variational Autoencoders with\n  Similarity Learning","summary":"  Catastrophic forgetting in neural networks during incremental learning\nremains a challenging problem. Previous research investigated catastrophic\nforgetting in fully connected networks, with some earlier work exploring\nactivation functions and learning algorithms. Applications of neural networks\nhave been extended to include similarity learning. Understanding how similarity\nlearning loss functions would be affected by catastrophic forgetting is of\nsignificant interest. Our research investigates catastrophic forgetting for\nfour well-known similarity-based loss functions during incremental class\nlearning. The loss functions are Angular, Contrastive, Center, and Triplet\nloss. Our results show that the catastrophic forgetting rate differs across\nloss functions on multiple datasets. The Angular loss was least affected,\nfollowed by Contrastive, Triplet loss, and Center loss with good mining\ntechniques. We implemented three existing incremental learning techniques,\niCaRL, EWC, and EBLL. We further proposed a novel technique using Variational\nAutoencoders (VAEs) to generate representation as exemplars passed through the\nnetwork's intermediate layers. Our method outperformed three existing\nstate-of-the-art techniques. We show that one does not require stored images\n(exemplars) for incremental learning with similarity learning. The generated\nrepresentations from VAEs help preserve regions of the embedding space used by\nprior knowledge so that new knowledge does not ``overwrite'' it.\n","authors":["Jiahao Huo","Terence L. van Zyl"],"pdf_url":"https://arxiv.org/pdf/2110.01303v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01970v4","updated":"2023-03-14T11:40:34Z","published":"2023-01-05T09:11:16Z","title":"CAT: LoCalization and IdentificAtion Cascade Detection Transformer for\n  Open-World Object Detection","summary":"  Open-world object detection (OWOD), as a more general and challenging goal,\nrequires the model trained from data on known objects to detect both known and\nunknown objects and incrementally learn to identify these unknown objects. The\nexisting works which employ standard detection framework and fixed\npseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion\nof detecting unknown objects substantially reduces the model's ability to\ndetect known ones. (ii) The PLM does not adequately utilize the priori\nknowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee\nthat the model is trained in the right direction. We observe that humans\nsubconsciously prefer to focus on all foreground objects and then identify each\none in detail, rather than localize and identify a single object\nsimultaneously, for alleviating the confusion. This motivates us to propose a\nnovel solution called CAT: LoCalization and IdentificAtion Cascade Detection\nTransformer which decouples the detection process via the shared decoder in the\ncascade decoding way. In the meanwhile, we propose the self-adaptive\npseudo-labelling mechanism which combines the model-driven with input-driven\nPLM and self-adaptively generates robust pseudo-labels for unknown objects,\nsignificantly improving the ability of CAT to retrieve unknown objects.\nComprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL\nVOC, show that our model outperforms the state-of-the-art in terms of all\nmetrics in the task of OWOD, incremental object detection (IOD) and open-set\ndetection.\n","authors":["Shuailei Ma","Yuefeng Wang","Jiaqi Fan","Ying Wei","Thomas H. Li","Hongli Liu","Fanbing Lv"],"pdf_url":"https://arxiv.org/pdf/2301.01970v4.pdf","comment":null},{"id":"http://arxiv.org/abs/1911.10375v2","updated":"2023-03-14T11:38:46Z","published":"2019-11-23T15:16:36Z","title":"Region Normalization for Image Inpainting","summary":"  Feature Normalization (FN) is an important technique to help neural network\ntraining, which typically normalizes features across spatial dimensions. Most\nprevious image inpainting methods apply FN in their networks without\nconsidering the impact of the corrupted regions of the input image on\nnormalization, e.g. mean and variance shifts. In this work, we show that the\nmean and variance shifts caused by full-spatial FN limit the image inpainting\nnetwork training and we propose a spatial region-wise normalization named\nRegion Normalization (RN) to overcome the limitation. RN divides spatial pixels\ninto different regions according to the input mask, and computes the mean and\nvariance in each region for normalization. We develop two kinds of RN for our\nimage inpainting network: (1) Basic RN (RN-B), which normalizes pixels from the\ncorrupted and uncorrupted regions separately based on the original inpainting\nmask to solve the mean and variance shift problem; (2) Learnable RN (RN-L),\nwhich automatically detects potentially corrupted and uncorrupted regions for\nseparate normalization, and performs global affine transformation to enhance\ntheir fusion. We apply RN-B in the early layers and RN-L in the latter layers\nof the network respectively. Experiments show that our method outperforms\ncurrent state-of-the-art methods quantitatively and qualitatively. We further\ngeneralize RN to other inpainting networks and achieve consistent performance\nimprovements. Our code is available at https://github.com/geekyutao/RN.\n","authors":["Tao Yu","Zongyu Guo","Xin Jin","Shilin Wu","Zhibo Chen","Weiping Li","Zhizheng Zhang","Sen Liu"],"pdf_url":"https://arxiv.org/pdf/1911.10375v2.pdf","comment":"Accepted to AAAI-2020. Code URL:https://github.com/geekyutao/RN"},{"id":"http://arxiv.org/abs/2303.07811v1","updated":"2023-03-14T11:31:45Z","published":"2023-03-14T11:31:45Z","title":"ICICLE: Interpretable Class Incremental Continual Learning","summary":"  Continual learning enables incremental learning of new tasks without\nforgetting those previously learned, resulting in positive knowledge transfer\nthat can enhance performance on both new and old tasks. However, continual\nlearning poses new challenges for interpretability, as the rationale behind\nmodel predictions may change over time, leading to interpretability concept\ndrift. We address this problem by proposing Interpretable Class-InCremental\nLEarning (ICICLE), an exemplar-free approach that adopts a prototypical\npart-based approach. It consists of three crucial novelties: interpretability\nregularization that distills previously learned concepts while preserving\nuser-friendly positive reasoning; proximity-based prototype initialization\nstrategy dedicated to the fine-grained setting; and task-recency bias\ncompensation devoted to prototypical parts. Our experimental results\ndemonstrate that ICICLE reduces the interpretability concept drift and\noutperforms the existing exemplar-free methods of common class-incremental\nlearning when applied to concept-based models. We make the code available.\n","authors":["Dawid Rymarczyk","Joost van de Weijer","Bartosz Zieliński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2303.07811v1.pdf","comment":"Under review, code will be shared after the acceptance"},{"id":"http://arxiv.org/abs/2303.07806v1","updated":"2023-03-14T11:25:02Z","published":"2023-03-14T11:25:02Z","title":"USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised\n  Semantic Segmentation","summary":"  Seed area generation is usually the starting point of weakly supervised\nsemantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a\nmulti-label classification network is the de facto paradigm for seed area\ngeneration, but CAMs generated from Convolutional Neural Networks (CNNs) and\nTransformers are prone to be under- and over-activated, respectively, which\nmakes the strategies to refine CAMs for CNNs usually inappropriate for\nTransformers, and vice versa. In this paper, we propose a Unified optimization\nparadigm for Seed Area GEneration (USAGE) for both types of networks, in which\nthe objective function to be optimized consists of two terms: One is a\ngeneration loss, which controls the shape of seed areas by a temperature\nparameter following a deterministic principle for different types of networks;\nThe other is a regularization loss, which ensures the consistency between the\nseed areas that are generated by self-adaptive network adjustment from\ndifferent views, to overturn false activation in seed areas. Experimental\nresults show that USAGE consistently improves seed area generation for both\nCNNs and Transformers by large margins, e.g., outperforming state-of-the-art\nmethods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated\nseed areas on Transformers, we achieve state-of-the-art WSSS results on both\nPASCAL VOC and MS COCO.\n","authors":["Zelin Peng","Guanchun Wang","Lingxi Xie","Dongsheng Jiang","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.07806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07798v1","updated":"2023-03-14T11:15:37Z","published":"2023-03-14T11:15:37Z","title":"OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav","summary":"  We present a single neural network architecture composed of task-agnostic\ncomponents (ViTs, convolutions, and LSTMs) that achieves state-of-art results\non both the ImageNav (\"go to location in <this picture>\") and ObjectNav (\"find\na chair\") tasks without any task-specific modules like object detection,\nsegmentation, mapping, or planning modules. Such general-purpose methods offer\nadvantages of simplicity in design, positive scaling with available compute,\nand versatile applicability to multiple tasks. Our work builds upon the recent\nsuccess of self-supervised learning (SSL) for pre-training vision transformers\n(ViT). However, while the training recipes for convolutional networks are\nmature and robust, the recipes for ViTs are contingent and brittle, and in the\ncase of ViTs for visual navigation, yet to be fully discovered. Specifically,\nwe find that vanilla ViTs do not outperform ResNets on visual navigation. We\npropose the use of a compression layer operating over ViT patch representations\nto preserve spatial information along with policy training improvements. These\nimprovements allow us to demonstrate positive scaling laws for the first time\nin visual navigation tasks. Consequently, our model advances state-of-the-art\nperformance on ImageNav from 54.2% to 82.0% success and performs competitively\nagainst concurrent state-of-art on ObjectNav with success rate of 64.0% vs.\n65.0%. Overall, this work does not present a fundamentally new approach, but\nrather recommendations for training a general-purpose architecture that\nachieves state-of-art performance today and could serve as a strong baseline\nfor future methods.\n","authors":["Karmesh Yadav","Arjun Majumdar","Ram Ramrakhya","Naoki Yokoyama","Alexei Baevski","Zsolt Kira","Oleksandr Maksymets","Dhruv Batra"],"pdf_url":"https://arxiv.org/pdf/2303.07798v1.pdf","comment":"15 pages, 7 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.07790v1","updated":"2023-03-14T11:04:50Z","published":"2023-03-14T11:04:50Z","title":"Object Detection During Newborn Resuscitation Activities","summary":"  Birth asphyxia is a major newborn mortality problem in low-resource\ncountries. International guideline provides treatment recommendations; however,\nthe importance and effect of the different treatments are not fully explored.\nThe available data is collected in Tanzania, during newborn resuscitation, for\nanalysis of the resuscitation activities and the response of the newborn. An\nimportant step in the analysis is to create activity timelines of the episodes,\nwhere activities include ventilation, suction, stimulation etc. Methods: The\navailable recordings are noisy real-world videos with large variations. We\npropose a two-step process in order to detect activities possibly overlapping\nin time. The first step is to detect and track the relevant objects, like\nbag-mask resuscitator, heart rate sensors etc., and the second step is to use\nthis information to recognize the resuscitation activities. The topic of this\npaper is the first step, and the object detection and tracking are based on\nconvolutional neural networks followed by post processing. Results: The\nperformance of the object detection during activities were 96.97 %\n(ventilations), 100 % (attaching/removing heart rate sensor) and 75 % (suction)\non a test set of 20 videos. The system also estimate the number of health care\nproviders present with a performance of 71.16 %. Conclusion: The proposed\nobject detection and tracking system provides promising results in noisy\nnewborn resuscitation videos. Significance: This is the first step in a\nthorough analysis of newborn resuscitation episodes, which could provide\nimportant insight about the importance and effect of different newborn\nresuscitation activities\n","authors":["Øyvind Meinich-Bache","Kjersti Engan","Ivar Austvoll","Trygve Eftestøl","Helge Myklebust","Ladislaus Blacy Yarrot","Hussein Kidanto","Hege Ersdal"],"pdf_url":"https://arxiv.org/pdf/2303.07790v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2212.03022v2","updated":"2023-03-14T11:04:43Z","published":"2022-12-06T14:49:41Z","title":"Iterative Next Boundary Detection for Instance Segmentation of Tree\n  Rings in Microscopy Images of Shrub Cross Sections","summary":"  We address the problem of detecting tree rings in microscopy images of shrub\ncross sections. This can be regarded as a special case of the instance\nsegmentation task with several unique challenges such as the concentric\ncircular ring shape of the objects and high precision requirements that result\nin inadequate performance of existing methods. We propose a new iterative\nmethod which we term Iterative Next Boundary Detection (INBD). It intuitively\nmodels the natural growth direction, starting from the center of the shrub\ncross section and detecting the next ring boundary in each iteration step. In\nour experiments, INBD shows superior performance to generic instance\nsegmentation methods and is the only one with a built-in notion of\nchronological order. Our dataset and source code are available at\nhttp://github.com/alexander-g/INBD.\n","authors":["Alexander Gillert","Giulia Resente","Alba Anadon-Rosell","Martin Wilmking","Uwe Freiherr von Lukas"],"pdf_url":"https://arxiv.org/pdf/2212.03022v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07789v1","updated":"2023-03-14T11:04:32Z","published":"2023-03-14T11:04:32Z","title":"Activity Recognition From Newborn Resuscitation Videos","summary":"  Objective: Birth asphyxia is one of the leading causes of neonatal deaths. A\nkey for survival is performing immediate and continuous quality newborn\nresuscitation. A dataset of recorded signals during newborn resuscitation,\nincluding videos, has been collected in Haydom, Tanzania, and the aim is to\nanalyze the treatment and its effect on the newborn outcome. An important step\nis to generate timelines of relevant resuscitation activities, including\nventilation, stimulation, suction, etc., during the resuscitation episodes.\nMethods: We propose a two-step deep neural network system, ORAA-net, utilizing\nlow-quality video recordings of resuscitation episodes to do activity\nrecognition during newborn resuscitation. The first step is to detect and track\nrelevant objects using Convolutional Neural Networks (CNN) and post-processing,\nand the second step is to analyze the proposed activity regions from step 1 to\ndo activity recognition using 3D CNNs. Results: The system recognized the\nactivities newborn uncovered, stimulation, ventilation and suction with a mean\nprecision of 77.67 %, a mean recall of 77,64 %, and a mean accuracy of 92.40 %.\nMoreover, the accuracy of the estimated number of Health Care Providers (HCPs)\npresent during the resuscitation episodes was 68.32 %. Conclusion: The results\nindicate that the proposed CNN-based two-step ORAAnet could be used for object\ndetection and activity recognition in noisy low-quality newborn resuscitation\nvideos. Significance: A thorough analysis of the effect the different\nresuscitation activities have on the newborn outcome could potentially allow us\nto optimize treatment guidelines, training, debriefing, and local quality\nimprovement in newborn resuscitation.\n","authors":["Øyvind Meinich-Bache","Simon Lennart Austnes","Kjersti Engan","Ivar Austvoll","Trygve Eftestøl","Helge Myklebust","Simeon Kusulla","Hussein Kidanto","Hege Ersdal"],"pdf_url":"https://arxiv.org/pdf/2303.07789v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2212.12902v2","updated":"2023-03-14T10:39:16Z","published":"2022-12-25T13:36:32Z","title":"TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose\n  Estimation","summary":"  In this paper, we introduce neural texture learning for 6D object pose\nestimation from synthetic data and a few unlabelled real images. Our major\ncontribution is a novel learning scheme which removes the drawbacks of previous\nworks, namely the strong dependency on co-modalities or additional refinement.\nThese have been previously necessary to provide training signals for\nconvergence. We formulate such a scheme as two sub-optimisation problems on\ntexture learning and pose learning. We separately learn to predict realistic\ntexture of objects from real image collections and learn pose estimation from\npixel-perfect synthetic data. Combining these two capabilities allows then to\nsynthesise photorealistic novel views to supervise the pose estimator with\naccurate geometry. To alleviate pose noise and segmentation imperfection\npresent during the texture learning phase, we propose a surfel-based\nadversarial training loss together with texture regularisation from synthetic\ndata. We demonstrate that the proposed approach significantly outperforms the\nrecent state-of-the-art methods without ground-truth pose annotations and\ndemonstrates substantial generalisation improvements towards unseen scenes.\nRemarkably, our scheme improves the adopted pose estimators substantially even\nwhen initialised with much inferior performance.\n","authors":["Hanzhi Chen","Fabian Manhardt","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2212.12902v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07775v1","updated":"2023-03-14T10:34:07Z","published":"2023-03-14T10:34:07Z","title":"Data-Free Sketch-Based Image Retrieval","summary":"  Rising concerns about privacy and anonymity preservation of deep learning\nmodels have facilitated research in data-free learning (DFL). For the first\ntime, we identify that for data-scarce tasks like Sketch-Based Image Retrieval\n(SBIR), where the difficulty in acquiring paired photos and hand-drawn sketches\nlimits data-dependent cross-modal learning algorithms, DFL can prove to be a\nmuch more practical paradigm. We thus propose Data-Free (DF)-SBIR, where,\nunlike existing DFL problems, pre-trained, single-modality classification\nmodels have to be leveraged to learn a cross-modal metric-space for retrieval\nwithout access to any training data. The widespread availability of pre-trained\nclassification models, along with the difficulty in acquiring paired\nphoto-sketch datasets for SBIR justify the practicality of this setting. We\npresent a methodology for DF-SBIR, which can leverage knowledge from models\nindependently trained to perform classification on photos and sketches. We\nevaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks,\ndesigning a variety of baselines based on state-of-the-art DFL literature, and\nobserve that our method surpasses all of them by significant margins. Our\nmethod also achieves mAPs competitive with data-dependent approaches, all the\nwhile requiring no training data. Implementation is available at\n\\url{https://github.com/abhrac/data-free-sbir}.\n","authors":["Abhra Chaudhuri","Ayan Kumar Bhunia","Yi-Zhe Song","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2303.07775v1.pdf","comment":"Computer Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2303.07771v1","updated":"2023-03-14T10:20:31Z","published":"2023-03-14T10:20:31Z","title":"Imbalanced Domain Generalization for Robust Single Cell Classification\n  in Hematological Cytomorphology","summary":"  Accurate morphological classification of white blood cells (WBCs) is an\nimportant step in the diagnosis of leukemia, a disease in which nonfunctional\nblast cells accumulate in the bone marrow. Recently, deep convolutional neural\nnetworks (CNNs) have been successfully used to classify leukocytes by training\nthem on single-cell images from a specific domain. Most CNN models assume that\nthe distributions of the training and test data are similar, i.e., that the\ndata are independently and identically distributed. Therefore, they are not\nrobust to different staining protocols, magnifications, resolutions, scanners,\nor imaging protocols, as well as variations in clinical centers or patient\ncohorts. In addition, domain-specific data imbalances affect the generalization\nperformance of classifiers. Here, we train a robust CNN for WBC classification\nby addressing cross-domain data imbalance and domain shifts. To this end, we\nuse two loss functions and demonstrate the effectiveness on out-of-distribution\n(OOD) generalization. Our approach achieves the best F1 macro score compared to\nother existing methods, and is able to consider rare cell types. This is the\nfirst demonstration of imbalanced domain generalization in hematological\ncytomorphology and paves the way for robust single cell classification methods\nfor the application in laboratories and clinics.\n","authors":["Rao Muhammad Umer","Armin Gruber","Sayedali Shetab Boushehri","Christian Metak","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2303.07771v1.pdf","comment":"Published as a ICLR 2023 workshop paper: What do we need for\n  successful domain generalization?"},{"id":"http://arxiv.org/abs/2303.07759v1","updated":"2023-03-14T10:06:19Z","published":"2023-03-14T10:06:19Z","title":"A Simple Baseline for Supervised Surround-view Depth Estimation","summary":"  Depth estimation has been widely studied and serves as the fundamental step\nof 3D perception for autonomous driving. Though significant progress has been\nmade for monocular depth estimation in the past decades, these attempts are\nmainly conducted on the KITTI benchmark with only front-view cameras, which\nignores the correlations across surround-view cameras. In this paper, we\npropose S3Depth, a Simple Baseline for Supervised Surround-view Depth\nEstimation, to jointly predict the depth maps across multiple surrounding\ncameras. Specifically, we employ a global-to-local feature extraction module\nwhich combines CNN with transformer layers for enriched representations.\nFurther, the Adjacent-view Attention mechanism is proposed to enable the\nintra-view and inter-view feature propagation. The former is achieved by the\nself-attention module within each view, while the latter is realized by the\nadjacent attention module, which computes the attention across multi-cameras to\nexchange the multi-scale representations across surround-view feature maps.\nExtensive experiments show that our method achieves superior performance over\nexisting state-of-the-art methods on both DDAD and nuScenes datasets.\n","authors":["Xianda Guo","Wenjie Yuan","Yunpeng Zhang","Tian Yang","Chenming Zhang","Zheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.07759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12220v2","updated":"2023-03-14T10:05:43Z","published":"2021-07-26T13:56:37Z","title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought","summary":"  When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2107.12220v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.13572v2","updated":"2023-03-14T09:53:55Z","published":"2022-11-24T12:44:33Z","title":"Real-Time Physics-Based Object Pose Tracking during Non-Prehensile\n  Manipulation","summary":"  We propose a method to track the 6D pose of an object over time, while the\nobject is under non-prehensile manipulation by a robot. At any given time\nduring the manipulation of the object, we assume access to the robot joint\ncontrols and an image from a camera. We use the robot joint controls to perform\na physics-based prediction of how the object might be moving. We then combine\nthis prediction with the observation coming from the camera, to estimate the\nobject pose as accurately as possible. We use a particle filtering approach to\ncombine the control information with the visual information. We compare the\nproposed method with two baselines: (i) using only an image-based pose\nestimation system at each time-step, and (ii) a particle filter which does not\nperform the computationally expensive physics predictions, but assumes the\nobject moves with constant velocity. Our results show that making physics-based\npredictions is worth the computational cost, resulting in more accurate\ntracking, and estimating object pose even when the object is not clearly\nvisible to the camera.\n","authors":["Zisong Xu","Rafael Papallas","Mehmet Dogar"],"pdf_url":"https://arxiv.org/pdf/2211.13572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07748v1","updated":"2023-03-14T09:48:59Z","published":"2023-03-14T09:48:59Z","title":"Generation-Guided Multi-Level Unified Network for Video Grounding","summary":"  Video grounding aims to locate the timestamps best matching the query\ndescription within an untrimmed video. Prevalent methods can be divided into\nmoment-level and clip-level frameworks. Moment-level approaches directly\npredict the probability of each transient moment to be the boundary in a global\nperspective, and they usually perform better in coarse grounding. On the other\nhand, clip-level ones aggregate the moments in different time windows into\nproposals and then deduce the most similar one, leading to its advantage in\nfine-grained grounding. In this paper, we propose a multi-level unified\nframework to enhance performance by leveraging the merits of both moment-level\nand clip-level methods. Moreover, a novel generation-guided paradigm in both\nlevels is adopted. It introduces a multi-modal generator to produce the\nimplicit boundary feature and clip feature, later regarded as queries to\ncalculate the boundary scores by a discriminator. The generation-guided\nsolution enhances video grounding from a two-unique-modals' match task to a\ncross-modal attention task, which steps out of the previous framework and\nobtains notable gains. The proposed Generation-guided Multi-level Unified\nnetwork (GMU) surpasses previous methods and reaches State-Of-The-Art on\nvarious benchmarks with disparate features, e.g., Charades-STA, ActivityNet\ncaptions.\n","authors":["Xing Cheng","Xiangyu Wu","Dong Shen","Hezheng Lin","Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07747v1","updated":"2023-03-14T09:44:29Z","published":"2023-03-14T09:44:29Z","title":"LoG-CAN: local-global Class-aware Network for semantic segmentation of\n  remote sensing images","summary":"  Remote sensing images are known of having complex backgrounds, high\nintra-class variance and large variation of scales, which bring challenge to\nsemantic segmentation. We present LoG-CAN, a multi-scale semantic segmentation\nnetwork with a global class-aware (GCA) module and local class-aware (LCA)\nmodules to remote sensing images. Specifically, the GCA module captures the\nglobal representations of class-wise context modeling to circumvent background\ninterference; the LCA modules generate local class representations as\nintermediate aware elements, indirectly associating pixels with global class\nrepresentations to reduce variance within a class; and a multi-scale\narchitecture with GCA and LCA modules yields effective segmentation of objects\nat different scales via cascaded refinement and fusion of features. Through the\nevaluation on the ISPRS Vaihingen dataset and the ISPRS Potsdam dataset,\nexperimental results indicate that LoG-CAN outperforms the state-of-the-art\nmethods for general semantic segmentation, while significantly reducing network\nparameters and computation. Code is available\nat~\\href{https://github.com/xwmaxwma/rssegmentation}{https://github.com/xwmaxwma/rssegmentation}.\n","authors":["Xiaowen Ma","Mengting Ma","Chenlu Hu","Zhiyuan Song","Ziyan Zhao","Tian Feng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07747v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07744v1","updated":"2023-03-14T09:42:49Z","published":"2023-03-14T09:42:49Z","title":"Sliding at first order: Higher-order momentum distributions for\n  discontinuous image registration","summary":"  In this paper, we propose a new approach to deformable image registration\nthat captures sliding motions. The large deformation diffeomorphic metric\nmapping (LDDMM) registration method faces challenges in representing sliding\nmotion since it per construction generates smooth warps. To address this issue,\nwe extend LDDMM by incorporating both zeroth- and first-order momenta with a\nnon-differentiable kernel. This allows to represent both discontinuous\ndeformation at switching boundaries and diffeomorphic deformation in\nhomogeneous regions. We provide a mathematical analysis of the proposed\ndeformation model from the viewpoint of discontinuous systems. To evaluate our\napproach, we conduct experiments on both artificial images and the publicly\navailable DIR-Lab 4DCT dataset. Results show the effectiveness of our approach\nin capturing plausible sliding motion.\n","authors":["Lili Bao","Jiahao Lu","Shihui Ying","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2303.07744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07740v1","updated":"2023-03-14T09:36:42Z","published":"2023-03-14T09:36:42Z","title":"Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening","summary":"  Under the flourishing development in performance, current image-text\nretrieval methods suffer from $N$-related time complexity, which hinders their\napplication in practice. Targeting at efficiency improvement, this paper\npresents a simple and effective keyword-guided pre-screening framework for the\nimage-text retrieval. Specifically, we convert the image and text data into the\nkeywords and perform the keyword matching across modalities to exclude a large\nnumber of irrelevant gallery samples prior to the retrieval network. For the\nkeyword prediction, we transfer it into a multi-label classification problem\nand propose a multi-task learning scheme by appending the multi-label\nclassifiers to the image-text retrieval network to achieve a lightweight and\nhigh-performance keyword prediction. For the keyword matching, we introduce the\ninverted index in the search engine and create a win-win situation on both time\nand space complexities for the pre-screening. Extensive experiments on two\nwidely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of\nthe proposed framework. The proposed framework equipped with only two embedding\nlayers achieves $O(1)$ querying time complexity, while improving the retrieval\nefficiency and keeping its performance, when applied prior to the common\nimage-text retrieval methods. Our code will be released.\n","authors":["Min Cao","Yang Bai","Jingyao Wang","Ziqiang Cao","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07740v1.pdf","comment":"11 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.07034v2","updated":"2023-03-14T09:19:03Z","published":"2023-03-13T11:53:40Z","title":"Pretrained ViTs Yield Versatile Representations For Medical Images","summary":"  Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis, pushing the\nstate-of-the-art in classification, detection and segmentation tasks. Over the\nlast years, vision transformers (ViTs) have appeared as a competitive\nalternative to CNNs, yielding impressive levels of performance in the natural\nimage domain, while possessing several interesting properties that could prove\nbeneficial for medical imaging tasks. In this work, we explore the benefits and\ndrawbacks of transformer-based models for medical image classification. We\nconduct a series of experiments on several standard 2D medical image benchmark\ndatasets and tasks. Our findings show that, while CNNs perform better if\ntrained from scratch, off-the-shelf vision transformers can perform on par with\nCNNs when pretrained on ImageNet, both in a supervised and self-supervised\nsetting, rendering them as a viable alternative to CNNs.\n","authors":["Christos Matsoukas","Johan Fredin Haslum","Magnus Söderberg","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2303.07034v2.pdf","comment":"Extended version of arXiv:2108.09038 originally published at the ICCV\n  2021 Workshop on Computer Vision for Automated Medical Diagnosis"},{"id":"http://arxiv.org/abs/2303.04940v3","updated":"2023-03-14T09:10:20Z","published":"2023-03-08T23:23:44Z","title":"Non-aligned supervision for Real Image Dehazing","summary":"  Removing haze from real-world images is challenging due to unpredictable\nweather conditions, resulting in misaligned hazy and clear image pairs. In this\npaper, we propose a non-aligned supervision framework that consists of three\nnetworks - dehazing, airlight, and transmission. In particular, we explore a\nnon-alignment setting by utilizing a clear reference image that is not aligned\nwith the hazy input image to supervise the dehazing network through a\nmulti-scale reference loss that compares the features of the two images. Our\nsetting makes it easier to collect hazy/clear image pairs in real-world\nenvironments, even under conditions of misalignment and shift views. To\ndemonstrate this, we have created a new hazy dataset called \"Phone-Hazy\", which\nwas captured using mobile phones in both rural and urban areas. Additionally,\nwe present a mean and variance self-attention network to model the infinite\nairlight using dark channel prior as position guidance, and employ a channel\nattention network to estimate the three-channel transmission. Experimental\nresults show that our framework outperforms current state-of-the-art methods in\nthe real-world image dehazing. Phone-Hazy and code will be available at\nhttps://github.com/hello2377/NSDNet.\n","authors":["Junkai Fan","Fei Guo","Jianjun Qian","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.04940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14511v2","updated":"2023-03-14T09:10:14Z","published":"2023-02-28T12:01:16Z","title":"A Unified BEV Model for Joint Learning of 3D Local Features and Overlap\n  Estimation","summary":"  Pairwise point cloud registration is a critical task for many applications,\nwhich heavily depends on finding correct correspondences from the two point\nclouds. However, the low overlap between input point clouds causes the\nregistration to fail easily, leading to mistaken overlapping and mismatched\ncorrespondences, especially in scenes where non-overlapping regions contain\nsimilar structures. In this paper, we present a unified bird's-eye view (BEV)\nmodel for jointly learning of 3D local features and overlap estimation to\nfulfill pairwise registration and loop closure. Feature description is\nperformed by a sparse UNet-like network based on BEV representation, and 3D\nkeypoints are extracted by a detection head for 2D locations, and a regression\nhead for heights. For overlap detection, a cross-attention module is applied\nfor interacting contextual information of input point clouds, followed by a\nclassification head to estimate the overlapping region. We evaluate our unified\nmodel extensively on the KITTI dataset and Apollo-SouthBay dataset. The\nexperiments demonstrate that our method significantly outperforms existing\nmethods on overlap estimation, especially in scenes with small overlaps. It\nalso achieves top registration performance on both datasets in terms of\ntranslation and rotation errors.\n","authors":["Lin Li","Wendong Ding","Yongkun Wen","Yufei Liang","Yong Liu","Guowei Wan"],"pdf_url":"https://arxiv.org/pdf/2302.14511v2.pdf","comment":"8 pages. Accepted by ICRA-2023"},{"id":"http://arxiv.org/abs/2207.07921v2","updated":"2023-03-14T09:10:08Z","published":"2022-07-16T12:11:28Z","title":"CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image\n  Prior","summary":"  Euler's elastica constitute an appealing variational image inpainting model.\nIt minimises an energy that involves the total variation as well as the level\nline curvature. These components are transparent and make it attractive for\nshape completion tasks. However, its gradient flow is a singular, anisotropic,\nand nonlinear PDE of fourth order, which is numerically challenging: It is\ndifficult to find efficient algorithms that offer sharp edges and good rotation\ninvariance. As a remedy, we design the first neural algorithm that simulates\ninpainting with Euler's Elastica. We use the deep energy concept which employs\nthe variational energy as neural network loss. Furthermore, we pair it with a\ndeep image prior where the network architecture itself acts as a prior. This\nyields better inpaintings by steering the optimisation trajectory closer to the\ndesired solution. Our results are qualitatively on par with state-of-the-art\nalgorithms on elastica-based shape completion. They combine good rotation\ninvariance with sharp edges. Moreover, we benefit from the high efficiency and\neffortless parallelisation within a neural framework. Our neural elastica\napproach only requires 3x3 central difference stencils. It is thus much simpler\nthan other well-performing algorithms for elastica inpainting. Last but not\nleast, it is unsupervised as it requires no ground truth training data.\n","authors":["Karl Schrader","Tobias Alt","Joachim Weickert","Michael Ertel"],"pdf_url":"https://arxiv.org/pdf/2207.07921v2.pdf","comment":"In Proceedings of the 10th European Workshop on Visual Information\n  Processing, Lisbon, 2022"},{"id":"http://arxiv.org/abs/2303.07717v1","updated":"2023-03-14T09:05:19Z","published":"2023-03-14T09:05:19Z","title":"HALOS: Hallucination-free Organ Segmentation after Organ Resection\n  Surgery","summary":"  The wide range of research in deep learning-based medical image segmentation\npushed the boundaries in a multitude of applications. A clinically relevant\nproblem that received less attention is the handling of scans with irregular\nanatomy, e.g., after organ resection. State-of-the-art segmentation models\noften lead to organ hallucinations, i.e., false-positive predictions of organs,\nwhich cannot be alleviated by oversampling or post-processing. Motivated by the\nincreasing need to develop robust deep learning models, we propose HALOS for\nabdominal organ segmentation in MR images that handles cases after organ\nresection surgery. To this end, we combine missing organ classification and\nmulti-organ segmentation tasks into a multi-task model, yielding a\nclassification-assisted segmentation pipeline. The segmentation network learns\nto incorporate knowledge about organ existence via feature fusion modules.\nExtensive experiments on a small labeled test set and large-scale UK Biobank\ndata demonstrate the effectiveness of our approach in terms of higher\nsegmentation Dice scores and near-to-zero false positive prediction rate.\n","authors":["Anne-Marie Rickmann","Murong Xu","Tom Nuno Wolf","Oksana Kovalenko","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07717v1.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging (IPMI) 2023"},{"id":"http://arxiv.org/abs/2303.07716v1","updated":"2023-03-14T09:03:54Z","published":"2023-03-14T09:03:54Z","title":"BlinkFlow: A Dataset to Push the Limits of Event-based Optical Flow\n  Estimation","summary":"  Event cameras provide high temporal precision, low data rates, and high\ndynamic range visual perception, which are well-suited for optical flow\nestimation. While data-driven optical flow estimation has obtained great\nsuccess in RGB cameras, its generalization performance is seriously hindered in\nevent cameras mainly due to the limited and biased training data. In this\npaper, we present a novel simulator, BlinkSim, for the fast generation of\nlarge-scale data for event-based optical flow. BlinkSim consists of a\nconfigurable rendering engine and a flexible engine for event data simulation.\nBy leveraging the wealth of current 3D assets, the rendering engine enables us\nto automatically build up thousands of scenes with different objects, textures,\nand motion patterns and render very high-frequency images for realistic event\ndata simulation. Based on BlinkSim, we construct a large training dataset and\nevaluation benchmark BlinkFlow that contains sufficient, diversiform, and\nchallenging event data with optical flow ground truth. Experiments show that\nBlinkFlow improves the generalization performance of state-of-the-art methods\nby more than 40% on average and up to 90%. Moreover, we further propose an\nEvent optical Flow transFormer (E-FlowFormer) architecture. Powered by our\nBlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on MVSEC\ndataset and 14% on DSEC dataset and presents the best generalization\nperformance.\n","authors":["Yijin Li","Zhaoyang Huang","Shuo Chen","Xiaoyu Shi","Hongsheng Li","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14125v3","updated":"2023-03-14T08:59:16Z","published":"2022-08-30T10:21:40Z","title":"A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images","summary":"  Diffusion models are a special type of generative model, capable of\nsynthesising new data from a learnt distribution. We introduce DISPR, a\ndiffusion-based model for solving the inverse problem of three-dimensional (3D)\ncell shape prediction from two-dimensional (2D) single cell microscopy images.\nUsing the 2D microscopy image as a prior, DISPR is conditioned to predict\nrealistic 3D shape reconstructions. To showcase the applicability of DISPR as a\ndata augmentation tool in a feature-based single cell classification task, we\nextract morphological features from the red blood cells grouped into six highly\nimbalanced classes. Adding features from the DISPR predictions to the three\nminority classes improved the macro F1 score from $F1_\\text{macro} = 55.2 \\pm\n4.6\\%$ to $F1_\\text{macro} = 72.2 \\pm 4.9\\%$. We thus demonstrate that\ndiffusion models can be successfully applied to inverse biomedical problems,\nand that they learn to reconstruct 3D shapes with realistic morphological\nfeatures from 2D microscopy images.\n","authors":["Dominik J. E. Waibel","Ernst Röell","Bastian Rieck","Raja Giryes","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2208.14125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07714v1","updated":"2023-03-14T08:55:24Z","published":"2023-03-14T08:55:24Z","title":"Freehand 2D Ultrasound Probe Calibration for Image Fusion with 3D MRI/CT","summary":"  The aim of this work is to implement a simple freehand ultrasound (US) probe\ncalibration technique. This will enable us to visualize US image data during\nsurgical procedures using augmented reality. The performance of the system was\nevaluated with different experiments using two different pose estimation\ntechniques. A near-millimeter accuracy can be achieved with the proposed\napproach. The developed system is cost-effective, simple and rapid with low\ncalibration error\n","authors":["Yogesh Langhe","Katrin Skerl","Adrien Bartoli"],"pdf_url":"https://arxiv.org/pdf/2303.07714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07709v1","updated":"2023-03-14T08:51:51Z","published":"2023-03-14T08:51:51Z","title":"3D Face Arbitrary Style Transfer","summary":"  Style transfer of 3D faces has gained more and more attention. However,\nprevious methods mainly use images of artistic faces for style transfer while\nignoring arbitrary style images such as abstract paintings. To solve this\nproblem, we propose a novel method, namely Face-guided Dual Style Transfer\n(FDST). To begin with, FDST employs a 3D decoupling module to separate facial\ngeometry and texture. Then we propose a style fusion strategy for facial\ngeometry. Subsequently, we design an optimization-based DDSG mechanism for\ntextures that can guide the style transfer by two style images. Besides the\nnormal style image input, DDSG can utilize the original face input as another\nstyle input as the face prior. By this means, high-quality face arbitrary style\ntransfer results can be obtained. Furthermore, FDST can be applied in many\ndownstream tasks, including region-controllable style transfer, high-fidelity\nface texture reconstruction, large-pose face reconstruction, and artistic face\nreconstruction. Comprehensive quantitative and qualitative results show that\nour method can achieve comparable performance. All source codes and pre-trained\nweights will be released to the public.\n","authors":["Xiangwen Deng","Yingshuang Zou","Yuanhao Cai","Chendong Zhao","Yang Liu","Zhifang Liu","Yuxiao Liu","Jiawei Zhou","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06615v2","updated":"2023-03-14T08:39:23Z","published":"2023-03-12T09:11:14Z","title":"Iterative Geometry Encoding Volume for Stereo Matching","summary":"  Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in\nmatching tasks. However, all-pairs correlations lack non-local geometry\nknowledge and have difficulties tackling local ambiguities in ill-posed\nregions. In this paper, we propose Iterative Geometry Encoding Volume\n(IGEV-Stereo), a new deep network architecture for stereo matching. The\nproposed IGEV-Stereo builds a combined geometry encoding volume that encodes\ngeometry and context information as well as local matching details, and\niteratively indexes it to update the disparity map. To speed up the\nconvergence, we exploit GEV to regress an accurate starting point for ConvGRUs\niterations. Our IGEV-Stereo ranks $1^{st}$ on KITTI 2015 and 2012 (Reflective)\namong all published methods and is the fastest among the top 10 methods. In\naddition, IGEV-Stereo has strong cross-dataset generalization as well as high\ninference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e.\nIGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is\navailable at https://github.com/gangweiX/IGEV.\n","authors":["Gangwei Xu","Xianqi Wang","Xiaohuan Ding","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2303.06615v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07125v2","updated":"2023-03-14T08:29:49Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\nhttps://github.com/ai-med/PANIC .\n","authors":["Tom Nuno Wolf","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v2.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07700v1","updated":"2023-03-14T08:28:36Z","published":"2023-03-14T08:28:36Z","title":"PATS: Patch Area Transportation with Subdivision for Local Feature\n  Matching","summary":"  Local feature matching aims at establishing sparse correspondences between a\npair of images. Recently, detectorfree methods present generally better\nperformance but are not satisfactory in image pairs with large scale\ndifferences. In this paper, we propose Patch Area Transportation with\nSubdivision (PATS) to tackle this issue. Instead of building an expensive image\npyramid, we start by splitting the original image pair into equal-sized patches\nand gradually resizing and subdividing them into smaller patches with the same\nscale. However, estimating scale differences between these patches is\nnon-trivial since the scale differences are determined by both relative camera\nposes and scene structures, and thus spatially varying over image pairs.\nMoreover, it is hard to obtain the ground truth for real scenes. To this end,\nwe propose patch area transportation, which enables learning scale differences\nin a self-supervised manner. In contrast to bipartite graph matching, which\nonly handles one-to-one matching, our patch area transportation can deal with\nmany-to-many relationships. PATS improves both matching accuracy and coverage,\nand shows superior performance in downstream tasks, such as relative pose\nestimation, visual localization, and optical flow estimation. The source code\nwill be released to benefit the community.\n","authors":["Junjie Ni","Yijin Li","Zhaoyang Huang","Hongsheng Li","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07700v1.pdf","comment":"Project page: https://zju3dv.github.io/pats"},{"id":"http://arxiv.org/abs/2303.07697v1","updated":"2023-03-14T08:22:18Z","published":"2023-03-14T08:22:18Z","title":"DisCoHead: Audio-and-Video-Driven Talking Head Generation by\n  Disentangled Control of Head Pose and Facial Expressions","summary":"  For realistic talking head generation, creating natural head motion while\nmaintaining accurate lip synchronization is essential. To fulfill this\nchallenging task, we propose DisCoHead, a novel method to disentangle and\ncontrol head pose and facial expressions without supervision. DisCoHead uses a\nsingle geometric transformation as a bottleneck to isolate and extract head\nmotion from a head-driving video. Either an affine or a thin-plate spline\ntransformation can be used and both work well as geometric bottlenecks. We\nenhance the efficiency of DisCoHead by integrating a dense motion estimator and\nthe encoder of a generator which are originally separate modules. Taking a step\nfurther, we also propose a neural mix approach where dense motion is estimated\nand applied implicitly by the encoder. After applying the disentangled head\nmotion to a source identity, DisCoHead controls the mouth region according to\nspeech audio, and it blinks eyes and moves eyebrows following a separate\ndriving video of the eye region, via the weight modulation of convolutional\nneural networks. The experiments using multiple datasets show that DisCoHead\nsuccessfully generates realistic audio-and-video-driven talking heads and\noutperforms state-of-the-art methods. Project page:\nhttps://deepbrainai-research.github.io/discohead/\n","authors":["Geumbyeol Hwang","Sunwon Hong","Seunghyun Lee","Sungwoo Park","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07697v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2211.14769v3","updated":"2023-03-14T08:21:31Z","published":"2022-11-27T09:01:31Z","title":"Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied\n  Agents under Federated Learning","summary":"  Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n","authors":["Yunchao Zhang","Zonglin Di","Kaiwen Zhou","Cihang Xie","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.14769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11308v2","updated":"2023-03-14T08:16:51Z","published":"2022-08-24T05:29:47Z","title":"Deep model with built-in cross-attention alignment for acoustic echo\n  cancellation","summary":"  With recent research advances, deep learning models have become an attractive\nchoice for acoustic echo cancellation (AEC) in real-time teleconferencing\napplications. Since acoustic echo is one of the major sources of poor audio\nquality, a wide variety of deep models have been proposed. However, an\nimportant but often omitted requirement for good echo cancellation quality is\nthe synchronization of the microphone and far end signals. Typically\nimplemented using classical algorithms based on cross-correlation, the\nalignment module is a separate functional block with known design limitations.\nIn our work we propose a deep learning architecture with built-in\nself-attention based alignment, which is able to handle unaligned inputs,\nimproving echo cancellation performance while simplifying the communication\npipeline. Moreover, we show that our approach achieves significant improvements\nfor difficult delay estimation cases on real recordings from AEC Challenge data\nset.\n","authors":["Evgenii Indenbom","Nicolae-Cătălin Ristea","Ando Saabas","Tanel Pärnamaa","Jegor Gužvin"],"pdf_url":"https://arxiv.org/pdf/2208.11308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12268v3","updated":"2023-03-14T08:08:01Z","published":"2022-11-22T13:37:34Z","title":"Out-of-Candidate Rectification for Weakly Supervised Semantic\n  Segmentation","summary":"  Weakly supervised semantic segmentation is typically inspired by class\nactivation maps, which serve as pseudo masks with class-discriminative regions\nhighlighted. Although tremendous efforts have been made to recall precise and\ncomplete locations for each class, existing methods still commonly suffer from\nthe unsolicited Out-of-Candidate (OC) error predictions that not belongs to the\nlabel candidates, which could be avoidable since the contradiction with\nimage-level class tags is easy to be detected. In this paper, we develop a\ngroup ranking-based Out-of-Candidate Rectification (OCR) mechanism in a\nplug-and-play fashion. Firstly, we adaptively split the semantic categories\ninto In-Candidate (IC) and OC groups for each OC pixel according to their prior\nannotation correlation and posterior prediction correlation. Then, we derive a\ndifferentiable rectification loss to force OC pixels to shift to the IC group.\nIncorporating our OCR with seminal baselines (e.g., AffinityNet, SEAM,\nMCTformer), we can achieve remarkable performance gains on both Pascal VOC\n(+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with\nnegligible extra training overhead, which justifies the effectiveness and\ngenerality of our OCR.\n","authors":["Zesen Cheng","Pengchong Qiao","Kehan Li","Siheng Li","Pengxu Wei","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2211.12268v3.pdf","comment":"Accepted to CVPR2023"},{"id":"http://arxiv.org/abs/2303.07216v2","updated":"2023-03-14T07:48:31Z","published":"2023-03-13T15:51:38Z","title":"Parallel Vertex Diffusion for Unified Visual Grounding","summary":"  Unified visual grounding pursues a simple and generic technical route to\nleverage multi-task data with less task-specific design. The most advanced\nmethods typically present boxes and masks as vertex sequences to model\nreferring detection and segmentation as an autoregressive sequential vertex\ngeneration paradigm. However, generating high-dimensional vertex sequences\nsequentially is error-prone because the upstream of the sequence remains static\nand cannot be refined based on downstream vertex information, even if there is\na significant location gap. Besides, with limited vertexes, the inferior\nfitting of objects with complex contours restricts the performance upper bound.\nTo deal with this dilemma, we propose a parallel vertex generation paradigm for\nsuperior high-dimension scalability with a diffusion model by simply modifying\nthe noise dimension. An intuitive materialization of our paradigm is Parallel\nVertex Diffusion (PVD) to directly set vertex coordinates as the generation\ntarget and use a diffusion model to train and infer. We claim that it has two\nflaws: (1) unnormalized coordinate caused a high variance of loss value; (2)\nthe original training objective of PVD only considers point consistency but\nignores geometry consistency. To solve the first flaw, Center Anchor Mechanism\n(CAM) is designed to convert coordinates as normalized offset values to\nstabilize the training loss value. For the second flaw, Angle summation loss\n(ASL) is designed to constrain the geometry difference of prediction and ground\ntruth vertexes for geometry-level consistency. Empirical results show that our\nPVD achieves state-of-the-art in both referring detection and segmentation, and\nour paradigm is more scalable and efficient than sequential vertex generation\nwith high-dimension data.\n","authors":["Zesen Cheng","Kehan Li","Peng Jin","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06373v2","updated":"2023-03-14T07:47:53Z","published":"2023-03-11T10:44:44Z","title":"Recursive Generalization Transformer for Image Super-Resolution","summary":"  Transformer architectures have exhibited remarkable performance in image\nsuper-resolution (SR). Since the quadratic computational complexity of the\nself-attention (SA) in Transformer, existing methods tend to adopt SA in a\nlocal region to reduce overheads. However, the local design restricts the\nglobal context exploitation, which is critical for accurate image\nreconstruction. In this work, we propose the Recursive Generalization\nTransformer (RGT) for image SR, which can capture global spatial information\nand is suitable for high-resolution images. Specifically, we propose the\nrecursive-generalization self-attention (RG-SA). It recursively aggregates\ninput features into representative feature maps, and then utilizes\ncross-attention to extract global information. Meanwhile, the channel\ndimensions of attention matrices (query, key, and value) are further scaled for\na better trade-off between computational overheads and performance.\nFurthermore, we combine the RG-SA with local self-attention to enhance the\nexploitation of the global context, and propose the hybrid adaptive integration\n(HAI) for module integration. The HAI allows the direct and effective fusion\nbetween features at different levels (local or global). Extensive experiments\ndemonstrate that our RGT outperforms recent state-of-the-art methods.\n","authors":["Zheng Chen","Yulun Zhang","Jinjin Gu","Linghe Kong","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.06373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07679v1","updated":"2023-03-14T07:42:02Z","published":"2023-03-14T07:42:02Z","title":"Feature representations useful for predicting image memorability","summary":"  Predicting image memorability has attracted interest in various fields.\nConsequently, prediction accuracy with convolutional neural network (CNN)\nmodels has been approaching the empirical upper bound estimated based on human\nconsistency. However, identifying which feature representations embedded in CNN\nmodels are responsible for such high prediction accuracy of memorability\nremains an open question. To tackle this problem, this study sought to identify\nmemorability-related feature representations in CNN models using brain\nsimilarity. Specifically, memorability prediction accuracy and brain similarity\nwere examined and assessed by Brain-Score across 16,860 layers in 64 CNN models\npretrained for object recognition. A clear tendency was shown in this\ncomprehensive analysis that layers with high memorability prediction accuracy\nhad higher brain similarity with the inferior temporal (IT) cortex, which is\nthe highest stage in the ventral visual pathway. Furthermore, fine-tuning the\n64 CNN models revealed that brain similarity with the IT cortex at the\npenultimate layer was positively correlated with memorability prediction\naccuracy. This analysis also showed that the best fine-tuned model provided\naccuracy comparable to the state-of-the-art CNN models developed specifically\nfor memorability prediction. Overall, this study's results indicated that the\nCNN models' great success in predicting memorability relies on feature\nrepresentation acquisition similar to the IT cortex. This study advanced our\nunderstanding of feature representations and its use for predicting image\nmemorability.\n","authors":["Takumi Harada","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2303.07679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00290v2","updated":"2023-03-14T07:30:03Z","published":"2022-12-01T05:31:07Z","title":"Component Segmentation of Engineering Drawings Using Graph Convolutional\n  Networks","summary":"  We present a data-driven framework to automate the vectorization and machine\ninterpretation of 2D engineering part drawings. In industrial settings, most\nmanufacturing engineers still rely on manual reads to identify the topological\nand manufacturing requirements from drawings submitted by designers. The\ninterpretation process is laborious and time-consuming, which severely inhibits\nthe efficiency of part quotation and manufacturing tasks. While recent advances\nin image-based computer vision methods have demonstrated great potential in\ninterpreting natural images through semantic segmentation approaches, the\napplication of such methods in parsing engineering technical drawings into\nsemantically accurate components remains a significant challenge. The severe\npixel sparsity in engineering drawings also restricts the effective\nfeaturization of image-based data-driven methods. To overcome these challenges,\nwe propose a deep learning based framework that predicts the semantic type of\neach vectorized component. Taking a raster image as input, we vectorize all\ncomponents through thinning, stroke tracing, and cubic bezier fitting. Then a\ngraph of such components is generated based on the connectivity between the\ncomponents. Finally, a graph convolutional neural network is trained on this\ngraph data to identify the semantic type of each component. We test our\nframework in the context of semantic segmentation of text, dimension and,\ncontour components in engineering drawings. Results show that our method yields\nthe best performance compared to recent image, and graph-based segmentation\nmethods.\n","authors":["Wentai Zhang","Joe Joseph","Yue Yin","Liuyue Xie","Tomotake Furuhata","Soji Yamakawa","Kenji Shimada","Levent Burak Kara"],"pdf_url":"https://arxiv.org/pdf/2212.00290v2.pdf","comment":"Preprint accepted to Computers in Industry"},{"id":"http://arxiv.org/abs/2303.07677v1","updated":"2023-03-14T07:26:55Z","published":"2023-03-14T07:26:55Z","title":"Sr-init: An interpretable layer pruning method","summary":"  Despite the popularization of deep neural networks (DNNs) in many fields, it\nis still challenging to deploy state-of-the-art models to resource-constrained\ndevices due to high computational overhead. Model pruning provides a feasible\nsolution to the aforementioned challenges. However, the interpretation of\nexisting pruning criteria is always overlooked. To counter this issue, we\npropose a novel layer pruning method by exploring the Stochastic\nRe-initialization. Our SR-init method is inspired by the discovery that the\naccuracy drop due to stochastic re-initialization of layer parameters differs\nin various layers. On the basis of this observation, we come up with a layer\npruning criterion, i.e., those layers that are not sensitive to stochastic\nre-initialization (low accuracy drop) produce less contribution to the model\nand could be pruned with acceptable loss. Afterward, we experimentally verify\nthe interpretability of SR-init via feature visualization. The visual\nexplanation demonstrates that SR-init is theoretically feasible, thus we\ncompare it with state-of-the-art methods to further evaluate its\npracticability. As for ResNet56 on CIFAR-10 and CIFAR-100, SR-init achieves a\ngreat reduction in parameters (63.98% and 37.71%) with an ignorable drop in\ntop-1 accuracy (-0.56% and 0.8%). With ResNet50 on ImageNet, we achieve a\n15.59% FLOPs reduction by removing 39.29% of the parameters, with only a drop\nof 0.6% in top-1 accuracy. Our code is available at\nhttps://github.com/huitang-zjut/SRinit.\n","authors":["Hui Tang","Yao Lu","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2303.07677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07674v1","updated":"2023-03-14T07:25:38Z","published":"2023-03-14T07:25:38Z","title":"Koos Classification of Vestibular Schwannoma via Image Translation-Based\n  Unsupervised Cross-Modality Domain Adaptation","summary":"  The Koos grading scale is a classification system for vestibular schwannoma\n(VS) used to characterize the tumor and its effects on adjacent brain\nstructures. The Koos classification captures many of the characteristics of\ntreatment deci-sions and is often used to determine treatment plans. Although\nboth contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2)\nscanning can be used for Koos Classification, hrT2 scanning is gaining interest\nbecause of its higher safety and cost-effectiveness. However, in the absence of\nannotations for hrT2 scans, deep learning methods often inevitably suffer from\nperformance deg-radation due to unsupervised learning. If ceT1 scans and their\nannotations can be used for unsupervised learning of hrT2 scans, the\nperformance of Koos classifi-cation using unlabeled hrT2 scans will be greatly\nimproved. In this regard, we propose an unsupervised cross-modality domain\nadaptation method based on im-age translation by transforming annotated ceT1\nscans into hrT2 modality and us-ing their annotations to achieve supervised\nlearning of hrT2 modality. Then, the VS and 7 adjacent brain structures related\nto Koos classification in hrT2 scans were segmented. Finally, handcrafted\nfeatures are extracted from the segmenta-tion results, and Koos grade is\nclassified using a random forest classifier. The proposed method received rank\n1 on the Koos classification task of the Cross-Modality Domain Adaptation\n(crossMoDA 2022) challenge, with Macro-Averaged Mean Absolute Error (MA-MAE) of\n0.2148 for the validation set and 0.26 for the test set.\n","authors":["Tao Yang","Lisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07674v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07670v1","updated":"2023-03-14T07:23:27Z","published":"2023-03-14T07:23:27Z","title":"Co-Salient Object Detection with Co-Representation Purification","summary":"  Co-salient object detection (Co-SOD) aims at discovering the common objects\nin a group of relevant images. Mining a co-representation is essential for\nlocating co-salient objects. Unfortunately, the current Co-SOD method does not\npay enough attention that the information not related to the co-salient object\nis included in the co-representation. Such irrelevant information in the\nco-representation interferes with its locating of co-salient objects. In this\npaper, we propose a Co-Representation Purification (CoRP) method aiming at\nsearching noise-free co-representation. We search a few pixel-wise embeddings\nprobably belonging to co-salient regions. These embeddings constitute our\nco-representation and guide our prediction. For obtaining purer\nco-representation, we use the prediction to iteratively reduce irrelevant\nembeddings in our co-representation. Experiments on three datasets demonstrate\nthat our CoRP achieves state-of-the-art performances on the benchmark datasets.\nOur source code is available at https://github.com/ZZY816/CoRP.\n","authors":["Ziyue Zhu","Zhao Zhang","Zheng Lin","Xing Sun","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.07670v1.pdf","comment":"Accepted by TPAMI 2023"},{"id":"http://arxiv.org/abs/2303.07662v1","updated":"2023-03-14T07:07:34Z","published":"2023-03-14T07:07:34Z","title":"One scalar is all you need -- absolute depth estimation using monocular\n  self-supervision","summary":"  Self-supervised monocular depth estimators can be trained or fine-tuned on\nnew scenes using only images and no ground-truth depth data, achieving good\naccuracy. However, these estimators suffer from the inherent ambiguity of the\ndepth scale, significantly limiting their applicability. In this work, we\npresent a method for transferring the depth-scale from existing source datasets\ncollected with ground-truth depths to depth estimators that are trained using\nself-supervision on a newly collected target dataset consisting of images only,\nsolving a significant limiting factor. We show that self-supervision based on\nprojective geometry results in predicted depths that are linearly correlated\nwith their ground-truth depths. Moreover, the linearity of this relationship\nalso holds when jointly training on images from two different (real or\nsynthetic) source and target domains. We utilize this observed property and\nmodel the relationship between the ground-truth and the predicted up-to-scale\ndepths of images from the source domain using a single global scalar. Then, we\nscale the predicted up-to-scale depths of images from the target domain using\nthe estimated global scaling factor, performing depth-scale transfer between\nthe two domains. This suggested method was evaluated on the target KITTI and\nDDAD datasets, while using other real or synthetic source datasets, that have a\nlarger field-of-view, other image style or structural content. Our approach\nachieves competitive accuracy on KITTI, even without using the specially\ntailored vKITTI or vKITTI2 datasets, and higher accuracy on DDAD, when using\nboth real or synthetic source datasets.\n","authors":["Alexandra Dana","Nadav Carmel","Amit Shomer","Ofer Manela","Tomer Peleg"],"pdf_url":"https://arxiv.org/pdf/2303.07662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.09869v2","updated":"2023-03-14T07:03:54Z","published":"2023-01-24T09:09:35Z","title":"Image Super-Resolution using Efficient Striped Window Transformer","summary":"  Transformers have achieved remarkable results in single-image\nsuper-resolution (SR). However, the challenge of balancing model performance\nand complexity has hindered their application in lightweight SR (LSR). To\ntackle this challenge, we propose an efficient striped window transformer\n(ESWT). We revisit the normalization layer in the transformer and design a\nconcise and efficient transformer structure to build the ESWT. Furthermore, we\nintroduce a striped window mechanism to model long-term dependencies more\nefficiently. To fully exploit the potential of the ESWT, we propose a novel\nflexible window training strategy that can improve the performance of the ESWT\nwithout additional cost. Extensive experiments show that ESWT outperforms\nstate-of-the-art LSR transformers, and achieves a better trade-off between\nmodel performance and complexity. The ESWT requires fewer parameters, incurs\nfaster inference, smaller FLOPs, and less memory consumption, making it a\npromising solution for LSR.\n","authors":["Jinpeng Shi","Hui Li","Tianle Liu","Yulong Liu","Mingjian Zhang","Jinchen Zhu","Ling Zheng","Shizhuang Weng"],"pdf_url":"https://arxiv.org/pdf/2301.09869v2.pdf","comment":"SOTA lightweight super-resolution transformer. 8 pages, 9 figures and\n  6 tables. The Code is available at\n  https://github.com/Fried-Rice-Lab/FriedRiceLab"},{"id":"http://arxiv.org/abs/2303.07653v1","updated":"2023-03-14T06:45:13Z","published":"2023-03-14T06:45:13Z","title":"NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from\n  Multi-view Images","summary":"  We study the problem of reconstructing 3D feature curves of an object from a\nset of calibrated multi-view images. To do so, we learn a neural implicit field\nrepresenting the density distribution of 3D edges which we refer to as Neural\nEdge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based\nrendering loss where a 2D edge map is rendered at a given view and is compared\nto the ground-truth edge map extracted from the image of that view. The\nrendering-based differentiable optimization of NEF fully exploits 2D edge\ndetection, without needing a supervision of 3D edges, a 3D geometric operator\nor cross-view edge correspondence. Several technical designs are devised to\nensure learning a range-limited and view-independent NEF for robust edge\nextraction. The final parametric 3D curves are extracted from NEF with an\niterative optimization method. On our benchmark with synthetic data, we\ndemonstrate that NEF outperforms existing state-of-the-art methods on all\nmetrics. Project page: https://yunfan1202.github.io/NEF/.\n","authors":["Yunfan Ye","Renjiao Yi","Zhirui Gao","Chenyang Zhu","Zhiping Cai","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2303.07653v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07651v1","updated":"2023-03-14T06:38:17Z","published":"2023-03-14T06:38:17Z","title":"Context Normalization for Robust Image Classification","summary":"  Normalization is a pre-processing step that converts the data into a more\nusable representation. As part of the deep neural networks (DNNs), the batch\nnormalization (BN) technique uses normalization to address the problem of\ninternal covariate shift. It can be packaged as general modules, which have\nbeen extensively integrated into various DNNs, to stabilize and accelerate\ntraining, presumably leading to improved generalization. However, the effect of\nBN is dependent on the mini-batch size and it does not take into account any\ngroups or clusters that may exist in the dataset when estimating population\nstatistics. This study proposes a new normalization technique, called context\nnormalization, for image data. This approach adjusts the scaling of features\nbased on the characteristics of each sample, which improves the model's\nconvergence speed and performance by adapting the data values to the context of\nthe target task. The effectiveness of context normalization is demonstrated on\nvarious datasets, and its performance is compared to other standard\nnormalization techniques.\n","authors":["Bilal Faye","Mohamed-Djallel Dilmi","Hanane Azzag","Mustapha Lebbah","Fangchen Feng"],"pdf_url":"https://arxiv.org/pdf/2303.07651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07648v1","updated":"2023-03-14T06:30:55Z","published":"2023-03-14T06:30:55Z","title":"SimFLE: Simple Facial Landmark Encoding for Self-Supervised Facial\n  Expression Recognition in the Wild","summary":"  One of the key issues in facial expression recognition in the wild (FER-W) is\nthat curating large-scale labeled facial images is challenging due to the\ninherent complexity and ambiguity of facial images. Therefore, in this paper,\nwe propose a self-supervised simple facial landmark encoding (SimFLE) method\nthat can learn effective encoding of facial landmarks, which are important\nfeatures for improving the performance of FER-W, without expensive labels.\nSpecifically, we introduce novel FaceMAE module for this purpose. FaceMAE\nreconstructs masked facial images with elaborately designed semantic masking.\nUnlike previous random masking, semantic masking is conducted based on channel\ninformation processed in the backbone, so rich semantics of channels can be\nexplored. Additionally, the semantic masking process is fully trainable,\nenabling FaceMAE to guide the backbone to learn spatial details and contextual\nproperties of fine-grained facial landmarks. Experimental results on several\nFER-W benchmarks prove that the proposed SimFLE is superior in facial landmark\nlocalization and noticeably improved performance compared to the supervised\nbaseline and other self-supervised methods.\n","authors":["Jiyong Moon","Seongsik Park"],"pdf_url":"https://arxiv.org/pdf/2303.07648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08427v2","updated":"2023-03-14T06:07:21Z","published":"2022-07-18T08:22:18Z","title":"Adaptive Assignment for Geometry Aware Local Feature Matching","summary":"  The detector-free feature matching approaches are currently attracting great\nattention thanks to their excellent performance. However, these methods still\nstruggle at large-scale and viewpoint variations, due to the geometric\ninconsistency resulting from the application of the mutual nearest neighbour\ncriterion (\\ie, one-to-one assignment) in patch-level matching.Accordingly, we\nintroduce AdaMatcher, which first accomplishes the feature correlation and\nco-visible area estimation through an elaborate feature interaction module,\nthen performs adaptive assignment on patch-level matching while estimating the\nscales between images, and finally refines the co-visible matches through scale\nalignment and sub-pixel regression module.Extensive experiments show that\nAdaMatcher outperforms solid baselines and achieves state-of-the-art results on\nmany downstream tasks. Additionally, the adaptive assignment and sub-pixel\nrefinement module can be used as a refinement network for other matching\nmethods, such as SuperGlue, to boost their performance further. The code will\nbe publicly available at https://github.com/AbyssGaze/AdaMatcher.\n","authors":["Dihe Huang","Ying Chen","Shang Xu","Yong Liu","Wenlong Wu","Yikang Ding","Chengjie Wang","Fan Tang"],"pdf_url":"https://arxiv.org/pdf/2207.08427v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07641v1","updated":"2023-03-14T06:03:57Z","published":"2023-03-14T06:03:57Z","title":"Rethinking Image-based Table Recognition Using Weakly Supervised Methods","summary":"  Most of the previous methods for table recognition rely on training datasets\ncontaining many richly annotated table images. Detailed table image annotation,\ne.g., cell or text bounding box annotation, however, is costly and often\nsubjective. In this paper, we propose a weakly supervised model named WSTabNet\nfor table recognition that relies only on HTML (or LaTeX) code-level\nannotations of table images. The proposed model consists of three main parts:\nan encoder for feature extraction, a structure decoder for generating table\nstructure, and a cell decoder for predicting the content of each cell in the\ntable. Our system is trained end-to-end by stochastic gradient descent\nalgorithms, requiring only table images and their ground-truth HTML (or LaTeX)\nrepresentations. To facilitate table recognition with deep learning, we create\nand release WikiTableSet, the largest publicly available image-based table\nrecognition dataset built from Wikipedia. WikiTableSet contains nearly 4\nmillion English table images, 590K Japanese table images, and 640k French table\nimages with corresponding HTML representation and cell bounding boxes. The\nextensive experiments on WikiTableSet and two large-scale datasets: FinTabNet\nand PubTabNet demonstrate that the proposed weakly supervised model achieves\nbetter, or similar accuracies compared to the state-of-the-art models on all\nbenchmark datasets.\n","authors":["Nam Tuan Ly","Atsuhiro Takasu","Phuc Nguyen","Hideaki Takeda"],"pdf_url":"https://arxiv.org/pdf/2303.07641v1.pdf","comment":"10 pages, ICPRAM2023"},{"id":"http://arxiv.org/abs/2209.14609v4","updated":"2023-03-14T05:56:18Z","published":"2022-09-29T07:58:32Z","title":"Dataset Distillation Using Parameter Pruning","summary":"  In many fields, the acquisition of advanced models depends on large datasets,\nmaking data storage and model training expensive. As a solution, dataset\ndistillation can synthesize a small dataset that preserves most information of\nthe original large dataset. The recently proposed dataset distillation method\nby matching network parameters has been proven effective for several datasets.\nHowever, the dimensions of network parameters are typically large. Furthermore,\nsome parameters are difficult to match during the distillation process,\ndegrading distillation performance. Based on this observation, this study\nproposes a novel dataset distillation method based on parameter pruning that\nsolves the problem. The proposed method can synthesize more robust distilled\ndatasets and improve distillation performance by pruning difficult-to-match\nparameters during the distillation process. Experimental results on three\ndatasets show that the proposed method outperforms other state-of-the-art\ndataset distillation methods.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2209.14609v4.pdf","comment":"Submitted as a journal paper at IEEE SPL"},{"id":"http://arxiv.org/abs/2303.07634v1","updated":"2023-03-14T05:29:34Z","published":"2023-03-14T05:29:34Z","title":"I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via\n  Raytracing in Neural SDFs","summary":"  In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene\nreconstruction and editing using differentiable Monte Carlo raytracing on\nneural signed distance fields (SDFs). Our holistic neural SDF-based framework\njointly recovers the underlying shapes, incident radiance and materials from\nmulti-view images. We introduce a novel bubble loss for fine-grained small\nobjects and error-guided adaptive sampling scheme to largely improve the\nreconstruction quality on large-scale indoor scenes. Further, we propose to\ndecompose the neural radiance field into spatially-varying material of the\nscene as a neural field through surface-based, differentiable Monte Carlo\nraytracing and emitter semantic segmentations, which enables physically based\nand photorealistic scene relighting and editing applications. Through a number\nof qualitative and quantitative experiments, we demonstrate the superior\nquality of our method on indoor scene reconstruction, novel view synthesis, and\nscene editing compared to state-of-the-art baselines.\n","authors":["Jingsen Zhu","Yuchi Huo","Qi Ye","Fujun Luan","Jifan Li","Dianbing Xi","Lisha Wang","Rui Tang","Wei Hua","Hujun Bao","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07634v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2209.13679v3","updated":"2023-03-14T04:58:08Z","published":"2022-09-27T20:34:41Z","title":"V2XP-ASG: Generating Adversarial Scenes for Vehicle-to-Everything\n  Perception","summary":"  Recent advancements in Vehicle-to-Everything communication technology have\nenabled autonomous vehicles to share sensory information to obtain better\nperception performance. With the rapid growth of autonomous vehicles and\nintelligent infrastructure, the V2X perception systems will soon be deployed at\nscale, which raises a safety-critical question: \\textit{how can we evaluate and\nimprove its performance under challenging traffic scenarios before the\nreal-world deployment?} Collecting diverse large-scale real-world test scenes\nseems to be the most straightforward solution, but it is expensive and\ntime-consuming, and the collections can only cover limited scenarios. To this\nend, we propose the first open adversarial scene generator V2XP-ASG that can\nproduce realistic, challenging scenes for modern LiDAR-based multi-agent\nperception systems. V2XP-ASG learns to construct an adversarial collaboration\ngraph and simultaneously perturb multiple agents' poses in an adversarial and\nplausible manner. The experiments demonstrate that V2XP-ASG can effectively\nidentify challenging scenes for a large range of V2X perception systems.\nMeanwhile, by training on the limited number of generated challenging scenes,\nthe accuracy of V2X perception systems can be further improved by 12.3\\% on\nchallenging and 4\\% on normal scenes. Our code will be released at\nhttps://github.com/XHwind/V2XP-ASG.\n","authors":["Hao Xiang","Runsheng Xu","Xin Xia","Zhaoliang Zheng","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2209.13679v3.pdf","comment":"ICRA 2023, see https://github.com/XHwind/V2XP-ASG"},{"id":"http://arxiv.org/abs/2303.07625v1","updated":"2023-03-14T04:48:18Z","published":"2023-03-14T04:48:18Z","title":"PlanarTrack: A Large-scale Challenging Benchmark for Planar Object\n  Tracking","summary":"  Planar object tracking is a critical computer vision problem and has drawn\nincreasing interest owing to its key roles in robotics, augmented reality, etc.\nDespite rapid progress, its further development, especially in the deep\nlearning era, is largely hindered due to the lack of large-scale challenging\nbenchmarks. Addressing this, we introduce PlanarTrack, a large-scale\nchallenging planar tracking benchmark. Specifically, PlanarTrack consists of\n1,000 videos with more than 490K images. All these videos are collected in\ncomplex unconstrained scenarios from the wild, which makes PlanarTrack,\ncompared with existing benchmarks, more challenging but realistic for\nreal-world applications. To ensure the high-quality annotation, each frame in\nPlanarTrack is manually labeled using four corners with multiple-round careful\ninspection and refinement. To our best knowledge, PlanarTrack, to date, is the\nlargest and most challenging dataset dedicated to planar object tracking. In\norder to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and\nconduct comprehensive comparisons and in-depth analysis. Our results, not\nsurprisingly, demonstrate that current top-performing planar trackers\ndegenerate significantly on the challenging PlanarTrack and more efforts are\nneeded to improve planar tracking in the future. In addition, we further derive\na variant named PlanarTrack$_{\\mathbf{BB}}$ for generic object tracking from\nPlanarTrack. Our evaluation of 10 excellent generic trackers on\nPlanarTrack$_{\\mathrm{BB}}$ manifests that, surprisingly,\nPlanarTrack$_{\\mathrm{BB}}$ is even more challenging than several popular\ngeneric tracking benchmarks and more attention should be paid to handle such\nplanar objects, though they are rigid. All benchmarks and evaluations will be\nreleased at the project webpage.\n","authors":["Xinran Liu","Xiaoqiong Liu","Ziruo Yi","Xin Zhou","Thanh Le","Libo Zhang","Yan Huang","Qing Yang","Heng Fan"],"pdf_url":"https://arxiv.org/pdf/2303.07625v1.pdf","comment":"Tech. Report"},{"id":"http://arxiv.org/abs/2303.07618v1","updated":"2023-03-14T03:57:16Z","published":"2023-03-14T03:57:16Z","title":"Medical Phrase Grounding with Region-Phrase Context Contrastive\n  Alignment","summary":"  Medical phrase grounding (MPG) aims to locate the most relevant region in a\nmedical image, given a phrase query describing certain medical findings, which\nis an important task for medical image analysis and radiological diagnosis.\nHowever, existing visual grounding methods rely on general visual features for\nidentifying objects in natural images and are not capable of capturing the\nsubtle and specialized features of medical findings, leading to sub-optimal\nperformance in MPG. In this paper, we propose MedRPG, an end-to-end approach\nfor MPG. MedRPG is built on a lightweight vision-language transformer encoder\nand directly predicts the box coordinates of mentioned medical findings, which\ncan be trained with limited medical data, making it a valuable tool in medical\nimage analysis. To enable MedRPG to locate nuanced medical findings with better\nregion-phrase correspondences, we further propose Tri-attention Context\ncontrastive alignment (TaCo). TaCo seeks context alignment to pull both the\nfeatures and attention outputs of relevant region-phrase pairs close together\nwhile pushing those of irrelevant regions far away. This ensures that the final\nbox prediction depends more on its finding-specific regions and phrases.\nExperimental results on three MPG datasets demonstrate that our MedRPG\noutperforms state-of-the-art visual grounding approaches by a large margin.\nAdditionally, the proposed TaCo strategy is effective in enhancing finding\nlocalization ability and reducing spurious region-phrase correlations.\n","authors":["Zhihao Chen","Yang Zhou","Anh Tran","Junting Zhao","Liang Wan","Gideon Ooi","Lionel Cheng","Choon Hua Thng","Xinxing Xu","Yong Liu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.07618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07615v1","updated":"2023-03-14T03:42:47Z","published":"2023-03-14T03:42:47Z","title":"Variation of Gender Biases in Visual Recognition Models Before and After\n  Finetuning","summary":"  We introduce a framework to measure how biases change before and after\nfine-tuning a large scale visual recognition model for a downstream task. Deep\nlearning models trained on increasing amounts of data are known to encode\nsocietal biases. Many computer vision systems today rely on models typically\npretrained on large scale datasets. While bias mitigation techniques have been\ndeveloped for tuning models for downstream tasks, it is currently unclear what\nare the effects of biases already encoded in a pretrained model. Our framework\nincorporates sets of canonical images representing individual and pairs of\nconcepts to highlight changes in biases for an array of off-the-shelf\npretrained models across model sizes, dataset sizes, and training objectives.\nThrough our analyses, we find that (1) supervised models trained on datasets\nsuch as ImageNet-21k are more likely to retain their pretraining biases\nregardless of the target dataset compared to self-supervised models. We also\nfind that (2) models finetuned on larger scale datasets are more likely to\nintroduce new biased associations. Our results also suggest that (3) biases can\ntransfer to finetuned models and the finetuning objective and dataset can\nimpact the extent of transferred biases.\n","authors":["Jaspreet Ranjit","Tianlu Wang","Baishakhi Ray","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2303.07615v1.pdf","comment":"10 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2301.07074v3","updated":"2023-03-14T03:36:41Z","published":"2023-01-17T18:36:57Z","title":"SegViz: A federated-learning based framework for multi-organ\n  segmentation on heterogeneous data sets with partial annotations","summary":"  Segmentation is one of the most primary tasks in deep learning for medical\nimaging, owing to its multiple downstream clinical applications. However,\ngenerating manual annotations for medical images is time-consuming, requires\nhigh skill, and is an expensive effort, especially for 3D images. One potential\nsolution is to aggregate knowledge from partially annotated datasets from\nmultiple groups to collaboratively train global models using Federated\nLearning. To this end, we propose SegViz, a federated learning-based framework\nto train a segmentation model from distributed non-i.i.d datasets with partial\nannotations. The performance of SegViz was compared against training individual\nmodels separately on each dataset as well as centrally aggregating all the\ndatasets in one place and training a single model. The SegViz framework using\nFedBN as the aggregation strategy demonstrated excellent performance on the\nexternal BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for\nsegmentation of liver, spleen, pancreas, and kidneys, respectively,\nsignificantly ($p<0.05$) better (except spleen) than the dice scores of 0.87,\n0.83, 0.42, and 0.48 for the baseline models. In contrast, the central\naggregation model significantly ($p<0.05$) performed poorly on the test dataset\nwith dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the\npotential of the SegViz framework to train multi-task models from distributed\ndatasets with partial labels. All our implementations are open-source and\navailable at https://anonymous.4open.science/r/SegViz-B746\n","authors":["Adway U. Kanhere","Pranav Kulkarni","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.07074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.14360v2","updated":"2023-03-14T03:32:35Z","published":"2022-03-27T17:57:08Z","title":"Observation-Centric SORT: Rethinking SORT for Robust Multi-Object\n  Tracking","summary":"  Kalman filter (KF) based methods for multi-object tracking (MOT) make an\nassumption that objects move linearly. While this assumption is acceptable for\nvery short periods of occlusion, linear estimates of motion for prolonged time\ncan be highly inaccurate. Moreover, when there is no measurement available to\nupdate Kalman filter parameters, the standard convention is to trust the priori\nstate estimations for posteriori update. This leads to the accumulation of\nerrors during a period of occlusion. The error causes significant motion\ndirection variance in practice. In this work, we show that a basic Kalman\nfilter can still obtain state-of-the-art tracking performance if proper care is\ntaken to fix the noise accumulated during occlusion. Instead of relying only on\nthe linear state estimate (i.e., estimation-centric approach), we use object\nobservations (i.e., the measurements by object detector) to compute a virtual\ntrajectory over the occlusion period to fix the error accumulation of filter\nparameters during the occlusion period. This allows more time steps to correct\nerrors accumulated during occlusion. We name our method Observation-Centric\nSORT (OC-SORT). It remains Simple, Online, and Real-Time but improves\nrobustness during occlusion and non-linear motion. Given off-the-shelf\ndetections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves\nstate-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head\ntracking, and especially DanceTrack where the object motion is highly\nnon-linear. The code and models are available at\n\\url{https://github.com/noahcao/OC_SORT}.\n","authors":["Jinkun Cao","Jiangmiao Pang","Xinshuo Weng","Rawal Khirodkar","Kris Kitani"],"pdf_url":"https://arxiv.org/pdf/2203.14360v2.pdf","comment":"Accepted by CVPR 2023. 8 pages + 10 pages of appendix. Renamed OOS as\n  Observation-centric Re-Update (ORU)"},{"id":"http://arxiv.org/abs/2303.07609v1","updated":"2023-03-14T03:09:56Z","published":"2023-03-14T03:09:56Z","title":"Training Robust Spiking Neural Networks with ViewPoint Transform and\n  SpatioTemporal Stretching","summary":"  Neuromorphic vision sensors (event cameras) simulate biological visual\nperception systems and have the advantages of high temporal resolution, less\ndata redundancy, low power consumption, and large dynamic range. Since both\nevents and spikes are modeled from neural signals, event cameras are inherently\nsuitable for spiking neural networks (SNNs), which are considered promising\nmodels for artificial intelligence (AI) and theoretical neuroscience. However,\nthe unconventional visual signals of these cameras pose a great challenge to\nthe robustness of spiking neural networks. In this paper, we propose a novel\ndata augmentation method, ViewPoint Transform and SpatioTemporal Stretching\n(VPT-STS). It improves the robustness of SNNs by transforming the rotation\ncenters and angles in the spatiotemporal domain to generate samples from\ndifferent viewpoints. Furthermore, we introduce the spatiotemporal stretching\nto avoid potential information loss in viewpoint transformation. Extensive\nexperiments on prevailing neuromorphic datasets demonstrate that VPT-STS is\nbroadly effective on multi-event representations and significantly outperforms\npure spatial geometric transformations. Notably, the SNNs model with VPT-STS\nachieves a state-of-the-art accuracy of 84.4\\% on the DVS-CIFAR10 dataset.\n","authors":["Haibo Shen","Juyu Xiao","Yihao Luo","Xiang Cao","Liangqi Zhang","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07609v1.pdf","comment":"Accepted by ICASSP 2023. arXiv admin note: text overlap with\n  arXiv:2207.11659"},{"id":"http://arxiv.org/abs/2209.07959v2","updated":"2023-03-14T03:03:52Z","published":"2022-09-16T14:19:48Z","title":"Towards Bridging the Performance Gaps of Joint Energy-based Models","summary":"  Can we train a hybrid discriminative-generative model within a single\nnetwork? This question has recently been answered in the affirmative,\nintroducing the field of Joint Energy-based Model (JEM), which achieves high\nclassification accuracy and image generation quality simultaneously. Despite\nrecent advances, there remain two performance gaps: the accuracy gap to the\nstandard softmax classifier, and the generation quality gap to state-of-the-art\ngenerative models. In this paper, we introduce a variety of training techniques\nto bridge the accuracy gap and the generation quality gap of JEM. 1) We\nincorporate a recently proposed sharpness-aware minimization (SAM) framework to\ntrain JEM, which promotes the energy landscape smoothness and the\ngeneralizability of JEM. 2) We exclude data augmentation from the maximum\nlikelihood estimate pipeline of JEM, and mitigate the negative impact of data\naugmentation to image generation quality. Extensive experiments on multiple\ndatasets demonstrate that our SADA-JEM achieves state-of-the-art performances\nand outperforms JEM in image classification, image generation, calibration,\nout-of-distribution detection and adversarial robustness by a notable margin.\nOur code is available at https://github.com/sndnyang/SADAJEM.\n","authors":["Xiulong Yang","Qing Su","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2209.07959v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07606v1","updated":"2023-03-14T03:01:24Z","published":"2023-03-14T03:01:24Z","title":"PSNet: a deep learning model based digital phase shifting algorithm from\n  a single fringe image","summary":"  As the gold standard for phase retrieval, phase-shifting algorithm (PS) has\nbeen widely used in optical interferometry, fringe projection profilometry,\netc. However, capturing multiple fringe patterns in PS limits the algorithm to\nonly a narrow range of application. To this end, a deep learning (DL) model\nbased digital PS algorithm from only a single fringe image is proposed. By\ntraining on a simulated dataset of PS fringe patterns, the learnt model,\ndenoted PSNet, can predict fringe patterns with other PS steps when given a\npattern with the first PS step. Simulation and experiment results demonstrate\nthe PSNet's promising performance on accurate prediction of digital PS\npatterns, and robustness to complex scenarios such as surfaces with varying\ncurvature and reflectance.\n","authors":["Zhaoshuai Qi","Xiaojun Liu","Xiaolin Liu","Jiaqi Yang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07606v1.pdf","comment":"5 pages9 figures, a letter"},{"id":"http://arxiv.org/abs/2303.07605v1","updated":"2023-03-14T02:58:27Z","published":"2023-03-14T02:58:27Z","title":"Modeling Continuous Motion for 3D Point Cloud Object Tracking","summary":"  The task of 3D single object tracking (SOT) with LiDAR point clouds is\ncrucial for various applications, such as autonomous driving and robotics.\nHowever, existing approaches have primarily relied on appearance matching or\nmotion modeling within only two successive frames, thereby overlooking the\nlong-range continuous motion property of objects in 3D space. To address this\nissue, this paper presents a novel approach that views each tracklet as a\ncontinuous stream: at each timestamp, only the current frame is fed into the\nnetwork to interact with multi-frame historical features stored in a memory\nbank, enabling efficient exploitation of sequential information. To achieve\neffective cross-frame message passing, a hybrid attention mechanism is designed\nto account for both long-range relation modeling and local geometric feature\nextraction. Furthermore, to enhance the utilization of multi-frame features for\nrobust tracking, a contrastive sequence enhancement strategy is designed, which\nuses ground truth tracklets to augment training sequences and promote\ndiscrimination against false positives in a contrastive manner. Extensive\nexperiments demonstrate that the proposed method outperforms the\nstate-of-the-art method by significant margins (approximately 8%, 6%, and 12%\nimprovements in the success performance on KITTI, nuScenes, and Waymo,\nrespectively).\n","authors":["Zhipeng Luo","Gongjie Zhang","Changqing Zhou","Zhonghua Wu","Qingyi Tao","Lewei Lu","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2303.07605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11659v3","updated":"2023-03-14T02:52:54Z","published":"2022-07-24T04:23:56Z","title":"Training Robust Spiking Neural Networks on Neuromorphic Data with\n  Spatiotemporal Fragments","summary":"  Neuromorphic vision sensors (event cameras) are inherently suitable for\nspiking neural networks (SNNs) and provide novel neuromorphic vision data for\nthis biomimetic model. Due to the spatiotemporal characteristics, novel data\naugmentations are required to process the unconventional visual signals of\nthese cameras. In this paper, we propose a novel Event SpatioTemporal Fragments\n(ESTF) augmentation method. It preserves the continuity of neuromorphic data by\ndrifting or inverting fragments of the spatiotemporal event stream to simulate\nthe disturbance of brightness variations, leading to more robust spiking neural\nnetworks. Extensive experiments are performed on prevailing neuromorphic\ndatasets. It turns out that ESTF provides substantial improvements over pure\ngeometric transformations and outperforms other event data augmentation\nmethods. It is worth noting that the SNNs with ESTF achieve the\nstate-of-the-art accuracy of 83.9\\% on the CIFAR10-DVS dataset.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11659v3.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2207.11670v2","updated":"2023-03-14T02:51:07Z","published":"2022-07-24T06:12:23Z","title":"Training Stronger Spiking Neural Networks with Biomimetic Adaptive\n  Internal Association Neurons","summary":"  As the third generation of neural networks, spiking neural networks (SNNs)\nare dedicated to exploring more insightful neural mechanisms to achieve\nnear-biological intelligence. Intuitively, biomimetic mechanisms are crucial to\nunderstanding and improving SNNs. For example, the associative long-term\npotentiation (ALTP) phenomenon suggests that in addition to learning mechanisms\nbetween neurons, there are associative effects within neurons. However, most\nexisting methods only focus on the former and lack exploration of the internal\nassociation effects. In this paper, we propose a novel Adaptive Internal\nAssociation~(AIA) neuron model to establish previously ignored influences\nwithin neurons. Consistent with the ALTP phenomenon, the AIA neuron model is\nadaptive to input stimuli, and internal associative learning occurs only when\nboth dendrites are stimulated at the same time. In addition, we employ weighted\nweights to measure internal associations and introduce intermediate caches to\nreduce the volatility of associations. Extensive experiments on prevailing\nneuromorphic datasets show that the proposed method can potentiate or depress\nthe firing of spikes more specifically, resulting in better performance with\nfewer spikes. It is worth noting that without adding any parameters at\ninference, the AIA model achieves state-of-the-art performance on\nDVS-CIFAR10~(83.9\\%) and N-CARS~(95.64\\%) datasets.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11670v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07601v1","updated":"2023-03-14T02:49:20Z","published":"2023-03-14T02:49:20Z","title":"V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle\n  Cooperative Perception","summary":"  Modern perception systems of autonomous vehicles are known to be sensitive to\nocclusions and lack the capability of long perceiving range. It has been one of\nthe key bottlenecks that prevents Level 5 autonomy. Recent research has\ndemonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system\nhas great potential to revolutionize the autonomous driving industry. However,\nthe lack of a real-world dataset hinders the progress of this field. To\nfacilitate the development of cooperative perception, we present V2V4Real, the\nfirst large-scale real-world multi-modal dataset for V2V perception. The data\nis collected by two vehicles equipped with multi-modal sensors driving together\nthrough diverse scenarios. Our V2V4Real dataset covers a driving area of 410\nkm, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding\nboxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real\nintroduces three perception tasks, including cooperative 3D object detection,\ncooperative 3D object tracking, and Sim2Real domain adaptation for cooperative\nperception. We provide comprehensive benchmarks of recent cooperative\nperception algorithms on three tasks. The V2V4Real dataset and codebase can be\nfound at https://github.com/ucla-mobility/V2V4Real.\n","authors":["Runsheng Xu","Xin Xia","Jinlong Li","Hanzhao Li","Shuo Zhang","Zhengzhong Tu","Zonglin Meng","Hao Xiang","Xiaoyu Dong","Rui Song","Hongkai Yu","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2303.07601v1.pdf","comment":"Accepted by CVPR2023. Code link:\n  https://github.com/ucla-mobility/V2V4Real"},{"id":"http://arxiv.org/abs/2210.04150v2","updated":"2023-03-14T02:48:42Z","published":"2022-10-09T02:57:32Z","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP","summary":"  Open-vocabulary semantic segmentation aims to segment an image into semantic\nregions according to text descriptions, which may not have been seen during\ntraining. Recent two-stage methods first generate class-agnostic mask proposals\nand then leverage pre-trained vision-language models, e.g., CLIP, to classify\nmasked regions. We identify the performance bottleneck of this paradigm to be\nthe pre-trained CLIP model, since it does not perform well on masked images. To\naddress this, we propose to finetune CLIP on a collection of masked image\nregions and their corresponding text descriptions. We collect training data by\nmining an existing image-caption dataset (e.g., COCO Captions), using CLIP to\nmatch masked image regions to nouns in the image captions. Compared with the\nmore precise and manually annotated segmentation labels with fixed classes\n(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain\nCLIP's generalization ability. Along with finetuning the entire model, we\nutilize the \"blank\" areas in masked images using a method we dub mask prompt\ntuning. Experiments demonstrate mask prompt tuning brings significant\nimprovement without modifying any weights of CLIP, and it can further improve a\nfully finetuned model. In particular, when trained on COCO and evaluated on\nADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the\nprevious state-of-the-art. For the first time, open-vocabulary generalist\nmodels match the performance of supervised specialist models in 2017 without\ndataset-specific adaptations.\n","authors":["Feng Liang","Bichen Wu","Xiaoliang Dai","Kunpeng Li","Yinan Zhao","Hang Zhang","Peizhao Zhang","Peter Vajda","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2210.04150v2.pdf","comment":"CVPR 2023. Project page: https://jeff-liangf.github.io/projects/ovseg"},{"id":"http://arxiv.org/abs/2303.07598v1","updated":"2023-03-14T02:42:01Z","published":"2023-03-14T02:42:01Z","title":"AdPE: Adversarial Positional Embeddings for Pretraining Vision\n  Transformers via MAE+","summary":"  Unsupervised learning of vision transformers seeks to pretrain an encoder via\npretext tasks without labels. Among them is the Masked Image Modeling (MIM)\naligned with pretraining of language transformers by predicting masked patches\nas a pretext task. A criterion in unsupervised pretraining is the pretext task\nneeds to be sufficiently hard to prevent the transformer encoder from learning\ntrivial low-level features not generalizable well to downstream tasks. For this\npurpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It\ndistorts the local visual structures by perturbing the position encodings so\nthat the learned transformer cannot simply use the locally correlated patches\nto predict the missing ones. We hypothesize that it forces the transformer\nencoder to learn more discriminative features in a global context with stronger\ngeneralizability to downstream tasks. We will consider both absolute and\nrelative positional encodings, where adversarial positions can be imposed both\nin the embedding mode and the coordinate mode. We will also present a new MAE+\nbaseline that brings the performance of the MIM pretraining to a new level with\nthe AdPE. The experiments demonstrate that our approach can improve the\nfine-tuning accuracy of MAE by $0.8\\%$ and $0.4\\%$ over 1600 epochs of\npretraining ViT-B and ViT-L on Imagenet1K. For the transfer learning task, it\noutperforms the MAE with the ViT-B backbone by $2.6\\%$ in mIoU on ADE20K, and\nby $3.2\\%$ in AP$^{bbox}$ and $1.6\\%$ in AP$^{mask}$ on COCO, respectively.\nThese results are obtained with the AdPE being a pure MIM approach that does\nnot use any extra models or external datasets for pretraining. The code is\navailable at https://github.com/maple-research-lab/AdPE.\n","authors":["Xiao Wang","Ying Wang","Ziwei Xuan","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2303.07598v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.07596v1","updated":"2023-03-14T02:37:11Z","published":"2023-03-14T02:37:11Z","title":"Frequency-Modulated Point Cloud Rendering with Easy Editing","summary":"  We develop an effective point cloud rendering pipeline for novel view\nsynthesis, which enables high fidelity local detail reconstruction, real-time\nrendering and user-friendly editing. In the heart of our pipeline is an\nadaptive frequency modulation module called Adaptive Frequency Net (AFNet),\nwhich utilizes a hypernetwork to learn the local texture frequency encoding\nthat is consecutively injected into adaptive frequency activation layers to\nmodulate the implicit radiance signal. This mechanism improves the frequency\nexpressive ability of the network with richer frequency basis support, only at\na small computational budget. To further boost performance, a preprocessing\nmodule is also proposed for point cloud geometry optimization via point opacity\nestimation. In contrast to implicit rendering, our pipeline supports\nhigh-fidelity interactive editing based on point cloud manipulation. Extensive\nexperimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples\ndatasets demonstrate the superior performances achieved by our method in terms\nof PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.\n","authors":["Yi Zhang","Xiaoyang Huang","Bingbing Ni","Teng Li","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07596v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07582v1","updated":"2023-03-14T02:02:39Z","published":"2023-03-14T02:02:39Z","title":"Calibrated Teacher for Sparsely Annotated Object Detection","summary":"  Fully supervised object detection requires training images in which all\ninstances are annotated. This is actually impractical due to the high labor and\ntime costs and the unavoidable missing annotations. As a result, the incomplete\nannotation in each image could provide misleading supervision and harm the\ntraining. Recent works on sparsely annotated object detection alleviate this\nproblem by generating pseudo labels for the missing annotations. Such a\nmechanism is sensitive to the threshold of the pseudo label score. However, the\neffective threshold is different in different training stages and among\ndifferent object detectors. Therefore, the current methods with fixed\nthresholds have sub-optimal performance, and are difficult to be applied to\nother detectors. In order to resolve this obstacle, we propose a Calibrated\nTeacher, of which the confidence estimation of the prediction is well\ncalibrated to match its real precision. In this way, different detectors in\ndifferent training stages would share a similar distribution of the output\nconfidence, so that multiple detectors could share the same fixed threshold and\nachieve better performance. Furthermore, we present a simple but effective\nFocal IoU Weight (FIoU) for the classification loss. FIoU aims at reducing the\nloss weight of false negative samples caused by the missing annotation, and\nthus works as the complement of the teacher-student paradigm. Extensive\nexperiments show that our methods set new state-of-the-art under all different\nsparse settings in COCO. Code will be available at\nhttps://github.com/Whileherham/CalibratedTeacher.\n","authors":["Haohan Wang","Liang Liu","Boshen Zhang","Jiangning Zhang","Wuhao Zhang","Zhenye Gan","Yabiao Wang","Chengjie Wang","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07580v1","updated":"2023-03-14T01:56:15Z","published":"2023-03-14T01:56:15Z","title":"Sensitive Region-based Metamorphic Testing Framework using Explainable\n  AI","summary":"  Deep Learning (DL) is one of the most popular research topics in machine\nlearning and DL-driven image recognition systems have developed rapidly. Recent\nresearch has used metamorphic testing (MT) to detect misclassified images. Most\nof them discuss metamorphic relations (MR), with little discussion on which\nregions should be transformed. We focus on the fact that there are sensitive\nregions where even a small transformation can easily change the prediction\nresults and propose an MT framework that efficiently tests for regions prone to\nmisclassification by transforming the sensitive regions. Our evaluation showed\nthat the sensitive regions can be specified by Explainable AI (XAI) and our\nframework effectively detects faults.\n","authors":["Yuma Torikoshi","Yasuharu Nishi","Juichi Takahashi"],"pdf_url":"https://arxiv.org/pdf/2303.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14402v3","updated":"2023-03-14T01:41:44Z","published":"2023-02-28T08:35:50Z","title":"Neural Video Compression with Diverse Contexts","summary":"  For any video codecs, the coding efficiency highly relies on whether the\ncurrent signal to be encoded can find the relevant contexts from the previous\nreconstructed signals. Traditional codec has verified more contexts bring\nsubstantial coding gain, but in a time-consuming manner. However, for the\nemerging neural video codec (NVC), its contexts are still limited, leading to\nlow compression ratio. To boost NVC, this paper proposes increasing the context\ndiversity in both temporal and spatial dimensions. First, we guide the model to\nlearn hierarchical quality patterns across frames, which enriches long-term and\nyet high-quality temporal contexts. Furthermore, to tap the potential of\noptical flow-based coding framework, we introduce a group-based offset\ndiversity where the cross-group interaction is proposed for better context\nmining. In addition, this paper also adopts a quadtree-based partition to\nincrease spatial context diversity when encoding the latent representation in\nparallel. Experiments show that our codec obtains 23.5% bitrate saving over\nprevious SOTA NVC. Better yet, our codec has surpassed the under-developing\nnext generation traditional codec/ECM in both RGB and YUV420 colorspaces, in\nterms of PSNR. The codes are at https://github.com/microsoft/DCVC.\n","authors":["Jiahao Li","Bin Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.14402v3.pdf","comment":"Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC"},{"id":"http://arxiv.org/abs/2211.14462v2","updated":"2023-03-14T01:36:36Z","published":"2022-11-26T02:53:40Z","title":"Meta Architecture for Point Cloud Analysis","summary":"  Recent advances in 3D point cloud analysis bring a diverse set of network\narchitectures to the field. However, the lack of a unified framework to\ninterpret those networks makes any systematic comparison, contrast, or analysis\nchallenging, and practically limits healthy development of the field. In this\npaper, we take the initiative to explore and propose a unified framework called\nPointMeta, to which the popular 3D point cloud analysis approaches could fit.\nThis brings three benefits. First, it allows us to compare different approaches\nin a fair manner, and use quick experiments to verify any empirical\nobservations or assumptions summarized from the comparison. Second, the big\npicture brought by PointMeta enables us to think across different components,\nand revisit common beliefs and key design decisions made by the popular\napproaches. Third, based on the learnings from the previous two analyses, by\ndoing simple tweaks on the existing approaches, we are able to derive a basic\nbuilding block, termed PointMetaBase. It shows very strong performance in\nefficiency and effectiveness through extensive experiments on challenging\nbenchmarks, and thus verifies the necessity and benefits of high-level\ninterpretation, contrast, and comparison like PointMeta. In particular,\nPointMetaBase surpasses the previous state-of-the-art method by 0.7%/1.4/%2.1%\nmIoU with only 2%/11%/13% of the computation cost on the S3DIS datasets.\n","authors":["Haojia Lin","Xiawu Zheng","Lijiang Li","Fei Chao","Shanshan Wang","Yan Wang","Yonghong Tian","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2211.14462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13361v4","updated":"2023-03-14T01:28:35Z","published":"2023-01-31T01:31:43Z","title":"Iterative Loop Method Combining Active and Semi-Supervised Learning for\n  Domain Adaptive Semantic Segmentation","summary":"  Semantic segmentation is an important technique for environment perception in\nintelligent transportation systems. With the rapid development of convolutional\nneural networks (CNNs), road scene analysis can usually achieve satisfactory\nresults in the source domain. However, guaranteeing good generalization to\ndifferent target domain scenarios remains a significant challenge. Recently,\nsemi-supervised learning and active learning have been proposed to alleviate\nthis problem. Semisupervised learning can improve model accuracy with massive\nunlabeled data, but some pseudo labels containing noise would be generated with\nlimited or imbalanced training data. And there will be suboptimal models if\nhuman guidance is absent. Active learning can select more effective data to\nintervene, while the model accuracy can not be improved because the massive\nunlabeled data are not used. And the probability of querying sub-optimal\nsamples will increase when the domain difference is too large, increasing\nannotation cost. This paper proposes an iterative loop method combining active\nand semisupervised learning for domain adaptive semantic segmentation. The\nmethod first uses semi-supervised to learn massive unlabeled data to improve\nmodel accuracy and provide more accurate selection models for active learning.\nSecondly, combined with the predictive uncertainty sample selection strategy of\nactive learning, manual intervention is used to correct the pseudo-labels.\nFinally, flexible iterative loops achieve the best performance with minimal\nlabeling cost. Extensive experiments show that our method establishes\nstate-of-the-art performance on tasks of GTAV to Cityscapes, SYNTHIA to\nCityscapes, improving by 4.9% mIoU and 5.2% mIoU, compared to the previous best\nmethod, respectively.\n","authors":["Licong Guan","Xue Yuan"],"pdf_url":"https://arxiv.org/pdf/2301.13361v4.pdf","comment":"10 pages,5 figures"},{"id":"http://arxiv.org/abs/2203.04446v3","updated":"2023-03-14T01:13:36Z","published":"2022-03-08T23:35:04Z","title":"Self-Supervised Domain Calibration and Uncertainty Estimation for Place\n  Recognition","summary":"  Visual place recognition techniques based on deep learning, which have\nimposed themselves as the state-of-the-art in recent years, do not generalize\nwell to environments visually different from the training set. Thus, to achieve\ntop performance, it is sometimes necessary to fine-tune the networks to the\ntarget environment. To this end, we propose a self-supervised domain\ncalibration procedure based on robust pose graph optimization from Simultaneous\nLocalization and Mapping (SLAM) as the supervision signal without requiring GPS\nor manual labeling. Moreover, we leverage the procedure to improve uncertainty\nestimation for place recognition matches which is important in safety critical\napplications. We show that our approach can improve the performance of a\nstate-of-the-art technique on a target environment dissimilar from its training\nset and that we can obtain uncertainty estimates. We believe that this approach\nwill help practitioners to deploy robust place recognition solutions in\nreal-world applications. Our code is available publicly:\nhttps://github.com/MISTLab/vpr-calibration-and-uncertainty\n","authors":["Pierre-Yves Lajoie","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2203.04446v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07564v1","updated":"2023-03-14T01:10:59Z","published":"2023-03-14T01:10:59Z","title":"Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow","summary":"  Optical flow has achieved great success under clean scenes, but suffers from\nrestricted performance under foggy scenes. To bridge the clean-to-foggy domain\ngap, the existing methods typically adopt the domain adaptation to transfer the\nmotion knowledge from clean to synthetic foggy domain. However, these methods\nunexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous\nwhen applied to real-world scenes. To handle the practical optical flow under\nreal foggy scenes, in this work, we propose a novel unsupervised cumulative\ndomain adaptation optical flow (UCDA-Flow) framework: depth-association motion\nadaptation and correlation-alignment motion adaptation. Specifically, we\ndiscover that depth is a key ingredient to influence the optical flow: the\ndeeper depth, the inferior optical flow, which motivates us to design a\ndepth-association motion adaptation module to bridge the clean-to-foggy domain\ngap. Moreover, we figure out that the cost volume correlation shares similar\ndistribution of the synthetic and real foggy images, which enlightens us to\ndevise a correlation-alignment motion adaptation module to distill motion\nknowledge of the synthetic foggy domain to the real foggy domain. Note that\nsynthetic fog is designed as the intermediate domain. Under this unified\nframework, the proposed cumulative adaptation progressively transfers knowledge\nfrom clean scenes to real foggy scenes. Extensive experiments have been\nperformed to verify the superiority of the proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Wending Yan","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2303.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07560v1","updated":"2023-03-14T00:57:11Z","published":"2023-03-14T00:57:11Z","title":"Machine Learning Computer Vision Applications for Spatial AI Object\n  Recognition in Orange County, California","summary":"  We provide an integrated and systematic automation approach to spatial object\nrecognition and positional detection using AI machine learning and computer\nvision algorithms for Orange County, California. We describe a comprehensive\nmethodology for multi-sensor, high-resolution field data acquisition, along\nwith post-field processing and pre-analysis processing tasks. We developed a\nseries of algorithmic formulations and workflows that integrate convolutional\ndeep neural network learning with detected object positioning estimation in\n360{\\deg} equirectancular photosphere imagery. We provide examples of\napplication processing more than 800 thousand cardinal directions in\nphotosphere images across two areas in Orange County, and present detection\nresults for stop-sign and fire hydrant object recognition. We discuss the\nefficiency and effectiveness of our approach, along with broader inferences\nrelated to the performance and implications of this approach for future\ntechnological innovations, including automation of spatial data and public\nasset inventories, and near real-time AI field data systems.\n","authors":["Kostas Alexandridis"],"pdf_url":"https://arxiv.org/pdf/2303.07560v1.pdf","comment":"24 pages, 15 figures, 8 tables"},{"id":"http://arxiv.org/abs/2302.07577v3","updated":"2023-03-14T00:48:26Z","published":"2023-02-15T10:40:19Z","title":"Efficient Teacher: Semi-Supervised Object Detection for YOLOv5","summary":"  Semi-Supervised Object Detection (SSOD) has been successful in improving the\nperformance of both R-CNN series and anchor-free detectors. However, one-stage\nanchor-based detectors lack the structure to generate high-quality or flexible\npseudo labels, leading to serious inconsistency problems in SSOD. In this\npaper, we propose the Efficient Teacher framework for scalable and effective\none-stage anchor-based SSOD training, consisting of Dense Detector, Pseudo\nLabel Assigner, and Epoch Adaptor. Dense Detector is a baseline model that\nextends RetinaNet with dense sampling techniques inspired by YOLOv5. The\nEfficient Teacher framework introduces a novel pseudo label assignment\nmechanism, named Pseudo Label Assigner, which makes more refined use of pseudo\nlabels from Dense Detector. Epoch Adaptor is a method that enables a stable and\nefficient end-to-end semi-supervised training schedule for Dense Detector. The\nPseudo Label Assigner prevents the occurrence of bias caused by a large number\nof low-quality pseudo labels that may interfere with the Dense Detector during\nthe student-teacher mutual learning mechanism, and the Epoch Adaptor utilizes\ndomain and distribution adaptation to allow Dense Detector to learn globally\ndistributed consistent features, making the training independent of the\nproportion of labeled data. Our experiments show that the Efficient Teacher\nframework achieves state-of-the-art results on VOC, COCO-standard, and\nCOCO-additional using fewer FLOPs than previous methods. To the best of our\nknowledge, this is the first attempt to apply Semi-Supervised Object Detection\nto YOLOv5.Code is available:\nhttps://github.com/AlibabaResearch/efficientteacher\n","authors":["Bowen Xu","Mingtao Chen","Wenlong Guan","Lulu Hu"],"pdf_url":"https://arxiv.org/pdf/2302.07577v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.07547v1","updated":"2023-03-14T00:30:24Z","published":"2023-03-14T00:30:24Z","title":"HazardNet: Road Debris Detection by Augmentation of Synthetic Models","summary":"  We present an algorithm to detect unseen road debris using a small set of\nsynthetic models. Early detection of road debris is critical for safe\nautonomous or assisted driving, yet the development of a robust road debris\ndetection model has not been widely discussed. There are two main challenges to\nbuilding a road debris detector: first, data collection of road debris is\nchallenging since hazardous objects on the road are rare to encounter in real\ndriving scenarios; second, the variability of road debris is broad, ranging\nfrom a very small brick to a large fallen tree. To overcome these challenges,\nwe propose a novel approach to few-shot learning of road debris that uses\nsemantic augmentation and domain randomization to augment real road images with\nsynthetic models. We constrain the problem domain to uncommon objects on the\nroad and allow the deep neural network, HazardNet, to learn the semantic\nmeaning of road debris to eventually detect unseen road debris. Our results\ndemonstrate that HazardNet is able to accurately detect real road debris when\nonly trained on synthetic objects in augmented images.\n","authors":["Tae Eun Choe","Jane Wu","Xiaolin Lin","Karen Kwon","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2303.07547v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2211.01324v5","updated":"2023-03-14T00:22:14Z","published":"2022-11-02T17:43:04Z","title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert\n  Denoisers","summary":"  Large-scale diffusion-based generative models have led to breakthroughs in\ntext-conditioned high-resolution image synthesis. Starting from random noise,\nsuch text-to-image diffusion models gradually synthesize images in an iterative\nfashion while conditioning on text prompts. We find that their synthesis\nbehavior qualitatively changes throughout this process: Early in sampling,\ngeneration strongly relies on the text prompt to generate text-aligned content,\nwhile later, the text conditioning is almost entirely ignored. This suggests\nthat sharing model parameters throughout the entire generation process may not\nbe ideal. Therefore, in contrast to existing works, we propose to train an\nensemble of text-to-image diffusion models specialized for different synthesis\nstages. To maintain training efficiency, we initially train a single model,\nwhich is then split into specialized models that are trained for the specific\nstages of the iterative generation process. Our ensemble of diffusion models,\ncalled eDiff-I, results in improved text alignment while maintaining the same\ninference computation cost and preserving high visual quality, outperforming\nprevious large-scale text-to-image diffusion models on the standard benchmark.\nIn addition, we train our model to exploit a variety of embeddings for\nconditioning, including the T5 text, CLIP text, and CLIP image embeddings. We\nshow that these different embeddings lead to different behaviors. Notably, the\nCLIP image embedding allows an intuitive way of transferring the style of a\nreference image to the target text-to-image output. Lastly, we show a technique\nthat enables eDiff-I's \"paint-with-words\" capability. A user can select the\nword in the input text and paint it in a canvas to control the output, which is\nvery handy for crafting the desired image in mind. The project page is\navailable at https://deepimagination.cc/eDiff-I/\n","authors":["Yogesh Balaji","Seungjun Nah","Xun Huang","Arash Vahdat","Jiaming Song","Qinsheng Zhang","Karsten Kreis","Miika Aittala","Timo Aila","Samuli Laine","Bryan Catanzaro","Tero Karras","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2211.01324v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07545v1","updated":"2023-03-14T00:19:11Z","published":"2023-03-14T00:19:11Z","title":"Implicit and Explicit Commonsense for Multi-sentence Video Captioning","summary":"  Existing dense or paragraph video captioning approaches rely on holistic\nrepresentations of videos, possibly coupled with learned object/action\nrepresentations, to condition hierarchical language decoders. However, they\nfundamentally lack the commonsense knowledge of the world required to reason\nabout progression of events, causality, and even function of certain objects\nwithin a scene. To address this limitation we propose a novel video captioning\nTransformer-based model, that takes into account both implicit (visuo-lingual\nand purely linguistic) and explicit (knowledge-base) commonsense knowledge. We\nshow that these forms of knowledge, in isolation and in combination, enhance\nthe quality of produced captions. Further, inspired by imitation learning, we\npropose a new task of instruction generation, where the goal is to produce a\nset of linguistic instructions from a video demonstration of its performance.\nWe formalize the task using ALFRED dataset [52] generated using an AI2-THOR\nenvironment. While instruction generation is conceptually similar to paragraph\ncaptioning, it differs in the fact that it exhibits stronger object\npersistence, as well as spatially-aware and causal sentence structure. We show\nthat our commonsense knowledge enhanced approach produces significant\nimprovements on this task (up to 57% in METEOR and 8.5% in CIDEr), as well as\nthe state-of-the-art result on more traditional video captioning in the\nActivityNet Captions dataset [29].\n","authors":["Shih-Han Chou","James J. Little","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2303.07545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07543v1","updated":"2023-03-14T00:13:57Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear\n  Discriminative Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminative Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07540v1","updated":"2023-03-14T00:05:08Z","published":"2023-03-14T00:05:08Z","title":"Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial\n  Wedge Pressure from Cardiac MRI","summary":"  Heart failure is a serious and life-threatening condition that can lead to\nelevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure\n(PAWP) is an important surrogate marker indicating high pressure in the left\nventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an\ninvasive procedure. A non-invasive method is useful in quickly identifying\nhigh-risk patients from a large population. In this work, we develop a tensor\nlearning-based pipeline for identifying PAWP from multimodal cardiac Magnetic\nResonance Imaging (MRI). This pipeline extracts spatial and temporal features\nfrom high-dimensional scans. For quality control, we incorporate an epistemic\nuncertainty-based binning strategy to identify poor-quality training samples.\nTo improve the performance, we learn complementary information by integrating\nfeatures from multimodal data: cardiac MRI with short-axis and four-chamber\nviews, and Electronic Health Records. The experimental analysis on a large\ncohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation\nindicates that the proposed pipeline has a diagnostic value and can produce\npromising performance with significant improvement over the baseline in\nclinical practice (i.e., $\\Delta$AUC $=0.10$, $\\Delta$Accuracy $=0.06$, and\n$\\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical\nutility of our method.\n","authors":["Prasun C. Tripathi","Mohammod N. I. Suvon","Lawrence Schobs","Shuo Zhou","Samer Alabed","Andrew J. Swift","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2303.07540v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.08030v1","updated":"2023-03-14T16:10:55Z","published":"2023-03-14T16:10:55Z","title":"Improving information retrieval through correspondence analysis instead\n  of latent semantic analysis","summary":"  Both latent semantic analysis (LSA) and correspondence analysis (CA) are\ndimensionality reduction techniques that use singular value decomposition (SVD)\nfor information retrieval. Theoretically, the results of LSA display both the\nassociation between documents and terms, and marginal effects; in comparison,\nCA only focuses on the associations between documents and terms. Marginal\neffects are usually not relevant for information retrieval, and therefore, from\na theoretical perspective CA is more suitable for information retrieval.\n  In this paper, we empirically compare LSA and CA. The elements of the raw\ndocument-term matrix are weighted, and the weighting exponent of singular\nvalues is adjusted to improve the performance of LSA. We explore whether these\ntwo weightings also improve the performance of CA. In addition, we compare the\noptimal singular value weighting exponents for LSA and CA to identify what the\ninitial dimensions in LSA correspond to.\n  The results for four empirical datasets show that CA always performs better\nthan LSA. Weighting the elements of the raw data matrix can improve CA;\nhowever, it is data dependent and the improvement is small. Adjusting the\nsingular value weighting exponent usually improves the performance of CA;\nhowever, the extent of the improved performance depends on the dataset and\nnumber of dimensions. In general, CA needs a larger singular value weighting\nexponent than LSA to obtain the optimal performance. This indicates that CA\nemphasizes initial dimensions more than LSA, and thus, margins play an\nimportant role in the initial dimensions in LSA.\n","authors":["Qianqian Qi","David J. Hessen","Peter G. M. van der Heijden"],"pdf_url":"https://arxiv.org/pdf/2303.08030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07810v1","updated":"2023-03-14T11:28:53Z","published":"2023-03-14T11:28:53Z","title":"Disentangled Graph Social Recommendation","summary":"  Social recommender systems have drawn a lot of attention in many online web\nservices, because of the incorporation of social information between users in\nimproving recommendation results. Despite the significant progress made by\nexisting solutions, we argue that current methods fall short in two\nlimitations: (1) Existing social-aware recommendation models only consider\ncollaborative similarity between items, how to incorporate item-wise semantic\nrelatedness is less explored in current recommendation paradigms. (2) Current\nsocial recommender systems neglect the entanglement of the latent factors over\nheterogeneous relations (e.g., social connections, user-item interactions).\nLearning the disentangled representations with relation heterogeneity poses\ngreat challenge for social recommendation. In this work, we design a\nDisentangled Graph Neural Network (DGNN) with the integration of latent memory\nunits, which empowers DGNN to maintain factorized representations for\nheterogeneous types of user and item connections. Additionally, we devise new\nmemory-augmented message propagation and aggregation schemes under the graph\nneural architecture, allowing us to recursively distill semantic relatedness\ninto the representations of users and items in a fully automatic manner.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nour model by achieving great improvement over state-of-the-art recommendation\ntechniques. The source code is publicly available at:\nhttps://github.com/HKUDS/DGNN.\n","authors":["Lianghao Xia","Yizhen Shao","Chao Huang","Yong Xu","Huance Xu","Jian Pei"],"pdf_url":"https://arxiv.org/pdf/2303.07810v1.pdf","comment":"Accepted by IEEE ICDE 2023"},{"id":"http://arxiv.org/abs/2303.07797v1","updated":"2023-03-14T11:12:22Z","published":"2023-03-14T11:12:22Z","title":"Automated Self-Supervised Learning for Recommendation","summary":"  Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm\nfor collaborative filtering (CF). To improve the representation quality over\nlimited labeled data, contrastive learning has attracted attention in\nrecommendation and benefited graph-based CF model recently. However, the\nsuccess of most contrastive methods heavily relies on manually generating\neffective contrastive views for heuristic-based data augmentation. This does\nnot generalize across different datasets and downstream recommendation tasks,\nwhich is difficult to be adaptive for data augmentation and robust to noise\nperturbation. To fill this crucial gap, this work proposes a unified Automated\nCollaborative Filtering (AutoCF) to automatically perform data augmentation for\nrecommendation. Specifically, we focus on the generative self-supervised\nlearning framework with a learnable augmentation paradigm that benefits the\nautomated distillation of important self-supervised signals. To enhance the\nrepresentation discrimination ability, our masked graph autoencoder is designed\nto aggregate global information during the augmentation via reconstructing the\nmasked subgraph structures. Experiments and ablation studies are performed on\nseveral public datasets for recommending products, venues, and locations.\nResults demonstrate the superiority of AutoCF against various baseline methods.\nWe release the model implementation at https://github.com/HKUDS/AutoCF.\n","authors":["Lianghao Xia","Chao Huang","Chunzhen Huang","Kangyi Lin","Tao Yu","Ben Kao"],"pdf_url":"https://arxiv.org/pdf/2303.07797v1.pdf","comment":"Accepted by ACM The Web Conference, 2023"},{"id":"http://arxiv.org/abs/2301.00503v3","updated":"2023-03-14T11:01:26Z","published":"2023-01-02T02:10:18Z","title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay","summary":"  This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n","authors":["Yacheng He","Qianghuai Jia","Lin Yuan","Ruopeng Li","Yixin Ou","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00503v3.pdf","comment":"Accepted by WWW 2023 poster"},{"id":"http://arxiv.org/abs/2202.02113v7","updated":"2023-03-14T10:33:15Z","published":"2022-02-04T12:52:32Z","title":"From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer","summary":"  Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n","authors":["Xin Xie","Ningyu Zhang","Zhoubo Li","Shumin Deng","Hui Chen","Feiyu Xiong","Mosha Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2202.02113v7.pdf","comment":"Accepted by WWW 2022 Poster"},{"id":"http://arxiv.org/abs/2205.10852v5","updated":"2023-03-14T10:28:49Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Ningyu Zhang","Qiang Chen","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2205.10852v5.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.07678v1","updated":"2023-03-14T07:27:30Z","published":"2023-03-14T07:27:30Z","title":"Query2doc: Query Expansion with Large Language Models","summary":"  This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n","authors":["Liang Wang","Nan Yang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2303.07678v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2303.07607v1","updated":"2023-03-14T03:03:19Z","published":"2023-03-14T03:03:19Z","title":"CoMeta: Enhancing Meta Embeddings with Collaborative Information in\n  Cold-start Problem of Recommendation","summary":"  The cold-start problem is quite challenging for existing recommendation\nmodels. Specifically, for the new items with only a few interactions, their ID\nembeddings are trained inadequately, leading to poor recommendation\nperformance. Some recent studies introduce meta learning to solve the\ncold-start problem by generating meta embeddings for new items as their initial\nID embeddings. However, we argue that the capability of these methods is\nlimited, because they mainly utilize item attribute features which only contain\nlittle information, but ignore the useful collaborative information contained\nin the ID embeddings of users and old items. To tackle this issue, we propose\nCoMeta to enhance the meta embeddings with the collaborative information.\nCoMeta consists of two submodules: B-EG and S-EG. Specifically, for a new item:\nB-EG calculates the similarity-based weighted sum of the ID embeddings of old\nitems as its base embedding; S-EG generates its shift embedding not only with\nits attribute features but also with the average ID embedding of the users who\ninteracted with it. The final meta embedding is obtained by adding up the base\nembedding and the shift embedding. We conduct extensive experiments on two\npublic datasets. The experimental results demonstrate both the effectiveness\nand the compatibility of CoMeta.\n","authors":["Haonan Hu","Dazhong Rong","Jianhai Chen","Qinming He","Zhenguang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06095v2","updated":"2023-03-14T02:34:12Z","published":"2023-03-10T17:24:41Z","title":"HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical\n  Information Extraction","summary":"  Multi-scenario & multi-task learning has been widely applied to many\nrecommendation systems in industrial applications, wherein an effective and\npractical approach is to carry out multi-scenario transfer learning on the\nbasis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based\nmethod, which aims to project all information in the same feature space, cannot\neffectively deal with the complex relationships inherent among various\nscenarios and tasks, resulting in unsatisfactory performance. To tackle the\nproblem, we propose a Hierarchical information extraction Network (HiNet) for\nmulti-scenario and multi-task recommendation, which achieves hierarchical\nextraction based on coarse-to-fine knowledge transfer scheme. The multiple\nextraction layers of the hierarchical network enable the model to enhance the\ncapability of transferring valuable information across scenarios while\npreserving specific features of scenarios and tasks. Furthermore, a novel\nscenario-aware attentive network module is proposed to model correlations\nbetween scenarios explicitly. Comprehensive experiments conducted on real-world\nindustrial datasets from Meituan Meishi platform demonstrate that HiNet\nachieves a new state-of-the-art performance and significantly outperforms\nexisting solutions. HiNet is currently fully deployed in two scenarios and has\nachieved 2.87% and 1.75% order quantity gain respectively.\n","authors":["Jie Zhou","Xianshuai Cao","Wenhao Li","Lin Bo","Kun Zhang","Chuan Luo","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2303.06095v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.08133v1","updated":"2023-03-14T17:59:01Z","published":"2023-03-14T17:59:01Z","title":"MeshDiffusion: Score-based Generative 3D Mesh Modeling","summary":"  We consider the task of generating realistic 3D shapes, which is useful for a\nvariety of applications such as automatic scene generation and physical\nsimulation. Compared to other 3D representations like voxels and point clouds,\nmeshes are more desirable in practice, because (1) they enable easy and\narbitrary manipulation of shapes for relighting and simulation, and (2) they\ncan fully leverage the power of modern graphics pipelines which are mostly\noptimized for meshes. Previous scalable methods for generating meshes typically\nrely on sub-optimal post-processing, and they tend to produce overly-smooth or\nnoisy surfaces without fine-grained geometric details. To overcome these\nshortcomings, we take advantage of the graph structure of meshes and use a\nsimple yet very effective generative modeling method to generate 3D meshes.\nSpecifically, we represent meshes with deformable tetrahedral grids, and then\ntrain a diffusion model on this direct parametrization. We demonstrate the\neffectiveness of our model on multiple generative tasks.\n","authors":["Zhen Liu","Yao Feng","Michael J. Black","Derek Nowrouzezahrai","Liam Paull","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08133v1.pdf","comment":"Published in ICLR 2023 (Spotlight, Notable-top-25%)"},{"id":"http://arxiv.org/abs/2303.03340v2","updated":"2023-03-14T17:57:32Z","published":"2023-03-06T18:13:14Z","title":"Symbolic Synthesis of Neural Networks","summary":"  Neural networks adapt very well to distributed and continuous\nrepresentations, but struggle to generalize from small amounts of data.\nSymbolic systems commonly achieve data efficient generalization by exploiting\nmodularity to benefit from local and discrete features of a representation.\nThese features allow symbolic programs to be improved one module at a time and\nto experience combinatorial growth in the values they can successfully process.\nHowever, it is difficult to design a component that can be used to form\nsymbolic abstractions and which is adequately overparametrized to learn\narbitrary high-dimensional transformations. I present Graph-based Symbolically\nSynthesized Neural Networks (G-SSNNs), a class of neural modules that operate\non representations modified with synthesized symbolic programs to include a\nfixed set of local and discrete features. I demonstrate that the choice of\ninjected features within a G-SSNN module modulates the data efficiency and\ngeneralization of baseline neural models, creating predictable patterns of both\nheightened and curtailed generalization. By training G-SSNNs, we also derive\ninformation about desirable semantics of symbolic programs without manual\nengineering. This information is compact and amenable to abstraction, but can\nalso be flexibly recontextualized for other high-dimensional settings. In\nfuture work, I will investigate data efficient generalization and the\ntransferability of learned symbolic representations in more complex G-SSNN\ndesigns based on more complex classes of symbolic programs. Experimental code\nand data are available at\nhttps://github.com/shlomenu/symbolically_synthesized_networks .\n","authors":["Eli Whitehouse"],"pdf_url":"https://arxiv.org/pdf/2303.03340v2.pdf","comment":"8 pages, 1 figure. Minor formula correction and minor textual\n  revision"},{"id":"http://arxiv.org/abs/2303.08127v1","updated":"2023-03-14T17:57:06Z","published":"2023-03-14T17:57:06Z","title":"CB2: Collaborative Natural Language Interaction Research Platform","summary":"  CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n","authors":["Jacob Sharf","Mustafa Omer Gul","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2303.08127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.08871v4","updated":"2023-03-14T17:50:47Z","published":"2021-10-17T17:15:13Z","title":"Expectation Distance-based Distributional Clustering for\n  Noise-Robustness","summary":"  This paper presents a clustering technique that reduces the susceptibility to\ndata noise by learning and clustering the data-distribution and then assigning\nthe data to the cluster of its distribution. In the process, it reduces the\nimpact of noise on clustering results. This method involves introducing a new\ndistance among distributions, namely the expectation distance (denoted, ED),\nthat goes beyond the state-of-art distribution distance of optimal mass\ntransport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends\nonly on the marginal distributions while the former also employs the\ninformation about the joint distributions. Using the ED, the paper extends the\nclassical $K$-means and $K$-medoids clustering to those over data-distributions\n(rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also\npresents the closed-form expressions of the $W_2$ and ED distance measures. The\nimplementation results of the proposed ED and the $W_2$ distance measures to\ncluster real-world weather data as well as stock data are also presented, which\ninvolves efficiently extracting and using the underlying data distributions --\nGaussians for weather data versus lognormals for stock data. The results show\nstriking performance improvement over classical clustering of raw-data, with\nhigher accuracy realized for ED. Also, not only does the distribution-based\nclustering offer higher accuracy, but it also lowers the computation time due\nto reduced time-complexity.\n","authors":["Rahmat Adesunkanmi","Ratnesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2110.08871v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08117v1","updated":"2023-03-14T17:49:50Z","published":"2023-03-14T17:49:50Z","title":"Do Transformers Parse while Predicting the Masked Word?","summary":"  Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.\n","authors":["Haoyu Zhao","Abhishek Panigrahi","Rong Ge","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2303.08117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.01680v3","updated":"2023-03-14T17:47:50Z","published":"2022-01-05T16:19:16Z","title":"Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems","summary":"  TWe establish regret lower bounds for adaptively controlling an unknown\nlinear Gaussian system with quadratic costs. We combine ideas from experiment\ndesign, estimation theory and a perturbation bound of certain information\nmatrices to derive regret lower bounds exhibiting scaling on the order of\nmagnitude $\\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the\nrole of control-theoretic parameters and we are able to show that systems that\nare hard to control are also hard to learn to control; when instantiated to\nstate feedback systems we recover the dimensional dependency of earlier work\nbut with improved scaling with system-theoretic constants such as system costs\nand Gramians. Furthermore, we extend our results to a class of partially\nobserved systems and demonstrate that systems with poor observability structure\nalso are hard to learn to control.\n","authors":["Ingvar Ziemann","Henrik Sandberg"],"pdf_url":"https://arxiv.org/pdf/2201.01680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08114v1","updated":"2023-03-14T17:47:25Z","published":"2023-03-14T17:47:25Z","title":"Simfluence: Modeling the Influence of Individual Training Examples by\n  Simulating Training Runs","summary":"  Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.\n","authors":["Kelvin Guu","Albert Webson","Ellie Pavlick","Lucas Dixon","Ian Tenney","Tolga Bolukbasi"],"pdf_url":"https://arxiv.org/pdf/2303.08114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08112v1","updated":"2023-03-14T17:47:09Z","published":"2023-03-14T17:47:09Z","title":"Eliciting Latent Predictions from Transformers with the Tuned Lens","summary":"  We analyze transformers from the perspective of iterative inference, seeking\nto understand how model predictions are refined layer by layer. To do so, we\ntrain an affine probe for each block in a frozen pretrained model, making it\npossible to decode every hidden state into a distribution over the vocabulary.\nOur method, the \\emph{tuned lens}, is a refinement of the earlier ``logit\nlens'' technique, which yielded useful insights but is often brittle.\n  We test our method on various autoregressive language models with up to 20B\nparameters, showing it to be more predictive, reliable and unbiased than the\nlogit lens. With causal experiments, we show the tuned lens uses similar\nfeatures to the model itself. We also find the trajectory of latent predictions\ncan be used to detect malicious inputs with high accuracy. All code needed to\nreproduce our results can be found at\nhttps://github.com/AlignmentResearch/tuned-lens.\n","authors":["Nora Belrose","Zach Furman","Logan Smith","Danny Halawi","Igor Ostrovsky","Lev McKinney","Stella Biderman","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2303.08112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08109v1","updated":"2023-03-14T17:44:23Z","published":"2023-03-14T17:44:23Z","title":"Vision-based route following by an embodied insect-inspired sparse\n  neural network","summary":"  We compared the efficiency of the FlyHash model, an insect-inspired sparse\nneural network (Dasgupta et al., 2017), to similar but non-sparse models in an\nembodied navigation task. This requires a model to control steering by\ncomparing current visual inputs to memories stored along a training route. We\nconcluded the FlyHash model is more efficient than others, especially in terms\nof data encoding.\n","authors":["Lu Yihe","Rana Alkhoury Maroun","Barbara Webb"],"pdf_url":"https://arxiv.org/pdf/2303.08109v1.pdf","comment":"8 pages, 4 figures; work-in-progress submission, accepted as a poster\n  at ICLR 2023 Workshop on Sparsity in Neural Networks; non-archival"},{"id":"http://arxiv.org/abs/2303.08102v1","updated":"2023-03-14T17:41:31Z","published":"2023-03-14T17:41:31Z","title":"Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice","summary":"  We investigate the problem of bandits with expert advice when the experts are\nfixed and known distributions over the actions. Improving on previous analyses,\nwe show that the regret in this setting is controlled by information-theoretic\nquantities that measure the similarity between experts. In some natural special\ncases, this allows us to obtain the first regret bound for EXP4 that can get\narbitrarily close to zero if the experts are similar enough. While for a\ndifferent algorithm, we provide another bound that describes the similarity\nbetween the experts in terms of the KL-divergence, and we show that this bound\ncan be smaller than the one of EXP4 in some cases. Additionally, we provide\nlower bounds for certain classes of experts showing that the algorithms we\nanalyzed are nearly optimal in some cases.\n","authors":["Khaled Eldowa","Nicolò Cesa-Bianchi","Alberto Maria Metelli","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2303.08102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13059v3","updated":"2023-03-14T17:36:44Z","published":"2022-02-26T04:49:01Z","title":"Variational Inference with Gaussian Mixture by Entropy Approximation","summary":"  Variational inference is a technique for approximating intractable posterior\ndistributions in order to quantify the uncertainty of machine learning.\nAlthough the unimodal Gaussian distribution is usually chosen as a parametric\ndistribution, it hardly approximates the multimodality. In this paper, we\nemploy the Gaussian mixture distribution as a parametric distribution. A main\ndifficulty of variational inference with the Gaussian mixture is how to\napproximate the entropy of the Gaussian mixture. We approximate the entropy of\nthe Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which\ncan be analytically calculated. In addition, we theoretically analyze the\napproximation error between the true entropy and approximated one in order to\nreveal when our approximation works well. Specifically, the approximation error\nis controlled by the ratios of the distances between the means to the sum of\nthe variances of the Gaussian mixture. Furthermore, it converges to zero when\nthe ratios go to infinity. This situation seems to be more likely to occur in\nhigher dimensional parametric spaces because of the curse of dimensionality.\nTherefore, our result guarantees that our approximation works well, for\nexample, in neural networks that assume a large number of weights.\n","authors":["Takashi Furuya","Hiroyuki Kusumoto","Koichi Taniguchi","Naoya Kanno","Kazuma Suetake"],"pdf_url":"https://arxiv.org/pdf/2202.13059v3.pdf","comment":"35 pages, 3 figures"},{"id":"http://arxiv.org/abs/2302.06025v2","updated":"2023-03-14T17:23:52Z","published":"2023-02-12T23:20:41Z","title":"Statistical Complexity and Optimal Algorithms for Non-linear Ridge\n  Bandits","summary":"  We consider the sequential decision-making problem where the mean outcome is\na non-linear function of the chosen action. Compared with the linear model, two\ncurious phenomena arise in non-linear models: first, in addition to the\n\"learning phase\" with a standard parametric rate for estimation or regret,\nthere is an \"burn-in period\" with a fixed cost determined by the non-linear\nfunction; second, achieving the smallest burn-in cost requires new exploration\nalgorithms. For a special family of non-linear functions named ridge functions\nin the literature, we derive upper and lower bounds on the optimal burn-in\ncost, and in addition, on the entire learning trajectory during the burn-in\nperiod via differential equations. In particular, a two-stage algorithm that\nfirst finds a good initial action and then treats the problem as locally linear\nis statistically optimal. In contrast, several classical algorithms, such as\nUCB and algorithms relying on regression oracles, are provably suboptimal.\n","authors":["Nived Rajaraman","Yanjun Han","Jiantao Jiao","Kannan Ramchandran"],"pdf_url":"https://arxiv.org/pdf/2302.06025v2.pdf","comment":"Title change; add a new lower bound for linear bandits in Theorem 13"},{"id":"http://arxiv.org/abs/2212.00881v2","updated":"2023-03-14T17:22:41Z","published":"2022-12-01T21:39:48Z","title":"Investigating Deep Learning Model Calibration for Classification\n  Problems in Mechanics","summary":"  Recently, there has been a growing interest in applying machine learning\nmethods to problems in engineering mechanics. In particular, there has been\nsignificant interest in applying deep learning techniques to predicting the\nmechanical behavior of heterogeneous materials and structures. Researchers have\nshown that deep learning methods are able to effectively predict mechanical\nbehavior with low error for systems ranging from engineered composites, to\ngeometrically complex metamaterials, to heterogeneous biological tissue.\nHowever, there has been comparatively little attention paid to deep learning\nmodel calibration, i.e., the match between predicted probabilities of outcomes\nand the true probabilities of outcomes. In this work, we perform a\ncomprehensive investigation into ML model calibration across seven open access\nengineering mechanics datasets that cover three distinct types of mechanical\nproblems. Specifically, we evaluate both model and model calibration error for\nmultiple machine learning methods, and investigate the influence of ensemble\naveraging and post hoc model calibration via temperature scaling. Overall, we\nfind that ensemble averaging of deep neural networks is both an effective and\nconsistent tool for improving model calibration, while temperature scaling has\ncomparatively limited benefits. Looking forward, we anticipate that this\ninvestigation will lay the foundation for future work in developing mechanics\nspecific approaches to deep learning model calibration.\n","authors":["Saeed Mohammadzadeh","Peerasait Prachaseree","Emma Lejeune"],"pdf_url":"https://arxiv.org/pdf/2212.00881v2.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.08081v1","updated":"2023-03-14T17:13:01Z","published":"2023-03-14T17:13:01Z","title":"Explanation Shift: Investigating Interactions between Models and\n  Shifting Data Distributions","summary":"  As input data distributions evolve, the predictive performance of machine\nlearning models tends to deteriorate. In practice, new input data tend to come\nwithout target labels. Then, state-of-the-art techniques model input data\ndistributions or model prediction distributions and try to understand issues\nregarding the interactions between learned models and shifting distributions.\nWe suggest a novel approach that models how explanation characteristics shift\nwhen affected by distribution shifts. We find that the modeling of explanation\nshifts can be a better indicator for detecting out-of-distribution model\nbehaviour than state-of-the-art techniques. We analyze different types of\ndistribution shifts using synthetic examples and real-world data sets. We\nprovide an algorithmic method that allows us to inspect the interaction between\ndata set features and learned models and compare them to the state-of-the-art.\nWe release our methods in an open-source Python package, as well as the code\nused to reproduce our experiments.\n","authors":["Carlos Mougan","Klaus Broelemann","David Masip","Gjergji Kasneci","Thanassis Thiropanis","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2303.08081v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2210.12369"},{"id":"http://arxiv.org/abs/2302.08893v2","updated":"2023-03-14T17:09:47Z","published":"2023-02-17T14:24:13Z","title":"A survey on online active learning","summary":"  Online active learning is a paradigm in machine learning that aims to select\nthe most informative data points to label from a data stream. The problem of\nminimizing the cost associated with collecting labeled observations has gained\na lot of attention in recent years, particularly in real-world applications\nwhere data is only available in an unlabeled form. Annotating each observation\ncan be time-consuming and costly, making it difficult to obtain large amounts\nof labeled data. To overcome this issue, many active learning strategies have\nbeen proposed in the last decades, aiming to select the most informative\nobservations for labeling in order to improve the performance of machine\nlearning models. These approaches can be broadly divided into two categories:\nstatic pool-based and stream-based active learning. Pool-based active learning\ninvolves selecting a subset of observations from a closed pool of unlabeled\ndata, and it has been the focus of many surveys and literature reviews.\nHowever, the growing availability of data streams has led to an increase in the\nnumber of approaches that focus on online active learning, which involves\ncontinuously selecting and labeling observations as they arrive in a stream.\nThis work aims to provide an overview of the most recently proposed approaches\nfor selecting the most informative observations from data streams in the\ncontext of online active learning. We review the various techniques that have\nbeen proposed and discuss their strengths and limitations, as well as the\nchallenges and opportunities that exist in this area of research. Our review\naims to provide a comprehensive and up-to-date overview of the field and to\nhighlight directions for future work.\n","authors":["Davide Cacciarelli","Murat Kulahci"],"pdf_url":"https://arxiv.org/pdf/2302.08893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03052v2","updated":"2023-03-14T17:04:49Z","published":"2023-03-06T11:51:28Z","title":"Masked Images Are Counterfactual Samples for Robust Fine-tuning","summary":"  Deep learning models are challenged by the distribution shift between the\ntraining data and test data. Recently, the large models pre-trained on diverse\ndata demonstrate unprecedented robustness to various distribution shifts.\nHowever, fine-tuning on these models can lead to a trade-off between\nin-distribution (ID) performance and out-of-distribution (OOD) robustness.\nExisting methods for tackling this trade-off do not explicitly address the OOD\nrobustness problem. In this paper, based on causal analysis on the\naforementioned problems, we propose a novel fine-tuning method, which use\nmasked images as counterfactual samples that help improving the robustness of\nthe fine-tuning model. Specifically, we mask either the semantics-related or\nsemantics-unrelated patches of the images based on class activation map to\nbreak the spurious correlation, and refill the masked patches with patches from\nother images. The resulting counterfactual samples are used in feature-based\ndistillation with the pre-trained model. Extensive experiments verify that\nregularizing the fine-tuning with the proposed masked images can achieve a\nbetter trade-off between ID and OOD performance, surpassing previous methods on\nthe OOD performance. Our code will be publicly available.\n","authors":["Yao Xiao","Ziyi Tang","Pengxu Wei","Cong Liu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.03052v2.pdf","comment":"Accepted by CVPR 2023 (v2: improve the clarity)"},{"id":"http://arxiv.org/abs/2210.10272v2","updated":"2023-03-14T17:02:34Z","published":"2022-10-19T03:29:58Z","title":"Training set cleansing of backdoor poisoning by self-supervised\n  representation learning","summary":"  A backdoor or Trojan attack is an important type of data poisoning attack\nagainst deep neural network (DNN) classifiers, wherein the training dataset is\npoisoned with a small number of samples that each possess the backdoor pattern\n(usually a pattern that is either imperceptible or innocuous) and which are\nmislabeled to the attacker's target class. When trained on a backdoor-poisoned\ndataset, a DNN behaves normally on most benign test samples but makes incorrect\npredictions to the target class when the test sample has the backdoor pattern\nincorporated (i.e., contains a backdoor trigger). Here we focus on image\nclassification tasks and show that supervised training may build stronger\nassociation between the backdoor pattern and the associated target class than\nthat between normal features and the true class of origin. By contrast,\nself-supervised representation learning ignores the labels of samples and\nlearns a feature embedding based on images' semantic content. %We thus propose\nto use unsupervised representation learning to avoid emphasising\nbackdoor-poisoned training samples and learn a similar feature embedding for\nsamples of the same class. Using a feature embedding found by self-supervised\nrepresentation learning, a data cleansing method, which combines sample\nfiltering and re-labeling, is developed. Experiments on CIFAR-10 benchmark\ndatasets show that our method achieves state-of-the-art performance in\nmitigating backdoor attacks.\n","authors":["H. Wang","S. Karami","O. Dia","H. Ritter","E. Emamjomeh-Zadeh","J. Chen","Z. Xiang","D. J. Miller","G. Kesidis"],"pdf_url":"https://arxiv.org/pdf/2210.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04115v2","updated":"2023-03-14T16:57:30Z","published":"2023-03-07T18:28:39Z","title":"Predicted Embedding Power Regression for Large-Scale Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) inputs can compromise the performance and safety of\nreal world machine learning systems. While many methods exist for OOD detection\nand work well on small scale datasets with lower resolution and few classes,\nfew methods have been developed for large-scale OOD detection. Existing\nlarge-scale methods generally depend on maximum classification probability,\nsuch as the state-of-the-art grouped softmax method. In this work, we develop a\nnovel approach that calculates the probability of the predicted class label\nbased on label distributions learned during the training process. Our method\nperforms better than current state-of-the-art methods with only a negligible\nincrease in compute cost. We evaluate our method against contemporary methods\nacross $14$ datasets and achieve a statistically significant improvement with\nrespect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).\n","authors":["Hong Yang","William Gebhardt","Alexander G. Ororbia","Travis Desell"],"pdf_url":"https://arxiv.org/pdf/2303.04115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.11741v2","updated":"2023-03-14T16:57:14Z","published":"2022-09-21T21:17:56Z","title":"Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking\n  Neural Networks with Learnable Neuronal Dynamics","summary":"  Event-based cameras have recently shown great potential for high-speed motion\nestimation owing to their ability to capture temporally rich information\nasynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired\nevent-driven processing can efficiently handle such asynchronous data, while\nneuron models such as the leaky-integrate and fire (LIF) can keep track of the\nquintessential timing information contained in the inputs. SNNs achieve this by\nmaintaining a dynamic state in the neuron memory, retaining important\ninformation while forgetting redundant data over time. Thus, we posit that SNNs\nwould allow for better performance on sequential regression tasks compared to\nsimilarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult\nto train due to vanishing spikes at later layers. To that effect, we propose an\nadaptive fully-spiking framework with learnable neuronal dynamics to alleviate\nthe spike vanishing problem. We utilize surrogate gradient-based\nbackpropagation through time (BPTT) to train our deep SNNs from scratch. We\nvalidate our approach for the task of optical flow estimation on the\nMulti-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset.\nOur experiments on these datasets show an average reduction of 13% in average\nendpoint error (AEE) compared to state-of-the-art ANNs. We also explore several\ndown-scaled models and observe that our SNN models consistently outperform\nsimilarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the\nimportance of SNNs for smaller models and their suitability at the edge. In\nterms of efficiency, our SNNs offer substantial savings in network parameters\n(48.3x) and computational energy (10.2x) while attaining ~10% lower EPE\ncompared to the state-of-the-art ANN implementations.\n","authors":["Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2209.11741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02390v2","updated":"2023-03-14T16:54:14Z","published":"2022-10-05T17:05:56Z","title":"Bayesian Prompt Learning for Image-Language Model Generalization","summary":"  Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\n","authors":["Mohammad Mahdi Derakhshani","Enrique Sanchez","Adrian Bulat","Victor Guilherme Turrisi da Costa","Cees G. M. Snoek","Georgios Tzimiropoulos","Brais Martinez"],"pdf_url":"https://arxiv.org/pdf/2210.02390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08059v1","updated":"2023-03-14T16:51:14Z","published":"2023-03-14T16:51:14Z","title":"Fast Rates for Maximum Entropy Exploration","summary":"  We consider the reinforcement learning (RL) setting, in which the agent has\nto act in unknown environment driven by a Markov Decision Process (MDP) with\nsparse or even reward free signals. In this situation, exploration becomes the\nmain challenge. In this work, we study the maximum entropy exploration problem\nof two different types. The first type is visitation entropy maximization that\nwas previously considered by Hazan et al. (2019) in the discounted setting. For\nthis type of exploration, we propose an algorithm based on a game theoretic\nrepresentation that has $\\widetilde{\\mathcal{O}}(H^3 S^2 A / \\varepsilon^2)$\nsample complexity thus improving the $\\varepsilon$-dependence of Hazan et al.\n(2019), where $S$ is a number of states, $A$ is a number of actions, $H$ is an\nepisode length, and $\\varepsilon$ is a desired accuracy. The second type of\nentropy we study is the trajectory entropy. This objective function is closely\nrelated to the entropy-regularized MDPs, and we propose a simple modification\nof the UCBVI algorithm that has a sample complexity of order\n$\\widetilde{\\mathcal{O}}(1/\\varepsilon)$ ignoring dependence in $S, A, H$.\nInterestingly enough, it is the first theoretical result in RL literature\nestablishing that the exploration problem for the regularized MDPs can be\nstatistically strictly easier (in terms of sample complexity) than for the\nordinary MDPs.\n","authors":["Daniil Tiapkin","Denis Belomestny","Daniele Calandriello","Eric Moulines","Remi Munos","Alexey Naumov","Pierre Perrault","Yunhao Tang","Michal Valko","Pierre Menard"],"pdf_url":"https://arxiv.org/pdf/2303.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08054v1","updated":"2023-03-14T16:37:38Z","published":"2023-03-14T16:37:38Z","title":"Statistical Hardware Design With Multi-model Active Learning","summary":"  With the rising complexity of numerous novel applications that serve our\nmodern society comes the strong need to design efficient computing platforms.\nDesigning efficient hardware is, however, a complex multi-objective problem\nthat deals with multiple parameters and their interactions. Given that there\nare a large number of parameters and objectives involved in hardware design,\nsynthesizing all possible combinations is not a feasible method to find the\noptimal solution. One promising approach to tackle this problem is statistical\nmodeling of a desired hardware performance. Here, we propose a model-based\nactive learning approach to solve this problem. Our proposed method uses\nBayesian models to characterize various aspects of hardware performance. We\nalso use transfer learning and Gaussian regression bootstrapping techniques in\nconjunction with active learning to create more accurate models. Our proposed\nstatistical modeling method provides hardware models that are sufficiently\naccurate to perform design space exploration as well as performance prediction\nsimultaneously. We use our proposed method to perform design space exploration\nand performance prediction for various hardware setups, such as\nmicro-architecture design and OpenCL kernels for FPGA targets. Our experiments\nshow that the number of samples required to create performance models\nsignificantly reduces while maintaining the predictive power of our proposed\nstatistical models. For instance, in our performance prediction setting, the\nproposed method needs 65\\% fewer samples to create the model, and in the design\nspace exploration setting, our proposed method can find the best parameter\nsettings by exploring less than 50 samples.\n","authors":["Alireza Ghaffari","Masoud Asgharian","Yvon Savaria"],"pdf_url":"https://arxiv.org/pdf/2303.08054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08040v1","updated":"2023-03-14T16:19:44Z","published":"2023-03-14T16:19:44Z","title":"Demographic Parity Inspector: Fairness Audits via the Explanation Space","summary":"  Even if deployed with the best intentions, machine learning methods can\nperpetuate, amplify or even create social biases. Measures of (un-)fairness\nhave been proposed as a way to gauge the (non-)discriminatory nature of machine\nlearning models. However, proxies of protected attributes causing\ndiscriminatory effects remain challenging to address. In this work, we propose\na new algorithmic approach that measures group-wise demographic parity\nviolations and allows us to inspect the causes of inter-group discrimination.\nOur method relies on the novel idea of measuring the dependence of a model on\nthe protected attribute based on the explanation space, an informative space\nthat allows for more sensitive audits than the primary space of input data or\nprediction distributions, and allowing for the assertion of theoretical\ndemographic parity auditing guarantees. We provide a mathematical analysis,\nsynthetic examples, and experimental evaluation of real-world data. We release\nan open-source Python package with methods, routines, and tutorials.\n","authors":["Carlos Mougan","Laura State","Antonio Ferrara","Salvatore Ruggieri","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2303.08040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06710v2","updated":"2023-03-14T16:16:58Z","published":"2023-03-12T17:22:54Z","title":"Decision Making for Human-in-the-loop Robotic Agents via\n  Uncertainty-Aware Reinforcement Learning","summary":"  In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly\nautonomously in solving a task, but can request help from an external expert\nwhen needed. However, knowing when to request such assistance is critical: too\nfew requests can lead to the robot making mistakes, but too many requests can\noverload the expert. In this paper, we present a Reinforcement Learning based\napproach to this problem, where a semi-autonomous agent asks for external\nassistance when it has low confidence in the eventual success of the task. The\nconfidence level is computed by estimating the variance of the return from the\ncurrent state. We show that this estimate can be iteratively improved during\ntraining using a Bellman-like recursion. On discrete navigation problems with\nboth fully- and partially-observable state information, we show that our method\nmakes effective use of a limited budget of expert calls at run-time, despite\nhaving no access to the expert at training time.\n","authors":["Siddharth Singi","Zhanpeng He","Alvin Pan","Sandip Patel","Gunnar A. Sigurdsson","Robinson Piramuthu","Shuran Song","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2303.06710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08035v1","updated":"2023-03-14T16:15:28Z","published":"2023-03-14T16:15:28Z","title":"ISimDL: Importance Sampling-Driven Acceleration of Fault Injection\n  Simulations for Evaluating the Robustness of Deep Learning","summary":"  Deep Learning (DL) systems have proliferated in many applications, requiring\nspecialized hardware accelerators and chips. In the nano-era, devices have\nbecome increasingly more susceptible to permanent and transient faults.\nTherefore, we need an efficient methodology for analyzing the resilience of\nadvanced DL systems against such faults, and understand how the faults in\nneural accelerator chips manifest as errors at the DL application level, where\nfaults can lead to undetectable and unrecoverable errors. Using fault\ninjection, we can perform resilience investigations of the DL system by\nmodifying neuron weights and outputs at the software-level, as if the hardware\nhad been affected by a transient fault. Existing fault models reduce the search\nspace, allowing faster analysis, but requiring a-priori knowledge on the model,\nand not allowing further analysis of the filtered-out search space. Therefore,\nwe propose ISimDL, a novel methodology that employs neuron sensitivity to\ngenerate importance sampling-based fault-scenarios. Without any a-priori\nknowledge of the model-under-test, ISimDL provides an equivalent reduction of\nthe search space as existing works, while allowing long simulations to cover\nall the possible faults, improving on existing model requirements. Our\nexperiments show that the importance sampling provides up to 15x higher\nprecision in selecting critical faults than the random uniform sampling,\nreaching such precision in less than 100 faults. Additionally, we showcase\nanother practical use-case for importance sampling for reliable DNN design,\nnamely Fault Aware Training (FAT). By using ISimDL to select the faults leading\nto errors, we can insert the faults during the DNN training process to harden\nthe DNN against such faults. Using importance sampling in FAT reduces the\noverhead required for finding faults that lead to a predetermined drop in\naccuracy by more than 12x.\n","authors":["Alessio Colucci","Andreas Steininger","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.08035v1.pdf","comment":"Under review at IJCNN2023"},{"id":"http://arxiv.org/abs/2211.05634v2","updated":"2023-03-14T16:12:11Z","published":"2022-11-07T07:58:29Z","title":"Generalization of generative model for neuronal ensemble inference\n  method","summary":"  Various brain functions that are necessary to maintain life activities\nmaterialize through the interaction of countless neurons. Therefore, it is\nimportant to analyze the structure of functional neuronal network. To elucidate\nthe mechanism of brain function, many studies are being actively conducted on\nthe structure of functional neuronal ensemble and hub, including all areas of\nneuroscience. In addition, recent study suggests that the existence of\nfunctional neuronal ensembles and hubs contributes to the efficiency of\ninformation processing. For these reasons, there is a demand for methods to\ninfer functional neuronal ensembles from neuronal activity data, and methods\nbased on Bayesian inference have been proposed. However, there is a problem in\nmodeling the activity in Bayesian inference. The features of each neuron's\nactivity have non-stationarity depending on physiological experimental\nconditions. As a result, the assumption of stationarity in Bayesian inference\nmodel impedes inference, which leads to destabilization of inference results\nand degradation of inference accuracy. In this study, we extend the range of\nthe variable for expressing the neuronal state, and generalize the likelihood\nof the model for extended variables. By comparing with the previous study, our\nmodel can express the neuronal state in larger space. This generalization\nwithout restriction of the binary input enables us to perform soft clustering\nand apply the method to non-stationary neuroactivity data. In addition, for the\neffectiveness of the method, we apply the developed method to multiple\nsynthetic fluorescence data generated from the electrical potential data in\nleaky integrated-and-fire model.\n","authors":["Shun Kimura","Koujin Takeda"],"pdf_url":"https://arxiv.org/pdf/2211.05634v2.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.08032v1","updated":"2023-03-14T16:11:47Z","published":"2023-03-14T16:11:47Z","title":"BODEGA: Benchmark for Adversarial Example Generation in Credibility\n  Assessment","summary":"  Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we introduce BODEGA: a benchmark for testing both victim models\nand attack methods on four misinformation detection tasks in an evaluation\nframework designed to simulate real use-cases of content moderation. We also\nsystematically test the robustness of popular text classifiers against\navailable attacking techniques and discover that, indeed, in some cases barely\nsignificant changes in input text can mislead the models. We openly share the\nBODEGA code and data in hope of enhancing the comparability and replicability\nof further research in this area.\n","authors":["Piotr Przybyła","Alexander Shvets","Horacio Saggion"],"pdf_url":"https://arxiv.org/pdf/2303.08032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07506v3","updated":"2023-03-14T16:11:35Z","published":"2022-07-15T14:39:57Z","title":"Augmenting Softmax Information for Selective Classification with\n  Out-of-Distribution Data","summary":"  Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2207.07506v3.pdf","comment":"ACCV 2022 (Best Paper Award)\n  https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html"},{"id":"http://arxiv.org/abs/2303.08027v1","updated":"2023-03-14T16:08:45Z","published":"2023-03-14T16:08:45Z","title":"A Hierarchical Regression Chain Framework for Affective Vocal Burst\n  Recognition","summary":"  As a common way of emotion signaling via non-linguistic vocalizations, vocal\nburst (VB) plays an important role in daily social interaction. Understanding\nand modeling human vocal bursts are indispensable for developing robust and\ngeneral artificial intelligence. Exploring computational approaches for\nunderstanding vocal bursts is attracting increasing research attention. In this\nwork, we propose a hierarchical framework, based on chain regression models,\nfor affective recognition from VBs, that explicitly considers multiple\nrelationships: (i) between emotional states and diverse cultures; (ii) between\nlow-dimensional (arousal & valence) and high-dimensional (10 emotion classes)\nemotion spaces; and (iii) between various emotion classes within the\nhigh-dimensional space. To address the challenge of data sparsity, we also use\nself-supervised learning (SSL) representations with layer-wise and temporal\naggregation modules. The proposed systems participated in the ACII Affective\nVocal Burst (A-VB) Challenge 2022 and ranked first in the \"TWO'' and \"CULTURE''\ntasks. Experimental results based on the ACII Challenge 2022 dataset\ndemonstrate the superior performance of the proposed system and the\neffectiveness of considering multiple relationships using hierarchical\nregression chain models.\n","authors":["Jinchao Li","Xixin Wu","Kaitao Song","Dongsheng Li","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2303.08027v1.pdf","comment":"5 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.08019v1","updated":"2023-03-14T16:03:28Z","published":"2023-03-14T16:03:28Z","title":"Leveraging Pretrained Representations with Task-related Keywords for\n  Alzheimer's Disease Detection","summary":"  With the global population aging rapidly, Alzheimer's disease (AD) is\nparticularly prominent in older adults, which has an insidious onset and leads\nto a gradual, irreversible deterioration in cognitive domains (memory,\ncommunication, etc.). Speech-based AD detection opens up the possibility of\nwidespread screening and timely disease intervention. Recent advances in\npre-trained models motivate AD detection modeling to shift from low-level\nfeatures to high-level representations. This paper presents several efficient\nmethods to extract better AD-related cues from high-level acoustic and\nlinguistic features. Based on these features, the paper also proposes a novel\ntask-oriented approach by modeling the relationship between the participants'\ndescription and the cognitive task. Experiments are carried out on the ADReSS\ndataset in a binary classification setup, and models are evaluated on the\nunseen test set. Results and comparison with recent literature demonstrate the\nefficiency and superior performance of proposed acoustic, linguistic and\ntask-oriented methods. The findings also show the importance of semantic and\nsyntactic information, and feasibility of automation and generalization with\nthe promising audio-only and task-oriented methods for the AD detection task.\n","authors":["Jinchao Li","Kaitao Song","Junan Li","Bo Zheng","Dongsheng Li","Xixin Wu","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2303.08019v1.pdf","comment":"5 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.08017v1","updated":"2023-03-14T16:02:46Z","published":"2023-03-14T16:02:46Z","title":"Reliable Beamforming at Terahertz Bands: Are Causal Representations the\n  Way Forward?","summary":"  Future wireless services, such as the metaverse require high information\nrate, reliability, and low latency. Multi-user wireless systems can meet such\nrequirements by utilizing the abundant terahertz bandwidth with a massive\nnumber of antennas, creating narrow beamforming solutions. However, existing\nsolutions lack proper modeling of channel dynamics, resulting in inaccurate\nbeamforming solutions in high-mobility scenarios. Herein, a dynamic,\nsemantically aware beamforming solution is proposed for the first time,\nutilizing novel artificial intelligence algorithms in variational causal\ninference to compute the time-varying dynamics of the causal representation of\nmulti-modal data and the beamforming. Simulations show that the proposed\ncausality-guided approach for Terahertz (THz) beamforming outperforms classical\nMIMO beamforming techniques.\n","authors":["Christo Kurisummoottil Thomas","Walid Saad"],"pdf_url":"https://arxiv.org/pdf/2303.08017v1.pdf","comment":"Accepted at IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.08010v1","updated":"2023-03-14T15:57:54Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07991v1","updated":"2023-03-14T15:45:35Z","published":"2023-03-14T15:45:35Z","title":"Finding the Needle in a Haystack: Unsupervised Rationale Extraction from\n  Long Text Classifiers","summary":"  Long-sequence transformers are designed to improve the representation of\nlonger texts by language models and their performance on downstream\ndocument-level tasks. However, not much is understood about the quality of\ntoken-level predictions in long-form models. We investigate the performance of\nsuch architectures in the context of document classification with unsupervised\nrationale extraction. We find standard soft attention methods to perform\nsignificantly worse when combined with the Longformer language model. We\npropose a compositional soft attention architecture that applies RoBERTa\nsentence-wise to extract plausible rationales at the token-level. We find this\nmethod to significantly outperform Longformer-driven baselines on sentiment\nclassification datasets, while also exhibiting significantly lower runtimes.\n","authors":["Kamil Bujel","Andrew Caines","Helen Yannakoudakis","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2303.07991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07988v1","updated":"2023-03-14T15:44:40Z","published":"2023-03-14T15:44:40Z","title":"Partial Neural Optimal Transport","summary":"  We propose a novel neural method to compute partial optimal transport (OT)\nmaps, i.e., OT maps between parts of measures of the specified masses. We test\nour partial neural optimal transport algorithm on synthetic examples.\n","authors":["Milena Gazdieva","Alexander Korotin","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2303.07988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07987v1","updated":"2023-03-14T15:44:20Z","published":"2023-03-14T15:44:20Z","title":"Practically Solving LPN in High Noise Regimes Faster Using Neural\n  Networks","summary":"  We conduct a systematic study of solving the learning parity with noise\nproblem (LPN) using neural networks. Our main contribution is designing\nfamilies of two-layer neural networks that practically outperform classical\nalgorithms in high-noise, low-dimension regimes. We consider three settings\nwhere the numbers of LPN samples are abundant, very limited, and in between. In\neach setting we provide neural network models that solve LPN as fast as\npossible. For some settings we are also able to provide theories that explain\nthe rationale of the design of our models. Comparing with the previous\nexperiments of Esser, Kubler, and May (CRYPTO 2017), for dimension $n = 26$,\nnoise rate $\\tau = 0.498$, the ''Guess-then-Gaussian-elimination'' algorithm\ntakes 3.12 days on 64 CPU cores, whereas our neural network algorithm takes 66\nminutes on 8 GPUs. Our algorithm can also be plugged into the hybrid algorithms\nfor solving middle or large dimension LPN instances.\n","authors":["Haozhe Jiang","Kaiyue Wen","Yilei Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07987v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2303.07971v1","updated":"2023-03-14T15:24:05Z","published":"2023-03-14T15:24:05Z","title":"A Theory of Emergent In-Context Learning as Implicit Structure Induction","summary":"  Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.\n","authors":["Michael Hahn","Navin Goyal"],"pdf_url":"https://arxiv.org/pdf/2303.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09078v3","updated":"2023-03-14T15:13:06Z","published":"2022-09-19T15:12:47Z","title":"NIERT: Accurate Numerical Interpolation through Unifying Scattered Data\n  Representations using Transformer Encoder","summary":"  Interpolation for scattered data is a classical problem in numerical\nanalysis, with a long history of theoretical and practical contributions.\nRecent advances have utilized deep neural networks to construct interpolators,\nexhibiting excellent and generalizable performance. However, they still fall\nshort in two aspects: \\textbf{1) inadequate representation learning}, resulting\nfrom separate embeddings of observed and target points in popular\nencoder-decoder frameworks and \\textbf{2) limited generalization power}, caused\nby overlooking prior interpolation knowledge shared across different domains.\nTo overcome these limitations, we present a \\textbf{N}umerical\n\\textbf{I}nterpolation approach using \\textbf{E}ncoder \\textbf{R}epresentation\nof \\textbf{T}ransformers (called \\textbf{NIERT}). On one hand, NIERT utilizes\nan encoder-only framework rather than the encoder-decoder structure. This way,\nNIERT can embed observed and target points into a unified encoder\nrepresentation space, thus effectively exploiting the correlations among them\nand obtaining more precise representations. On the other hand, we propose to\npre-train NIERT on large-scale synthetic mathematical functions to acquire\nprior interpolation knowledge, and transfer it to multiple interpolation\ndomains with consistent performance gain. On both synthetic and real-world\ndatasets, NIERT outperforms the existing approaches by a large margin, i.e.,\n4.3$\\sim$14.3$\\times$ lower MAE on TFRD subsets, and 1.7/1.8/8.7$\\times$ lower\nMSE on Mathit/PhysioNet/PTV datasets. The source code of NIERT is available at\nhttps://github.com/DingShizhe/NIERT.\n","authors":["Shizhe Ding","Boyang Xia","Milong Ren","Dongbo Bu"],"pdf_url":"https://arxiv.org/pdf/2209.09078v3.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2206.04636v3","updated":"2023-03-14T15:07:16Z","published":"2022-06-09T17:34:39Z","title":"Spatial Entropy as an Inductive Bias for Vision Transformers","summary":"  Recent work on Vision Transformers (VTs) showed that introducing a local\ninductive bias in the VT architecture helps reducing the number of samples\nnecessary for training. However, the architecture modifications lead to a loss\nof generality of the Transformer backbone, partially contradicting the push\ntowards the development of uniform architectures, shared, e.g., by both the\nComputer Vision and the Natural Language Processing areas. In this work, we\npropose a different and complementary direction, in which a local bias is\nintroduced using an auxiliary self-supervised task, performed jointly with\nstandard supervised training. Specifically, we exploit the observation that the\nattention maps of VTs, when trained with self-supervision, can contain a\nsemantic segmentation structure which does not spontaneously emerge when\ntraining is supervised. Thus, we explicitly encourage the emergence of this\nspatial clustering as a form of training regularization. In more detail, we\nexploit the assumption that, in a given image, objects usually correspond to\nfew connected regions, and we propose a spatial formulation of the information\nentropy to quantify this object-based inductive bias. By minimizing the\nproposed spatial entropy, we include an additional self-supervised signal\nduring training. Using extensive experiments, we show that the proposed\nregularization leads to equivalent or better results than other VT proposals\nwhich include a local bias by changing the basic Transformer architecture, and\nit can drastically boost the VT final accuracy when using small-medium training\nsets. The code is available at https://github.com/helia95/SAR.\n","authors":["Elia Peruzzo","Enver Sangineto","Yahui Liu","Marco De Nadai","Wei Bi","Bruno Lepri","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2206.04636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00841v3","updated":"2023-03-14T15:04:33Z","published":"2022-10-03T11:57:30Z","title":"Smooth image-to-image translations with latent space interpolations","summary":"  Multi-domain image-to-image (I2I) translations can transform a source image\naccording to the style of a target domain. One important, desired\ncharacteristic of these transformations, is their graduality, which corresponds\nto a smooth change between the source and the target image when their\nrespective latent-space representations are linearly interpolated. However,\nstate-of-the-art methods usually perform poorly when evaluated using\ninter-domain interpolations, often producing abrupt changes in the appearance\nor non-realistic intermediate images. In this paper, we argue that one of the\nmain reasons behind this problem is the lack of sufficient inter-domain\ntraining data and we propose two different regularization methods to alleviate\nthis issue: a new shrinkage loss, which compacts the latent space, and a Mixup\ndata-augmentation strategy, which flattens the style representations between\ndomains. We also propose a new metric to quantitatively evaluate the degree of\nthe interpolation smoothness, an aspect which is not sufficiently covered by\nthe existing I2I translation metrics. Using both our proposed metric and\nstandard evaluation protocols, we show that our regularization techniques can\nimprove the state-of-the-art multi-domain I2I translations by a large margin.\nOur code will be made publicly available upon the acceptance of this article.\n","authors":["Yahui Liu","Enver Sangineto","Yajing Chen","Linchao Bao","Haoxian Zhang","Nicu Sebe","Bruno Lepri","Marco De Nadai"],"pdf_url":"https://arxiv.org/pdf/2210.00841v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01286v2","updated":"2023-03-14T14:53:27Z","published":"2022-12-31T22:56:04Z","title":"Pseudo-Inverted Bottleneck Convolution for DARTS Search Space","summary":"  Differentiable Architecture Search (DARTS) has attracted considerable\nattention as a gradient-based neural architecture search method. Since the\nintroduction of DARTS, there has been little work done on adapting the action\nspace based on state-of-art architecture design principles for CNNs. In this\nwork, we aim to address this gap by incrementally augmenting the DARTS search\nspace with micro-design changes inspired by ConvNeXt and studying the trade-off\nbetween accuracy, evaluation layer count, and computational cost. We introduce\nthe Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the\ncomputational footprint of the inverted bottleneck block proposed in ConvNeXt.\nOur proposed architecture is much less sensitive to evaluation layer count and\noutperforms a DARTS network with similar size significantly, at layer counts as\nsmall as 2. Furthermore, with less layers, not only does it achieve higher\naccuracy with lower computational footprint (measured in GMACs) and parameter\ncount, GradCAM comparisons show that our network can better detect distinctive\nfeatures of target objects compared to DARTS. Code is available from\nhttps://github.com/mahdihosseini/PIBConv.\n","authors":["Arash Ahmadian","Louis S. P. Liu","Yue Fei","Konstantinos N. Plataniotis","Mahdi S. Hosseini"],"pdf_url":"https://arxiv.org/pdf/2301.01286v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.06274v2","updated":"2023-03-14T14:53:19Z","published":"2023-03-11T01:21:13Z","title":"CoNIC Challenge: Pushing the Frontiers of Nuclear Detection,\n  Segmentation, Classification and Counting","summary":"  Nuclear detection, segmentation and morphometric profiling are essential in\nhelping us further understand the relationship between histology and patient\noutcome. To drive innovation in this area, we setup a community-wide challenge\nusing the largest available dataset of its kind to assess nuclear segmentation\nand cellular composition. Our challenge, named CoNIC, stimulated the\ndevelopment of reproducible algorithms for cellular recognition with real-time\nresult inspection on public leaderboards. We conducted an extensive\npost-challenge analysis based on the top-performing models using 1,658\nwhole-slide images of colon tissue. With around 700 million detected nuclei per\nmodel, associated features were used for dysplasia grading and survival\nanalysis, where we demonstrated that the challenge's improvement over the\nprevious state-of-the-art led to significant boosts in downstream performance.\nOur findings also suggest that eosinophils and neutrophils play an important\nrole in the tumour microevironment. We release challenge models and WSI-level\nresults to foster the development of further methods for biomarker discovery.\n","authors":["Simon Graham","Quoc Dang Vu","Mostafa Jahanifar","Martin Weigert","Uwe Schmidt","Wenhua Zhang","Jun Zhang","Sen Yang","Jinxi Xiang","Xiyue Wang","Josef Lorenz Rumberger","Elias Baumann","Peter Hirsch","Lihao Liu","Chenyang Hong","Angelica I. Aviles-Rivero","Ayushi Jain","Heeyoung Ahn","Yiyu Hong","Hussam Azzuni","Min Xu","Mohammad Yaqub","Marie-Claire Blache","Benoît Piégu","Bertrand Vernay","Tim Scherr","Moritz Böhland","Katharina Löffler","Jiachen Li","Weiqin Ying","Chixin Wang","Dagmar Kainmueller","Carola-Bibiane Schönlieb","Shuolin Liu","Dhairya Talsania","Yughender Meda","Prakash Mishra","Muhammad Ridzuan","Oliver Neumann","Marcel P. Schilling","Markus Reischl","Ralf Mikut","Banban Huang","Hsiang-Chin Chien","Ching-Ping Wang","Chia-Yen Lee","Hong-Kun Lin","Zaiyi Liu","Xipeng Pan","Chu Han","Jijun Cheng","Muhammad Dawood","Srijay Deshpande","Raja Muhammad Saad Bashir","Adam Shephard","Pedro Costa","João D. Nunes","Aurélio Campilho","Jaime S. Cardoso","Hrishikesh P S","Densen Puthussery","Devika R G","Jiji C V","Ye Zhang","Zijie Fang","Zhifan Lin","Yongbing Zhang","Chunhui Lin","Liukun Zhang","Lijian Mao","Min Wu","Vi Thi-Tuong Vo","Soo-Hyung Kim","Taebum Lee","Satoshi Kondo","Satoshi Kasai","Pranay Dumbhare","Vedant Phuse","Yash Dubey","Ankush Jamthikar","Trinh Thi Le Vuong","Jin Tae Kwak","Dorsa Ziaei","Hyun Jung","Tianyi Miao","David Snead","Shan E Ahmed Raza","Fayyaz Minhas","Nasir M. Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2303.06274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06871v2","updated":"2023-03-14T14:49:08Z","published":"2023-03-13T05:42:58Z","title":"Physics-driven machine learning models coupling PyTorch and Firedrake","summary":"  Partial differential equations (PDEs) are central to describing and modelling\ncomplex physical systems that arise in many disciplines across science and\nengineering. However, in many realistic applications PDE modelling provides an\nincomplete description of the physics of interest. PDE-based machine learning\ntechniques are designed to address this limitation. In this approach, the PDE\nis used as an inductive bias enabling the coupled model to rely on fundamental\nphysical laws while requiring less training data. The deployment of\nhigh-performance simulations coupling PDEs and machine learning to complex\nproblems necessitates the composition of capabilities provided by machine\nlearning and PDE-based frameworks. We present a simple yet effective coupling\nbetween the machine learning framework PyTorch and the PDE system Firedrake\nthat provides researchers, engineers and domain specialists with a high\nproductive way of specifying coupled models while only requiring trivial\nchanges to existing code.\n","authors":["Nacime Bouziani","David A. Ham"],"pdf_url":"https://arxiv.org/pdf/2303.06871v2.pdf","comment":"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning"},{"id":"http://arxiv.org/abs/2207.14653v2","updated":"2023-03-14T14:47:08Z","published":"2022-07-29T12:57:50Z","title":"Ensemble forecasts in reproducing kernel Hilbert space family: dynamical\n  systems in Wonderland","summary":"  A methodological framework for ensemble-based estimation and simulation of\nhigh dimensional dynamical systems such as the oceanic or atmospheric flows is\nproposed. To that end, the dynamical system is embedded in a family of\nreproducing kernel Hilbert spaces with kernel functions driven by the dynamics.\nThis family is nicknamed Wonderland for its appealing properties. In Wonderland\nthe Koopman and Perron-Frobenius operators are unitary and uniformly\ncontinuous. This property warrants they can be expressed in exponential series\nof diagonalizable bounded infinitesimal generators. Access to Lyapunov\nexponents and to exact ensemble based expressions of the tangent linear\ndynamics are directly available as well. Wonderland enables us the devise of\nstrikingly simple ensemble data assimilation methods for trajectory\nreconstructions in terms of constant-in-time linear combinations of trajectory\nsamples. Such an embarrassingly simple strategy is made possible through a\nfully justified superposition principle ensuing from several fundamental\ntheorems.\n","authors":["Benjamin Dufée","Bérenger Hug","Etienne Memin","Gilles Tissot"],"pdf_url":"https://arxiv.org/pdf/2207.14653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07940v1","updated":"2023-03-14T14:25:54Z","published":"2023-03-14T14:25:54Z","title":"On the Connection between Concept Drift and Uncertainty in Industrial\n  Artificial Intelligence","summary":"  AI-based digital twins are at the leading edge of the Industry 4.0\nrevolution, which are technologically empowered by the Internet of Things and\nreal-time data analysis. Information collected from industrial assets is\nproduced in a continuous fashion, yielding data streams that must be processed\nunder stringent timing constraints. Such data streams are usually subject to\nnon-stationary phenomena, causing that the data distribution of the streams may\nchange, and thus the knowledge captured by models used for data analysis may\nbecome obsolete (leading to the so-called concept drift effect). The early\ndetection of the change (drift) is crucial for updating the model's knowledge,\nwhich is challenging especially in scenarios where the ground truth associated\nto the stream data is not readily available. Among many other techniques, the\nestimation of the model's confidence has been timidly suggested in a few\nstudies as a criterion for detecting drifts in unsupervised settings. The goal\nof this manuscript is to confirm and expose solidly the connection between the\nmodel's confidence in its output and the presence of a concept drift,\nshowcasing it experimentally and advocating for a major consideration of\nuncertainty estimation in comparative studies to be reported in the future.\n","authors":["Jesus L. Lobo","Ibai Laña","Eneko Osaba","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2303.07940v1.pdf","comment":"2 pages, 1 figure, 2023 IEEE Conference on Artificial Intelligence\n  (IEEE CAI)"},{"id":"http://arxiv.org/abs/2303.07925v1","updated":"2023-03-14T14:10:37Z","published":"2023-03-14T14:10:37Z","title":"Understanding Model Complexity for temporal tabular and multi-variate\n  time series, case study with Numerai data science tournament","summary":"  In this paper, we explore the use of different feature engineering and\ndimensionality reduction methods in multi-variate time-series modelling. Using\na feature-target cross correlation time series dataset created from Numerai\ntournament, we demonstrate under over-parameterised regime, both the\nperformance and predictions from different feature engineering methods converge\nto the same equilibrium, which can be characterised by the reproducing kernel\nHilbert space. We suggest a new Ensemble method, which combines different\nrandom non-linear transforms followed by ridge regression for modelling high\ndimensional time-series. Compared to some commonly used deep learning models\nfor sequence modelling, such as LSTM and transformers, our method is more\nrobust (lower model variance over different random seeds and less sensitive to\nthe choice of architecture) and more efficient. An additional advantage of our\nmethod is model simplicity as there is no need to use sophisticated deep\nlearning frameworks such as PyTorch. The learned feature rankings are then\napplied to the temporal tabular prediction problem in the Numerai tournament,\nand the predictive power of feature rankings obtained from our method is better\nthan the baseline prediction model based on moving averages\n","authors":["Thomas Wong","Prof. Mauricio Barahona"],"pdf_url":"https://arxiv.org/pdf/2303.07925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07924v1","updated":"2023-03-14T14:10:16Z","published":"2023-03-14T14:10:16Z","title":"Improving Accented Speech Recognition with Multi-Domain Training","summary":"  Thanks to the rise of self-supervised learning, automatic speech recognition\n(ASR) systems now achieve near-human performance on a wide variety of datasets.\nHowever, they still lack generalization capability and are not robust to domain\nshifts like accent variations. In this work, we use speech audio representing\nfour different French accents to create fine-tuning datasets that improve the\nrobustness of pre-trained ASR models. By incorporating various accents in the\ntraining set, we obtain both in-domain and out-of-domain improvements. Our\nnumerical experiments show that we can reduce error rates by up to 25%\n(relative) on African and Belgian accents compared to single-domain training\nwhile keeping a good performance on standard French.\n","authors":["Lucas Maison","Yannick Estève"],"pdf_url":"https://arxiv.org/pdf/2303.07924v1.pdf","comment":"5 pages, 2 figures. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07917v1","updated":"2023-03-14T14:00:32Z","published":"2023-03-14T14:00:32Z","title":"Reachability Analysis of Neural Networks with Uncertain Parameters","summary":"  The literature on reachability analysis methods for neural networks currently\nonly focuses on uncertainties on the network's inputs. In this paper, we\nintroduce two new approaches for the reachability analysis of neural networks\nwith additional uncertainties on their internal parameters (weight matrices and\nbias vectors of each layer), which may open the field of formal methods on\nneural networks to new topics, such as safe training or network repair. The\nfirst and main method that we propose relies on existing reachability analysis\napproach based on mixed monotonicity (initially introduced for dynamical\nsystems). The second proposed approach extends the ESIP (Error-based Symbolic\nInterval Propagation) approach which was first implemented in the verification\ntool Neurify, and first mentioned in the publication of the tool VeriNet.\nAlthough the ESIP approach has been shown to often outperform the\nmixed-monotonicity reachability analysis in the classical case with\nuncertainties only on the network's inputs, we show in this paper through\nnumerical simulations that the situation is greatly reversed (in terms of\nprecision, computation time, memory usage, and broader applicability) when\ndealing with uncertainties on the weights and biases.\n","authors":["Pierre-Jean Meyer"],"pdf_url":"https://arxiv.org/pdf/2303.07917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07909v1","updated":"2023-03-14T13:49:54Z","published":"2023-03-14T13:49:54Z","title":"Text-to-image Diffusion Model in Generative AI: A Survey","summary":"  This survey reviews text-to-image diffusion models in the context that\ndiffusion models have emerged to be popular for a wide range of generative\ntasks. As a self-contained work, this survey starts with a brief introduction\nof how a basic diffusion model works for image synthesis, followed by how\ncondition or guidance improves learning. Based on that, we present a review of\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\ntext-to-image. We further summarize applications beyond text-to-image\ngeneration: text-guided creative generation and text-guided image editing.\nBeyond the progress made so far, we discuss existing challenges and promising\nfuture directions.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Mengchun Zhang","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.07909v1.pdf","comment":"First survey on the recent progress of text-to-image generation based\n  on the diffusion model (under progress)"},{"id":"http://arxiv.org/abs/2303.06965v2","updated":"2023-03-14T13:47:14Z","published":"2023-03-13T10:06:41Z","title":"Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction\n  Pretraining and Conditional Molecule Generation","summary":"  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. In recent years, there has been a growing need for\na large-scale deep-learning framework that can efficiently capture the basic\nrules of chemical reactions. In this paper, we have proposed a unified\nframework that addresses both the reaction representation learning and molecule\ngeneration tasks, which allows for a more holistic approach. Inspired by the\norganic chemistry mechanism, we develop a novel pretraining framework that\nenables us to incorporate inductive biases into the model. Our framework\nachieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, this framework can be applied to reaction-based\ngenerative models, overcoming the limitations of current molecule generation\nmodels that rely on a small number of reaction templates. In the extensive\nexperiments, our model generates synthesizable drug-like structures of high\nquality. Overall, our work presents a significant step toward a large-scale\ndeep-learning framework for a variety of reaction-based applications.\n","authors":["Bo Qiang","Yiran Zhou","Yuheng Ding","Ningfeng Liu","Song Song","Liangren Zhang","Bo Huang","Zhenming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07900v1","updated":"2023-03-14T13:41:28Z","published":"2023-03-14T13:41:28Z","title":"Generalised Scale-Space Properties for Probabilistic Diffusion Models","summary":"  Probabilistic diffusion models enjoy increasing popularity in the deep\nlearning community. They generate convincing samples from a learned\ndistribution of input images with a wide field of practical applications.\nOriginally, these approaches were motivated from drift-diffusion processes, but\nthese origins find less attention in recent, practice-oriented publications.\n  We investigate probabilistic diffusion models from the viewpoint of\nscale-space research and show that they fulfil generalised scale-space\nproperties on evolving probability distributions. Moreover, we discuss\nsimilarities and differences between interpretations of the physical core\nconcept of drift-diffusion in the deep learning and model-based world. To this\nend, we examine relations of probabilistic diffusion to osmosis filters.\n","authors":["Pascal Peter"],"pdf_url":"https://arxiv.org/pdf/2303.07900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07127v2","updated":"2023-03-14T13:26:03Z","published":"2023-03-13T13:58:03Z","title":"Improving physics-informed neural networks with meta-learned\n  optimization","summary":"  We show that the error achievable using physics-informed neural networks for\nsolving systems of differential equations can be substantially reduced when\nthese networks are trained using meta-learned optimization methods rather than\nto using fixed, hand-crafted optimizers as traditionally done. We choose a\nlearnable optimization method based on a shallow multi-layer perceptron that is\nmeta-trained for specific classes of differential equations. We illustrate\nmeta-trained optimizers for several equations of practical relevance in\nmathematical physics, including the linear advection equation, Poisson's\nequation, the Korteweg--de Vries equation and Burgers' equation. We also\nillustrate that meta-learned optimizers exhibit transfer learning abilities, in\nthat a meta-trained optimizer on one differential equation can also be\nsuccessfully deployed on another differential equation.\n","authors":["Alex Bihlo"],"pdf_url":"https://arxiv.org/pdf/2303.07127v2.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2103.03076v2","updated":"2023-03-14T13:05:12Z","published":"2021-03-04T14:57:53Z","title":"Dynamic Efficient Adversarial Training Guided by Gradient Magnitude","summary":"  Adversarial training is an effective but time-consuming way to train robust\ndeep neural networks that can withstand strong adversarial attacks. As a\nresponse to its inefficiency, we propose Dynamic Efficient Adversarial Training\n(DEAT), which gradually increases the adversarial iteration during training. We\ndemonstrate that the gradient's magnitude correlates with the curvature of the\ntrained model's loss landscape, allowing it to reflect the effect of\nadversarial training. Therefore, based on the magnitude of the gradient, we\npropose a general acceleration strategy, M+ acceleration, which enables an\nautomatic and highly effective method of adjusting the training procedure. M+\nacceleration is computationally efficient and easy to implement. It is suited\nfor DEAT and compatible with the majority of existing adversarial training\ntechniques. Extensive experiments have been done on CIFAR-10 and ImageNet\ndatasets with various training environments. The results show that the proposed\nM+ acceleration significantly improves the training efficiency of existing\nadversarial training methods while achieving similar robustness performance.\nThis demonstrates that the strategy is highly adaptive and offers a valuable\nsolution for automatic adversarial training.\n","authors":["Fu Wang","Yanghao Zhang","Yanbin Zheng","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2103.03076v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.07864v1","updated":"2023-03-14T12:55:42Z","published":"2023-03-14T12:55:42Z","title":"DualMix: Unleashing the Potential of Data Augmentation for Online\n  Class-Incremental Learning","summary":"  Online Class-Incremental (OCI) learning has sparked new approaches to expand\nthe previously trained model knowledge from sequentially arriving data streams\nwith new classes. Unfortunately, OCI learning can suffer from catastrophic\nforgetting (CF) as the decision boundaries for old classes can become\ninaccurate when perturbated by new ones. Existing literature have applied the\ndata augmentation (DA) to alleviate the model forgetting, while the role of DA\nin OCI has not been well understood so far. In this paper, we theoretically\nshow that augmented samples with lower correlation to the original data are\nmore effective in preventing forgetting. However, aggressive augmentation may\nalso reduce the consistency between data and corresponding labels, which\nmotivates us to exploit proper DA to boost the OCI performance and prevent the\nCF problem. We propose the Enhanced Mixup (EnMix) method that mixes the\naugmented samples and their labels simultaneously, which is shown to enhance\nthe sample diversity while maintaining strong consistency with corresponding\nlabels. Further, to solve the class imbalance problem, we design an Adaptive\nMixup (AdpMix) method to calibrate the decision boundaries by mixing samples\nfrom both old and new classes and dynamically adjusting the label mixing ratio.\nOur approach is demonstrated to be effective on several benchmark datasets\nthrough extensive experiments, and it is shown to be compatible with other\nreplay-based techniques.\n","authors":["Yunfeng Fan","Wenchao Xu","Haozhao Wang","Jiaqi Zhu","Junxiao Wang","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2303.07864v1.pdf","comment":"10 pages, 7 figures and 3 tables"},{"id":"http://arxiv.org/abs/2204.10416v2","updated":"2023-03-14T12:49:22Z","published":"2022-04-21T21:43:23Z","title":"CycleSense: Detecting Near Miss Incidents in Bicycle Traffic from Mobile\n  Motion Sensors","summary":"  In cities worldwide, cars cause health and traffic problems whichcould be\npartly mitigated through an increased modal share of bicycles. Many people,\nhowever, avoid cycling due to a lack of perceived safety. For city planners,\naddressing this is hard as they lack insights intowhere cyclists feel safe and\nwhere they do not. To gain such insights,we have in previous work proposed the\ncrowdsourcing platform SimRa,which allows cyclists to record their rides and\nreport near miss incidentsvia a smartphone app. In this paper, we present\nCycleSense, a combination of signal pro-cessing and Machine Learning\ntechniques, which partially automatesthe detection of near miss incidents, thus\nmaking the reporting of nearmiss incidents easier. Using the SimRa data set, we\nevaluate CycleSenseby comparing it to a baseline method used by SimRa and show\nthat itsignificantly improves incident detection.\n","authors":["Ahmet-Serdar Karakaya","Thomas Ritter","Felix Biessmann","David Bermbach"],"pdf_url":"https://arxiv.org/pdf/2204.10416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07853v1","updated":"2023-03-14T12:46:52Z","published":"2023-03-14T12:46:52Z","title":"BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised\n  Semantic Segmentation of Medical Images","summary":"  Weakly Supervised Semantic Segmentation (WSSS) with only image-level\nsupervision is a promising approach to deal with the need for Segmentation\nnetworks, especially for generating a large number of pixel-wise masks in a\ngiven dataset. However, most state-of-the-art image-level WSSS techniques lack\nan understanding of the geometric features embedded in the images since the\nnetwork cannot derive any object boundary information from just image-level\nlabels. We define a boundary here as the line separating an object and its\nbackground, or two different objects. To address this drawback, we propose our\nnovel BoundaryCAM framework, which deploys state-of-the-art class activation\nmaps combined with various post-processing techniques in order to achieve\nfine-grained higher-accuracy segmentation masks. To achieve this, we\ninvestigate a state-of-the-art unsupervised semantic segmentation network that\ncan be used to construct a boundary map, which enables BoundaryCAM to predict\nobject locations with sharper boundaries. By applying our method to WSSS\npredictions, we were able to achieve up to 10% improvements even to the benefit\nof the current state-of-the-art WSSS methods for medical imaging. The framework\nis open-source and accessible online at\nhttps://github.com/bharathprabakaran/BoundaryCAM.\n","authors":["Bharath Srinivas Prabakaran","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07852v1","updated":"2023-03-14T12:46:48Z","published":"2023-03-14T12:46:48Z","title":"FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network\n  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features","summary":"  Ultrasound imaging is one of the most prominent technologies to evaluate the\ngrowth, progression, and overall health of a fetus during its gestation.\nHowever, the interpretation of the data obtained from such studies is best left\nto expert physicians and technicians who are trained and well-versed in\nanalyzing such images. To improve the clinical workflow and potentially develop\nan at-home ultrasound-based fetal monitoring platform, we present a novel fetus\nphantom ultrasound dataset, FPUS23, which can be used to identify (1) the\ncorrect diagnostic planes for estimating fetal biometric values, (2) fetus\norientation, (3) their anatomical features, and (4) bounding boxes of the fetus\nphantom anatomies at 23 weeks gestation. The entire dataset is composed of\n15,728 images, which are used to train four different Deep Neural Network\nmodels, built upon a ResNet34 backbone, for detecting aforementioned fetus\nfeatures and use-cases. We have also evaluated the models trained using our\nFPUS23 dataset, to show that the information learned by these models can be\nused to substantially increase the accuracy on real-world ultrasound fetus\ndatasets. We make the FPUS23 dataset and the pre-trained models publicly\naccessible at https://github.com/bharathprabakaran/FPUS23, which will further\nfacilitate future research on fetal ultrasound imaging and analysis.\n","authors":["Bharath Srinivas Prabakaran","Paul Hamelmann","Erik Ostrowski","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.03540v2","updated":"2023-03-14T12:38:35Z","published":"2021-10-07T15:01:33Z","title":"A Broad Ensemble Learning System for Drifting Stream Classification","summary":"  In a data stream environment, classification models must handle concept drift\nefficiently and effectively. Ensemble methods are widely used for this purpose;\nhowever, the ones available in the literature either use a large data chunk to\nupdate the model or learn the data one by one. In the former, the model may\nmiss the changes in the data distribution, and in the latter, the model may\nsuffer from inefficiency and instability. To address these issues, we introduce\na novel ensemble approach based on the Broad Learning System (BLS), where mini\nchunks are used at each update. BLS is an effective lightweight neural\narchitecture recently developed for incremental learning. Although it is fast,\nit requires huge data chunks for effective updates, and is unable to handle\ndynamic changes observed in data streams. Our proposed approach named Broad\nEnsemble Learning System (BELS) uses a novel updating method that significantly\nimproves best-in-class model accuracy. It employs an ensemble of output layers\nto address the limitations of BLS and handle drifts. Our model tracks the\nchanges in the accuracy of the ensemble components and react to these changes.\nWe present the mathematical derivation of BELS, perform comprehensive\nexperiments with 20 datasets that demonstrate the adaptability of our model to\nvarious drift types, and provide hyperparameter and ablation analysis of our\nproposed model. Our experiments show that the proposed approach outperforms\nnine state-of-the-art baselines and supplies an overall improvement of 13.28%\nin terms of average prequential accuracy.\n","authors":["Sepehr Bakhshi","Pouya Ghahramanian","Hamed Bonab","Fazli Can"],"pdf_url":"https://arxiv.org/pdf/2110.03540v2.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2303.07847v1","updated":"2023-03-14T12:37:22Z","published":"2023-03-14T12:37:22Z","title":"Transfer Learning for Real-time Deployment of a Screening Tool for\n  Depression Detection Using Actigraphy","summary":"  Automated depression screening and diagnosis is a highly relevant problem\ntoday. There are a number of limitations of the traditional depression\ndetection methods, namely, high dependence on clinicians and biased\nself-reporting. In recent years, research has suggested strong potential in\nmachine learning (ML) based methods that make use of the user's passive data\ncollected via wearable devices. However, ML is data hungry. Especially in the\nhealthcare domain primary data collection is challenging. In this work, we\npresent an approach based on transfer learning, from a model trained on a\nsecondary dataset, for the real time deployment of the depression screening\ntool based on the actigraphy data of users. This approach enables machine\nlearning modelling even with limited primary data samples. A modified version\nof leave one out cross validation approach performed on the primary set\nresulted in mean accuracy of 0.96, where in each iteration one subject's data\nfrom the primary set was set aside for testing.\n","authors":["Rajanikant Ghate","Nayan Kalnad","Rahee Walambe","Ketan Kotecha"],"pdf_url":"https://arxiv.org/pdf/2303.07847v1.pdf","comment":"5 pages, 4 figures, conference, to be published in UKSIM23"},{"id":"http://arxiv.org/abs/2303.07846v1","updated":"2023-03-14T12:36:01Z","published":"2023-03-14T12:36:01Z","title":"Sample-efficient Adversarial Imitation Learning","summary":"  Imitation learning, in which learning is performed by demonstration, has been\nstudied and advanced for sequential decision-making tasks in which a reward\nfunction is not predefined. However, imitation learning methods still require\nnumerous expert demonstration samples to successfully imitate an expert's\nbehavior. To improve sample efficiency, we utilize self-supervised\nrepresentation learning, which can generate vast training signals from the\ngiven data. In this study, we propose a self-supervised representation-based\nadversarial imitation learning method to learn state and action representations\nthat are robust to diverse distortions and temporally predictive, on non-image\ncontrol tasks. In particular, in comparison with existing self-supervised\nlearning methods for tabular data, we propose a different corruption method for\nstate and action representations that is robust to diverse distortions. We\ntheoretically and empirically observe that making an informative feature\nmanifold with less sample complexity significantly improves the performance of\nimitation learning. The proposed method shows a 39% relative improvement over\nexisting adversarial imitation learning methods on MuJoCo in a setting limited\nto 100 expert state-action pairs. Moreover, we conduct comprehensive ablations\nand additional experiments using demonstrations with varying optimality to\nprovide insights into a range of factors.\n","authors":["Dahuin Jung","Hyungyu Lee","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.07846v1.pdf","comment":"A preliminary version of this manuscript was presented at Deep RL\n  Workshop, NeurIPS 2022"},{"id":"http://arxiv.org/abs/2301.00656v2","updated":"2023-03-14T12:23:33Z","published":"2022-12-12T05:55:07Z","title":"TriNet: stabilizing self-supervised learning from complete or slow\n  collapse on ASR","summary":"  Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. TriNet learns the SSL latent embedding space and\nincorporates it to a higher level space for predicting pseudo target vectors\ngenerated by a frozen teacher. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 6.06% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.\n","authors":["Lixin Cao","Jun Wang","Ben Yang","Dan Su","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2301.00656v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.02314v2","updated":"2023-03-14T12:12:00Z","published":"2023-02-05T06:27:45Z","title":"CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image\n  Classification","summary":"  Most computer vision models are developed based on either convolutional\nneural network (CNN) or transformer, while the former (latter) method captures\nlocal (global) features. To relieve model performance limitations due to the\nlack of global (local) features, we develop a novel classification network CECT\nby controllable ensemble CNN and transformer. CECT is composed of a\nconvolutional encoder block, a transposed-convolutional decoder block, and a\ntransformer classification block. Different from conventional CNN- or\ntransformer-based methods, our CECT can capture features at both multi-local\nand global scales. Besides, the contribution of local features at different\nscales can be controlled with the proposed ensemble coefficients. We evaluate\nCECT on two public COVID-19 datasets and it outperforms existing\nstate-of-the-art methods on all evaluation metrics. With remarkable feature\ncapture ability, we believe CECT can be extended to other medical image\nclassification scenarios as a diagnosis assistant.\n","authors":["Zhaoshan Liu","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2302.02314v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.09100v2","updated":"2023-03-14T12:08:11Z","published":"2022-12-18T14:56:22Z","title":"SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input\n  Images","summary":"  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset will be made public with the code and\nmodels on the project website https://abdullahamdi.com/sparf/ .\n","authors":["Abdullah Hamdi","Bernard Ghanem","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2212.09100v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2109.09500v2","updated":"2023-03-14T11:57:10Z","published":"2021-09-20T12:53:01Z","title":"Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale\n  Confirmatory Item Factor Analysis","summary":"  We investigate novel parameter estimation and goodness-of-fit (GOF)\nassessment methods for large-scale confirmatory item factor analysis (IFA) with\nmany respondents, items, and latent factors. For parameter estimation, we\nextend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to\nthe confirmatory setting by showing how to handle constraints on loadings and\nfactor correlations. For GOF assessment, we explore simulation-based tests and\nindices that extend the classifier two-sample test (C2ST), a method that tests\nwhether a deep neural network can distinguish between observed data and\nsynthetic data sampled from a fitted IFA model. Proposed extensions include a\ntest of approximate fit wherein the user specifies what percentage of observed\nand synthetic data should be distinguishable as well as a relative fit index\n(RFI) that is similar in spirit to the RFIs used in structural equation\nmodeling. Via simulation studies, we show that: (1) the confirmatory extension\nof Urban and Bauer's (2021) algorithm obtains comparable estimates to a\nstate-of-the-art estimation procedure in less time; (2) C2ST-based GOF tests\ncontrol the empirical type I error rate and detect when the latent\ndimensionality is misspecified; and (3) the sampling distribution of the\nC2ST-based RFI depends on the sample size.\n","authors":["Christopher J. Urban","Daniel J. Bauer"],"pdf_url":"https://arxiv.org/pdf/2109.09500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07814v1","updated":"2023-03-14T11:44:58Z","published":"2023-03-14T11:44:58Z","title":"Kinematic Data-Based Action Segmentation for Surgical Applications","summary":"  Action segmentation is a challenging task in high-level process analysis,\ntypically performed on video or kinematic data obtained from various sensors.\nIn the context of surgical procedures, action segmentation is critical for\nworkflow analysis algorithms. This work presents two contributions related to\naction segmentation on kinematic data. Firstly, we introduce two multi-stage\narchitectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for\nkinematic data. The architectures consist of a prediction generator with\nintra-stage regularization and Bidirectional LSTM or GRU-based refinement\nstages. Secondly, we propose two new data augmentation techniques, World Frame\nRotation and Horizontal-Flip, which utilize the strong geometric structure of\nkinematic data to improve algorithm performance and robustness. We evaluate our\nmodels on three datasets of surgical suturing tasks: the Variable Tissue\nSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)\nDataset, both of which are open surgery simulation datasets collected by us, as\nwell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a\nwell-known benchmark in robotic surgery. Our methods achieve state-of-the-art\nperformance on all benchmark datasets and establish a strong baseline for the\nBRS dataset.\n","authors":["Adam Goldbraikh","Omer Shubi","Or Rubin","Carla M Pugh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2303.07814v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2110.01303v3","updated":"2023-03-14T11:40:35Z","published":"2021-10-04T10:19:53Z","title":"Incremental Class Learning using Variational Autoencoders with\n  Similarity Learning","summary":"  Catastrophic forgetting in neural networks during incremental learning\nremains a challenging problem. Previous research investigated catastrophic\nforgetting in fully connected networks, with some earlier work exploring\nactivation functions and learning algorithms. Applications of neural networks\nhave been extended to include similarity learning. Understanding how similarity\nlearning loss functions would be affected by catastrophic forgetting is of\nsignificant interest. Our research investigates catastrophic forgetting for\nfour well-known similarity-based loss functions during incremental class\nlearning. The loss functions are Angular, Contrastive, Center, and Triplet\nloss. Our results show that the catastrophic forgetting rate differs across\nloss functions on multiple datasets. The Angular loss was least affected,\nfollowed by Contrastive, Triplet loss, and Center loss with good mining\ntechniques. We implemented three existing incremental learning techniques,\niCaRL, EWC, and EBLL. We further proposed a novel technique using Variational\nAutoencoders (VAEs) to generate representation as exemplars passed through the\nnetwork's intermediate layers. Our method outperformed three existing\nstate-of-the-art techniques. We show that one does not require stored images\n(exemplars) for incremental learning with similarity learning. The generated\nrepresentations from VAEs help preserve regions of the embedding space used by\nprior knowledge so that new knowledge does not ``overwrite'' it.\n","authors":["Jiahao Huo","Terence L. van Zyl"],"pdf_url":"https://arxiv.org/pdf/2110.01303v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07811v1","updated":"2023-03-14T11:31:45Z","published":"2023-03-14T11:31:45Z","title":"ICICLE: Interpretable Class Incremental Continual Learning","summary":"  Continual learning enables incremental learning of new tasks without\nforgetting those previously learned, resulting in positive knowledge transfer\nthat can enhance performance on both new and old tasks. However, continual\nlearning poses new challenges for interpretability, as the rationale behind\nmodel predictions may change over time, leading to interpretability concept\ndrift. We address this problem by proposing Interpretable Class-InCremental\nLEarning (ICICLE), an exemplar-free approach that adopts a prototypical\npart-based approach. It consists of three crucial novelties: interpretability\nregularization that distills previously learned concepts while preserving\nuser-friendly positive reasoning; proximity-based prototype initialization\nstrategy dedicated to the fine-grained setting; and task-recency bias\ncompensation devoted to prototypical parts. Our experimental results\ndemonstrate that ICICLE reduces the interpretability concept drift and\noutperforms the existing exemplar-free methods of common class-incremental\nlearning when applied to concept-based models. We make the code available.\n","authors":["Dawid Rymarczyk","Joost van de Weijer","Bartosz Zieliński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2303.07811v1.pdf","comment":"Under review, code will be shared after the acceptance"},{"id":"http://arxiv.org/abs/2301.13060v3","updated":"2023-03-14T11:30:48Z","published":"2023-01-30T17:02:23Z","title":"Zero-One Laws of Graph Neural Networks","summary":"  Graph neural networks (GNNs) are de facto standard deep learning\narchitectures for machine learning on graphs. This has led to a large body of\nwork analyzing the capabilities and limitations of these models, particularly\npertaining to their representation and extrapolation capacity. We offer a novel\ntheoretical perspective on the representation and extrapolation capacity of\nGNNs, by answering the question: how do GNNs behave as the number of graph\nnodes become very large? Under mild assumptions, we show that when we draw\ngraphs of increasing size from the Erd\\H{o}s-R\\'enyi model, the probability\nthat such graphs are mapped to a particular output by a class of GNN\nclassifiers tends to either zero or to one. This class includes the popular\ngraph convolutional network architecture. The result establishes 'zero-one\nlaws' for these GNNs, and analogously to other convergence laws, entails\ntheoretical limitations on their capacity. We empirically verify our results,\nobserving that the theoretical asymptotic limits are evident already on\nrelatively small graphs.\n","authors":["Sam Adam-Day","Theodor Mihai Iliant","İsmail İlkan Ceylan"],"pdf_url":"https://arxiv.org/pdf/2301.13060v3.pdf","comment":"8 pages + references + 9 pages appendices, 2 figures"},{"id":"http://arxiv.org/abs/2301.00503v3","updated":"2023-03-14T11:01:26Z","published":"2023-01-02T02:10:18Z","title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay","summary":"  This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n","authors":["Yacheng He","Qianghuai Jia","Lin Yuan","Ruopeng Li","Yixin Ou","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00503v3.pdf","comment":"Accepted by WWW 2023 poster"},{"id":"http://arxiv.org/abs/2303.07778v1","updated":"2023-03-14T10:39:58Z","published":"2023-03-14T10:39:58Z","title":"GANN: Graph Alignment Neural Network for Semi-Supervised Learning","summary":"  Graph neural networks (GNNs) have been widely investigated in the field of\nsemi-supervised graph machine learning. Most methods fail to exploit adequate\ngraph information when labeled data is limited, leading to the problem of\noversmoothing. To overcome this issue, we propose the Graph Alignment Neural\nNetwork (GANN), a simple and effective graph neural architecture. A unique\nlearning algorithm with three alignment rules is proposed to thoroughly explore\nhidden information for insufficient labels. Firstly, to better investigate\nattribute specifics, we suggest the feature alignment rule to align the inner\nproduct of both the attribute and embedding matrices. Secondly, to properly\nutilize the higher-order neighbor information, we propose the cluster center\nalignment rule, which involves aligning the inner product of the cluster center\nmatrix with the unit matrix. Finally, to get reliable prediction results with\nfew labels, we establish the minimum entropy alignment rule by lining up the\nprediction probability matrix with its sharpened result. Extensive studies on\ngraph benchmark datasets demonstrate that GANN can achieve considerable\nbenefits in semi-supervised node classification and outperform state-of-the-art\ncompetitors.\n","authors":["Linxuan Song","Wenxuan Tu","Sihang Zhou","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.07778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05735v2","updated":"2023-03-14T10:38:14Z","published":"2023-03-10T06:44:49Z","title":"Hardware Acceleration of Neural Graphics","summary":"  Rendering and inverse-rendering algorithms that drive conventional computer\ngraphics have recently been superseded by neural representations (NR). NRs have\nrecently been used to learn the geometric and the material properties of the\nscenes and use the information to synthesize photorealistic imagery, thereby\npromising a replacement for traditional rendering algorithms with scalable\nquality and predictable performance. In this work we ask the question: Does\nneural graphics (NG) need hardware support? We studied representative NG\napplications showing that, if we want to render 4k res. at 60FPS there is a gap\nof 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,\nthere is an even larger gap of 2-4 OOM between the desired performance and the\nrequired system power. We identify that the input encoding and the MLP kernels\nare the performance bottlenecks, consuming 72%,60% and 59% of application time\nfor multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,\nrespectively. We propose a NG processing cluster, a scalable and flexible\nhardware architecture that directly accelerates the input encoding and MLP\nkernels through dedicated engines and supports a wide range of NG applications.\nWe also accelerate the rest of the kernels by fusing them together in Vulkan,\nwhich leads to 9.94X kernel-level performance improvement compared to un-fused\nimplementation of the pre-processing and the post-processing kernels. Our\nresults show that, NGPC gives up to 58X end-to-end application-level\nperformance improvement, for multi res. hashgrid encoding on average across the\nfour NG applications, the performance benefits are 12X,20X,33X and 39X for the\nscaling factor of 8,16,32 and 64, respectively. Our results show that with\nmulti res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS\nfor NeRF and 8k res. at 120FPS for all our other NG applications.\n","authors":["Muhammad Husnain Mubarik","Ramakrishna Kanungo","Tobias Zirr","Rakesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.05735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15567v2","updated":"2023-03-14T10:37:52Z","published":"2022-05-31T06:57:56Z","title":"Few-Shot Unlearning by Model Inversion","summary":"  We consider a practical scenario of machine unlearning to erase a target\ndataset, which causes unexpected behavior from the trained model. The target\ndataset is often assumed to be fully identifiable in a standard unlearning\nscenario. Such a flawless identification, however, is almost impossible if the\ntraining dataset is inaccessible at the time of unlearning. Unlike previous\napproaches requiring a complete set of targets, we consider few-shot unlearning\nscenario when only a few samples of target data are available. To this end, we\nformulate the few-shot unlearning problem specifying intentions behind the\nunlearning request (e.g., purely unlearning, mislabel correction, privacy\nprotection), and we devise a straightforward framework that (i) retrieves a\nproxy of the training data via model inversion fully exploiting information\navailable in the context of unlearning; (ii) adjusts the proxy according to the\nunlearning intention; and (iii) updates the model with the adjusted proxy. We\ndemonstrate that our method using only a subset of target data can outperform\nthe state-of-the-art unlearning methods even with a complete indication of\ntarget data.\n","authors":["Youngsik Yoon","Jinhwan Nam","Hyojeong Yun","Jaeho Lee","Dongwoo Kim","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2205.15567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01372v2","updated":"2023-03-14T10:34:55Z","published":"2023-03-02T15:58:09Z","title":"High-dimensional analysis of double descent for linear regression with\n  random projections","summary":"  We consider linear regression problems with a varying number of random\nprojections, where we provably exhibit a double descent curve for a fixed\nprediction problem, with a high-dimensional analysis based on random matrix\ntheory. We first consider the ridge regression estimator and review earlier\nresults using classical notions from non-parametric statistics, namely degrees\nof freedom, also known as effective dimensionality. We then compute asymptotic\nequivalents of the generalization performance (in terms of squared bias and\nvariance) of the minimum norm least-squares fit with random projections,\nproviding simple expressions for the double descent phenomenon.\n","authors":["Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2303.01372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02113v7","updated":"2023-03-14T10:33:15Z","published":"2022-02-04T12:52:32Z","title":"From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer","summary":"  Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n","authors":["Xin Xie","Ningyu Zhang","Zhoubo Li","Shumin Deng","Hui Chen","Feiyu Xiong","Mosha Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2202.02113v7.pdf","comment":"Accepted by WWW 2022 Poster"},{"id":"http://arxiv.org/abs/2110.03501v3","updated":"2023-03-14T10:30:51Z","published":"2021-10-07T14:37:06Z","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!","summary":"  Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npropose the generalizability of our pretrained language model from Anna\nKarenina Principle (AKP). We pretrain our model with different pairs of\nlanguage translations. Our results show language bias in solving symbolic\nmathematics tasks. Finally, we study the robustness of the fine-tuned model on\nsymbolic math tasks against distribution shift, and our approach generalizes\nbetter in distribution shift scenarios for the function integration.\n","authors":["Kimia Noorbakhsh","Modar Sulaiman","Mahdi Sharifi","Kallol Roy","Pooyan Jamshidi"],"pdf_url":"https://arxiv.org/pdf/2110.03501v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10852v5","updated":"2023-03-14T10:28:49Z","published":"2022-05-22T15:30:18Z","title":"Relphormer: Relational Graph Transformer for Knowledge Graph\n  Representations","summary":"  Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n","authors":["Zhen Bi","Siyuan Cheng","Jing Chen","Xiaozhuan Liang","Ningyu Zhang","Qiang Chen","Feiyu Xiong","Wei Guo","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2205.10852v5.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.07774v1","updated":"2023-03-14T10:25:56Z","published":"2023-03-14T10:25:56Z","title":"Testing Causality for High Dimensional Data","summary":"  Determining causal relationship between high dimensional observations are\namong the most important tasks in scientific discoveries. In this paper, we\nrevisited the \\emph{linear trace method}, a technique proposed\nin~\\citep{janzing2009telling,zscheischler2011testing} to infer the causal\ndirection between two random variables of high dimensions. We strengthen the\nexisting results significantly by providing an improved tail analysis in\naddition to extending the results to nonlinear trace functionals with sharper\nconfidence bounds under certain distributional assumptions. We obtain our\nresults by interpreting the trace estimator in the causal regime as a function\nover random orthogonal matrices, where the concentration of Lipschitz functions\nover such space could be applied. We additionally propose a novel\nridge-regularized variant of the estimator in \\cite{zscheischler2011testing},\nand give provable bounds relating the ridge-estimated terms to their\nground-truth counterparts. We support our theoretical results with encouraging\nexperiments on synthetic datasets, more prominently, under high-dimension low\nsample size regime.\n","authors":["Arun Jambulapati","Hilaf Hasson","Youngsuk Park","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01717v3","updated":"2023-03-14T10:21:56Z","published":"2022-11-03T11:13:02Z","title":"Learning Hypergraphs From Signals With Dual Smoothness Prior","summary":"  Hypergraph structure learning, which aims to learn the hypergraph structures\nfrom the observed signals to capture the intrinsic high-order relationships\namong the entities, becomes crucial when a hypergraph topology is not readily\navailable in the datasets. There are two challenges that lie at the heart of\nthis problem: 1) how to handle the huge search space of potential hyperedges,\nand 2) how to define meaningful criteria to measure the relationship between\nthe signals observed on nodes and the hypergraph structure. In this paper, for\nthe first challenge, we adopt the assumption that the ideal hypergraph\nstructure can be derived from a learnable graph structure that captures the\npairwise relations within signals. Further, we propose a hypergraph structure\nlearning framework HGSL with a novel dual smoothness prior that reveals a\nmapping between the observed node signals and the hypergraph structure, whereby\neach hyperedge corresponds to a subgraph with both node signal smoothness and\nedge signal smoothness in the learnable graph structure. Finally, we conduct\nextensive experiments to evaluate HGSL on both synthetic and real world\ndatasets. Experiments show that HGSL can efficiently infer meaningful\nhypergraph topologies from observed signals.\n","authors":["Bohan Tang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2211.01717v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07768v1","updated":"2023-03-14T10:18:31Z","published":"2023-03-14T10:18:31Z","title":"DBSCAN of Multi-Slice Clustering for three-order Tensor","summary":"  Several methods for triclustering three-dimensional data require the cluster\nsize or the number of clusters in each dimension to be specified. To address\nthis issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal\nslices that lie in a low dimensional subspace for a rank-one tensor dataset in\norder to find a cluster based on the threshold similarity. We propose an\nextension algorithm called MSC-DBSCAN to extract the different clusters of\nslices that lie in the different subspaces from the data if the dataset is a\nsum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC\nalgorithm and can find the same solution for rank-one tensor data as MSC.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.12220v2","updated":"2023-03-14T10:05:43Z","published":"2021-07-26T13:56:37Z","title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought","summary":"  When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n","authors":["Hendrik Schuff","Heike Adel","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2107.12220v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.07758v1","updated":"2023-03-14T10:03:37Z","published":"2023-03-14T10:03:37Z","title":"Traffic4cast at NeurIPS 2022 -- Predict Dynamics along Graph Edges from\n  Sparse Node Data: Whole City Traffic and ETA from Stationary Vehicle\n  Detectors","summary":"  The global trends of urbanization and increased personal mobility force us to\nrethink the way we live and use urban space. The Traffic4cast competition\nseries tackles this problem in a data-driven way, advancing the latest methods\nin machine learning for modeling complex spatial systems over time. In this\nedition, our dynamic road graph data combine information from road maps,\n$10^{12}$ probe data points, and stationary vehicle detectors in three cities\nover the span of two years. While stationary vehicle detectors are the most\naccurate way to capture traffic volume, they are only available in few\nlocations. Traffic4cast 2022 explores models that have the ability to\ngeneralize loosely related temporal vertex data on just a few nodes to predict\ndynamic future traffic states on the edges of the entire road graph. In the\ncore challenge, participants are invited to predict the likelihoods of three\ncongestion classes derived from the speed levels in the GPS data for the entire\nroad graph in three cities 15 min into the future. We only provide vehicle\ncount data from spatially sparse stationary vehicle detectors in these three\ncities as model input for this task. The data are aggregated in 15 min time\nbins for one hour prior to the prediction time. For the extended challenge,\nparticipants are tasked to predict the average travel times on super-segments\n15 min into the future - super-segments are longer sequences of road segments\nin the graph. The competition results provide an important advance in the\nprediction of complex city-wide traffic states just from publicly available\nsparse vehicle data and without the need for large amounts of real-time\nfloating vehicle data.\n","authors":["Moritz Neun","Christian Eichenberger","Henry Martin","Markus Spanring","Rahul Siripurapu","Daniel Springer","Leyan Deng","Chenwang Wu","Defu Lian","Min Zhou","Martin Lumiste","Andrei Ilie","Xinhua Wu","Cheng Lyu","Qing-Long Lu","Vishal Mahajan","Yichao Lu","Jiezhang Li","Junjun Li","Yue-Jiao Gong","Florian Grötschla","Joël Mathys","Ye Wei","He Haitao","Hui Fang","Kevin Malm","Fei Tang","Michael Kopp","David Kreil","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2303.07758v1.pdf","comment":"Pre-print under review, submitted to Proceedings of Machine Learning\n  Research"},{"id":"http://arxiv.org/abs/2303.07757v1","updated":"2023-03-14T10:02:52Z","published":"2023-03-14T10:02:52Z","title":"Multiway clustering of 3-order tensor via affinity matrix","summary":"  We propose a new method of multiway clustering for 3-order tensors via\naffinity matrix (MCAM). Based on a notion of similarity between the tensor\nslices and the spread of information of each slice, our model builds an\naffinity/similarity matrix on which we apply advanced clustering methods. The\ncombination of all clusters of the three modes delivers the desired multiway\nclustering. Finally, MCAM achieves competitive results compared with other\nknown algorithms on synthetics and real datasets.\n","authors":["Dina Faneva Andriantsiory","Joseph Ben Geloun","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2303.07757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07742v1","updated":"2023-03-14T09:40:37Z","published":"2023-03-14T09:40:37Z","title":"ForDigitStress: A multi-modal stress dataset employing a digital job\n  interview scenario","summary":"  We present a multi-modal stress dataset that uses digital job interviews to\ninduce stress. The dataset provides multi-modal data of 40 participants\nincluding audio, video (motion capturing, facial recognition, eye tracking) as\nwell as physiological information (photoplethysmography, electrodermal\nactivity). In addition to that, the dataset contains time-continuous\nannotations for stress and occurred emotions (e.g. shame, anger, anxiety,\nsurprise). In order to establish a baseline, five different machine learning\nclassifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest,\nLong-Short-Term Memory Network) have been trained and evaluated on the proposed\ndataset for a binary stress classification task. The best-performing classifier\nachieved an accuracy of 88.3% and an F1-score of 87.5%.\n","authors":["Alexander Heimerl","Pooja Prajod","Silvan Mertes","Tobias Baur","Matthias Kraus","Ailin Liu","Helen Risack","Nicolas Rohleder","Elisabeth André","Linda Becker"],"pdf_url":"https://arxiv.org/pdf/2303.07742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07735v1","updated":"2023-03-14T09:30:52Z","published":"2023-03-14T09:30:52Z","title":"Can neural networks do arithmetic? A survey on the elementary numerical\n  skills of state-of-the-art deep learning models","summary":"  Creating learning models that can exhibit sophisticated reasoning skills is\none of the greatest challenges in deep learning research, and mathematics is\nrapidly becoming one of the target domains for assessing scientific progress in\nthis direction. In the past few years there has been an explosion of neural\nnetwork architectures, data sets, and benchmarks specifically designed to\ntackle mathematical problems, reporting notable success in disparate fields\nsuch as automated theorem proving, numerical integration, and discovery of new\nconjectures or matrix multiplication algorithms. However, despite these\nimpressive achievements it is still unclear whether deep learning models\npossess an elementary understanding of quantities and symbolic numbers. In this\nsurvey we critically examine the recent literature, concluding that even\nstate-of-the-art architectures often fall short when probed with relatively\nsimple tasks designed to test basic numerical and arithmetic knowledge.\n","authors":["Alberto Testolin"],"pdf_url":"https://arxiv.org/pdf/2303.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04488v2","updated":"2023-03-14T09:23:25Z","published":"2023-01-11T14:33:42Z","title":"WuYun: Exploring hierarchical skeleton-guided melody generation using\n  knowledge-enhanced deep learning","summary":"  Although deep learning has revolutionized music generation, existing methods\nfor structured melody generation follow an end-to-end left-to-right\nnote-by-note generative paradigm and treat each note equally. Here, we present\nWuYun, a knowledge-enhanced deep learning architecture for improving the\nstructure of generated melodies, which first generates the most structurally\nimportant notes to construct a melodic skeleton and subsequently infills it\nwith dynamically decorative notes into a full-fledged melody. Specifically, we\nuse music domain knowledge to extract melodic skeletons and employ sequence\nlearning to reconstruct them, which serve as additional knowledge to provide\nauxiliary guidance for the melody generation process. We demonstrate that WuYun\ncan generate melodies with better long-term structure and musicality and\noutperforms other state-of-the-art methods by 0.51 on average on all subjective\nevaluation metrics. Our study provides a multidisciplinary lens to design\nmelodic hierarchical structures and bridge the gap between data-driven and\nknowledge-based approaches for numerous music generation tasks.\n","authors":["Kejun Zhang","Xinda Wu","Tieyao Zhang","Zhijie Huang","Xu Tan","Qihao Liang","Songruoyao Wu","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2301.04488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07726v1","updated":"2023-03-14T09:15:51Z","published":"2023-03-14T09:15:51Z","title":"Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme\n  Conversion","summary":"  Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework\nthat first transforms input sequences into character embeddings, obtains\nlinguistic information using language models, and then predicts the phonemes\nbased on global context about the entire input sequence. However, linguistic\nknowledge alone is often inadequate. Language models frequently encode overly\ngeneral structures of a sentence and fail to cover specific cases needed to use\nphonetic knowledge. Also, a handcrafted post-processing system is needed to\naddress the problems relevant to the tone of the characters. However, the\nsystem exhibits inconsistency in the segmentation of word boundaries which\nconsequently degrades the performance of the G2P system. To address these\nissues, we propose the Reinforcer that provides strong inductive bias for\nlanguage models by emphasizing the phonological information between neighboring\ncharacters to help disambiguate pronunciations. Experimental results show that\nthe Reinforcer boosts the cutting-edge architectures by a large margin. We also\ncombine the Reinforcer with a large-scale pre-trained model and demonstrate the\nvalidity of using neighboring context in knowledge transfer scenarios.\n","authors":["Jungjun Kim","Changjin Han","Gyuhyeon Nam","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07726v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.04660v2","updated":"2023-03-14T09:12:11Z","published":"2023-03-08T15:27:29Z","title":"Neural Probabilistic Logic Programming in Discrete-Continuous Domains","summary":"  Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic\nbackground knowledge in the form of logic. It has been shown to aid learning in\nthe limited data regime and to facilitate inference on out-of-distribution\ndata. Probabilistic NeSy focuses on integrating neural networks with both logic\nand probability theory, which additionally allows learning under uncertainty. A\nmajor limitation of current probabilistic NeSy systems, such as DeepProbLog, is\ntheir restriction to finite probability distributions, i.e., discrete random\nvariables. In contrast, deep probabilistic programming (DPP) excels in\nmodelling and optimising continuous probability distributions. Hence, we\nintroduce DeepSeaProbLog, a neural probabilistic logic programming language\nthat incorporates DPP techniques into NeSy. Doing so results in the support of\ninference and learning of both discrete and continuous probability\ndistributions under logical constraints. Our main contributions are 1) the\nsemantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a\nproven asymptotically unbiased learning algorithm, and 3) a series of\nexperiments that illustrate the versatility of our approach.\n","authors":["Lennert De Smet","Pedro Zuidberg Dos Martires","Robin Manhaeve","Giuseppe Marra","Angelika Kimmig","Luc De Raedt"],"pdf_url":"https://arxiv.org/pdf/2303.04660v2.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2302.02601v2","updated":"2023-03-14T09:12:00Z","published":"2023-02-06T07:45:57Z","title":"Learning Representations of Bi-level Knowledge Graphs for Reasoning\n  beyond Link Prediction","summary":"  Knowledge graphs represent known facts using triplets. While existing\nknowledge graph embedding methods only consider the connections between\nentities, we propose considering the relationships between triplets. For\nexample, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is\n(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,\nAcademy_Awards). Given these two base-level triplets, we see that $T_1$ is a\nprerequisite for $T_2$. In this paper, we define a higher-level triplet to\nrepresent a relationship between triplets, e.g., $\\langle T_1$,\nPrerequisiteFor, $T_2\\rangle$ where PrerequisiteFor is a higher-level relation.\nWe define a bi-level knowledge graph that consists of the base-level and the\nhigher-level triplets. We also propose a data augmentation strategy based on\nthe random walks on the bi-level knowledge graph to augment plausible triplets.\nOur model called BiVE learns embeddings by taking into account the structures\nof the base-level and the higher-level triplets, with additional consideration\nof the augmented triplets. We propose two new tasks: triplet prediction and\nconditional link prediction. Given a triplet $T_1$ and a higher-level relation,\nthe triplet prediction predicts a triplet that is likely to be connected to\n$T_1$ by the higher-level relation, e.g., $\\langle T_1$, PrerequisiteFor,\n?$\\rangle$. The conditional link prediction predicts a missing entity in a\ntriplet conditioned on another triplet, e.g., $\\langle T_1$, PrerequisiteFor,\n(Avatar, Wins, ?)$\\rangle$. Experimental results show that BiVE significantly\noutperforms all other methods in the two new tasks and the typical base-level\nlink prediction in real-world bi-level knowledge graphs.\n","authors":["Chanyoung Chung","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2302.02601v2.pdf","comment":"14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial\n  Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2207.07921v2","updated":"2023-03-14T09:10:08Z","published":"2022-07-16T12:11:28Z","title":"CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image\n  Prior","summary":"  Euler's elastica constitute an appealing variational image inpainting model.\nIt minimises an energy that involves the total variation as well as the level\nline curvature. These components are transparent and make it attractive for\nshape completion tasks. However, its gradient flow is a singular, anisotropic,\nand nonlinear PDE of fourth order, which is numerically challenging: It is\ndifficult to find efficient algorithms that offer sharp edges and good rotation\ninvariance. As a remedy, we design the first neural algorithm that simulates\ninpainting with Euler's Elastica. We use the deep energy concept which employs\nthe variational energy as neural network loss. Furthermore, we pair it with a\ndeep image prior where the network architecture itself acts as a prior. This\nyields better inpaintings by steering the optimisation trajectory closer to the\ndesired solution. Our results are qualitatively on par with state-of-the-art\nalgorithms on elastica-based shape completion. They combine good rotation\ninvariance with sharp edges. Moreover, we benefit from the high efficiency and\neffortless parallelisation within a neural framework. Our neural elastica\napproach only requires 3x3 central difference stencils. It is thus much simpler\nthan other well-performing algorithms for elastica inpainting. Last but not\nleast, it is unsupervised as it requires no ground truth training data.\n","authors":["Karl Schrader","Tobias Alt","Joachim Weickert","Michael Ertel"],"pdf_url":"https://arxiv.org/pdf/2207.07921v2.pdf","comment":"In Proceedings of the 10th European Workshop on Visual Information\n  Processing, Lisbon, 2022"},{"id":"http://arxiv.org/abs/2303.06836v2","updated":"2023-03-14T09:06:45Z","published":"2023-03-13T03:46:37Z","title":"Label Information Bottleneck for Label Enhancement","summary":"  In this work, we focus on the challenging problem of Label Enhancement (LE),\nwhich aims to exactly recover label distributions from logical labels, and\npresent a novel Label Information Bottleneck (LIB) method for LE. For the\nrecovery process of label distributions, the label irrelevant information\ncontained in the dataset may lead to unsatisfactory recovery performance. To\naddress this limitation, we make efforts to excavate the essential label\nrelevant information to improve the recovery performance. Our method formulates\nthe LE problem as the following two joint processes: 1) learning the\nrepresentation with the essential label relevant information, 2) recovering\nlabel distributions based on the learned representation. The label relevant\ninformation can be excavated based on the \"bottleneck\" formed by the learned\nrepresentation. Significantly, both the label relevant information about the\nlabel assignments and the label relevant information about the label gaps can\nbe explored in our method. Evaluation experiments conducted on several\nbenchmark label distribution learning datasets verify the effectiveness and\ncompetitiveness of LIB. Our source codes are available\nhttps://github.com/qinghai-zheng/LIBLE\n","authors":["Qinghai Zheng","Jihua Zhu","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2303.06836v2.pdf","comment":"Accepted by CVPR 2023, our source codes are available at\n  https://github.com/qinghai-zheng/LIBLE"},{"id":"http://arxiv.org/abs/2303.00515v4","updated":"2023-03-14T09:01:28Z","published":"2023-02-28T04:37:26Z","title":"Interpretable Water Level Forecaster with Spatiotemporal Causal\n  Attention Mechanisms","summary":"  Forecasting the water level of the Han river is important to control traffic\nand avoid natural disasters. There are many variables related to the Han river\nand they are intricately connected. In this work, we propose a novel\ntransformer that exploits the causal relationship based on the prior knowledge\namong the variables and forecasts the water level at the Jamsu bridge in the\nHan river. Our proposed model considers both spatial and temporal causation by\nformalizing the causal structure as a multilayer network and using masking\nmethods. Due to this approach, we can have interpretability that consistent\nwith prior knowledge. In real data analysis, we use the Han river dataset from\n2016 to 2021 and compare the proposed model with deep learning models.\n","authors":["Sunghcul Hong","Yunjin Choi","Jong-June Jeon"],"pdf_url":"https://arxiv.org/pdf/2303.00515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14125v3","updated":"2023-03-14T08:59:16Z","published":"2022-08-30T10:21:40Z","title":"A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images","summary":"  Diffusion models are a special type of generative model, capable of\nsynthesising new data from a learnt distribution. We introduce DISPR, a\ndiffusion-based model for solving the inverse problem of three-dimensional (3D)\ncell shape prediction from two-dimensional (2D) single cell microscopy images.\nUsing the 2D microscopy image as a prior, DISPR is conditioned to predict\nrealistic 3D shape reconstructions. To showcase the applicability of DISPR as a\ndata augmentation tool in a feature-based single cell classification task, we\nextract morphological features from the red blood cells grouped into six highly\nimbalanced classes. Adding features from the DISPR predictions to the three\nminority classes improved the macro F1 score from $F1_\\text{macro} = 55.2 \\pm\n4.6\\%$ to $F1_\\text{macro} = 72.2 \\pm 4.9\\%$. We thus demonstrate that\ndiffusion models can be successfully applied to inverse biomedical problems,\nand that they learn to reconstruct 3D shapes with realistic morphological\nfeatures from 2D microscopy images.\n","authors":["Dominik J. E. Waibel","Ernst Röell","Bastian Rieck","Raja Giryes","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2208.14125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07125v2","updated":"2023-03-14T08:29:49Z","published":"2023-03-13T13:56:20Z","title":"Don't PANIC: Prototypical Additive Neural Network for Interpretable\n  Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) has a complex and multifactorial etiology, which\nrequires integrating information about neuroanatomy, genetics, and\ncerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep\nlearning approaches combined image and tabular information to improve\ndiagnostic performance. However, the black-box nature of such neural networks\nis still a barrier for clinical applications, in which understanding the\ndecision of a heterogeneous model is integral. We propose PANIC, a prototypical\nadditive neural network for interpretable AD classification that integrates 3D\nimage and tabular data. It is interpretable by design and, thus, avoids the\nneed for post-hoc explanations that try to approximate the decision of a\nnetwork. Our results demonstrate that PANIC achieves state-of-the-art\nperformance in AD classification, while directly providing local and global\nexplanations. Finally, we show that PANIC extracts biologically meaningful\nsignatures of AD, and satisfies a set of desirable desiderata for trustworthy\nmachine learning. Our implementation is available at\nhttps://github.com/ai-med/PANIC .\n","authors":["Tom Nuno Wolf","Sebastian Pölsterl","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2303.07125v2.pdf","comment":"To be published in proceedings of Information Processing In Medical\n  Imaging 2023"},{"id":"http://arxiv.org/abs/2303.07697v1","updated":"2023-03-14T08:22:18Z","published":"2023-03-14T08:22:18Z","title":"DisCoHead: Audio-and-Video-Driven Talking Head Generation by\n  Disentangled Control of Head Pose and Facial Expressions","summary":"  For realistic talking head generation, creating natural head motion while\nmaintaining accurate lip synchronization is essential. To fulfill this\nchallenging task, we propose DisCoHead, a novel method to disentangle and\ncontrol head pose and facial expressions without supervision. DisCoHead uses a\nsingle geometric transformation as a bottleneck to isolate and extract head\nmotion from a head-driving video. Either an affine or a thin-plate spline\ntransformation can be used and both work well as geometric bottlenecks. We\nenhance the efficiency of DisCoHead by integrating a dense motion estimator and\nthe encoder of a generator which are originally separate modules. Taking a step\nfurther, we also propose a neural mix approach where dense motion is estimated\nand applied implicitly by the encoder. After applying the disentangled head\nmotion to a source identity, DisCoHead controls the mouth region according to\nspeech audio, and it blinks eyes and moves eyebrows following a separate\ndriving video of the eye region, via the weight modulation of convolutional\nneural networks. The experiments using multiple datasets show that DisCoHead\nsuccessfully generates realistic audio-and-video-driven talking heads and\noutperforms state-of-the-art methods. Project page:\nhttps://deepbrainai-research.github.io/discohead/\n","authors":["Geumbyeol Hwang","Sunwon Hong","Seunghyun Lee","Sungwoo Park","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2303.07697v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07200v2","updated":"2023-03-14T08:17:19Z","published":"2023-03-10T17:09:55Z","title":"Supervised Feature Selection with Neuron Evolution in Sparse Neural\n  Networks","summary":"  Feature selection that selects an informative subset of variables from data\nnot only enhances the model interpretability and performance but also\nalleviates the resource demands. Recently, there has been growing attention on\nfeature selection using neural networks. However, existing methods usually\nsuffer from high computational costs when applied to high-dimensional datasets.\nIn this paper, inspired by evolution processes, we propose a novel\nresource-efficient supervised feature selection method using sparse neural\nnetworks, named \\enquote{NeuroFS}. By gradually pruning the uninformative\nfeatures from the input layer of a sparse neural network trained from scratch,\nNeuroFS derives an informative subset of features efficiently. By performing\nseveral experiments on $11$ low and high-dimensional real-world benchmarks of\ndifferent types, we demonstrate that NeuroFS achieves the highest ranking-based\nscore among the considered state-of-the-art supervised feature selection\nmodels. The code is available on GitHub.\n","authors":["Zahra Atashgahi","Xuhao Zhang","Neil Kichler","Shiwei Liu","Lu Yin","Mykola Pechenizkiy","Raymond Veldhuis","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2303.07200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05660v2","updated":"2023-03-14T08:15:35Z","published":"2023-03-10T02:22:33Z","title":"Towards better traffic volume estimation: Tackling both underdetermined\n  and non-equilibrium problems via a correlation-adaptive graph convolution\n  network","summary":"  Traffic volume is an indispensable ingredient to provide fine-grained\ninformation for traffic management and control. However, due to limited\ndeployment of traffic sensors, obtaining full-scale volume information is far\nfrom easy. Existing works on this topic primarily focus on improving the\noverall estimation accuracy of a particular method and ignore the underlying\nchallenges of volume estimation, thereby having inferior performances on some\ncritical tasks. This paper studies two key problems with regard to traffic\nvolume estimation: (1) underdetermined traffic flows caused by undetected\nmovements, and (2) non-equilibrium traffic flows arise from congestion\npropagation. Here we demonstrate a graph-based deep learning method that can\noffer a data-driven, model-free and correlation adaptive approach to tackle the\nabove issues and perform accurate network-wide traffic volume estimation.\nParticularly, in order to quantify the dynamic and nonlinear relationships\nbetween traffic speed and volume for the estimation of underdetermined flows, a\nspeed patternadaptive adjacent matrix based on graph attention is developed and\nintegrated into the graph convolution process, to capture non-local\ncorrelations between sensors. To measure the impacts of non-equilibrium flows,\na temporal masked and clipped attention combined with a gated temporal\nconvolution layer is customized to capture time-asynchronous correlations\nbetween upstream and downstream sensors. We then evaluate our model on a\nreal-world highway traffic volume dataset and compare it with several benchmark\nmodels. It is demonstrated that the proposed model achieves high estimation\naccuracy even under 20% sensor coverage rate and outperforms other baselines\nsignificantly, especially on underdetermined and non-equilibrium flow\nlocations. Furthermore, comprehensive quantitative model analysis are also\ncarried out to justify the model designs.\n","authors":["Tong Nie","Guoyang Qin","Yunpeng Wang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2303.05660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07693v1","updated":"2023-03-14T08:13:21Z","published":"2023-03-14T08:13:21Z","title":"Adaptive Policy Learning for Offline-to-Online Reinforcement Learning","summary":"  Conventional reinforcement learning (RL) needs an environment to collect\nfresh data, which is impractical when online interactions are costly. Offline\nRL provides an alternative solution by directly learning from the previously\ncollected dataset. However, it will yield unsatisfactory performance if the\nquality of the offline datasets is poor. In this paper, we consider an\noffline-to-online setting where the agent is first learned from the offline\ndataset and then trained online, and propose a framework called Adaptive Policy\nLearning for effectively taking advantage of offline and online data.\nSpecifically, we explicitly consider the difference between the online and\noffline data and apply an adaptive update scheme accordingly, that is, a\npessimistic update strategy for the offline dataset and an optimistic/greedy\nupdate scheme for the online dataset. Such a simple and effective method\nprovides a way to mix the offline and online RL and achieve the best of both\nworlds. We further provide two detailed algorithms for implementing the\nframework through embedding value or policy-based RL algorithms into it.\nFinally, we conduct extensive experiments on popular continuous control tasks,\nand results show that our algorithm can learn the expert policy with high\nsample efficiency even when the quality of offline dataset is poor, e.g.,\nrandom dataset.\n","authors":["Han Zheng","Xufang Luo","Pengfei Wei","Xuan Song","Dongsheng Li","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.07693v1.pdf","comment":"AAAI2023"},{"id":"http://arxiv.org/abs/2108.07472v6","updated":"2023-03-14T08:08:20Z","published":"2021-08-17T07:06:46Z","title":"Is Nash Equilibrium Approximator Learnable?","summary":"  In this paper, we investigate the learnability of the function approximator\nthat approximates Nash equilibrium (NE) for games generated from a\ndistribution. First, we offer a generalization bound using the Probably\nApproximately Correct (PAC) learning model. The bound describes the gap between\nthe expected loss and empirical loss of the NE approximator. Afterward, we\nprove the agnostic PAC learnability of the Nash approximator. In addition to\ntheoretical analysis, we demonstrate an application of NE approximator in\nexperiments. The trained NE approximator can be used to warm-start and\naccelerate classical NE solvers. Together, our results show the practicability\nof approximating NE through function approximation.\n","authors":["Zhijian Duan","Wenhan Huang","Dinghuai Zhang","Yali Du","Jun Wang","Yaodong Yang","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2108.07472v6.pdf","comment":"Accepted by AAMAS 2023"},{"id":"http://arxiv.org/abs/2303.07685v1","updated":"2023-03-14T07:55:50Z","published":"2023-03-14T07:55:50Z","title":"FPTN: Fast Pure Transformer Network for Traffic Flow Forecasting","summary":"  Traffic flow forecasting is challenging due to the intricate spatio-temporal\ncorrelations in traffic flow data. Existing Transformer-based methods usually\ntreat traffic flow forecasting as multivariate time series (MTS) forecasting.\nHowever, too many sensors can cause a vector with a dimension greater than 800,\nwhich is difficult to process without information loss. In addition, these\nmethods design complex mechanisms to capture spatial dependencies in MTS,\nresulting in slow forecasting speed. To solve the abovementioned problems, we\npropose a Fast Pure Transformer Network (FPTN) in this paper. First, the\ntraffic flow data are divided into sequences along the sensor dimension instead\nof the time dimension. Then, to adequately represent complex spatio-temporal\ncorrelations, Three types of embeddings are proposed for projecting these\nvectors into a suitable vector space. After that, to capture the complex\nspatio-temporal correlations simultaneously in these vectors, we utilize\nTransformer encoder and stack it with several layers. Extensive experiments are\nconducted with 4 real-world datasets and 13 baselines, which demonstrate that\nFPTN outperforms the state-of-the-art on two metrics. Meanwhile, the\ncomputational time of FPTN spent is less than a quarter of other\nstate-of-the-art Transformer-based models spent, and the requirements for\ncomputing resources are significantly reduced.\n","authors":["Junhao Zhang","Junjie Tang","Juncheng Jin","Zehui Qu"],"pdf_url":"https://arxiv.org/pdf/2303.07685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07679v1","updated":"2023-03-14T07:42:02Z","published":"2023-03-14T07:42:02Z","title":"Feature representations useful for predicting image memorability","summary":"  Predicting image memorability has attracted interest in various fields.\nConsequently, prediction accuracy with convolutional neural network (CNN)\nmodels has been approaching the empirical upper bound estimated based on human\nconsistency. However, identifying which feature representations embedded in CNN\nmodels are responsible for such high prediction accuracy of memorability\nremains an open question. To tackle this problem, this study sought to identify\nmemorability-related feature representations in CNN models using brain\nsimilarity. Specifically, memorability prediction accuracy and brain similarity\nwere examined and assessed by Brain-Score across 16,860 layers in 64 CNN models\npretrained for object recognition. A clear tendency was shown in this\ncomprehensive analysis that layers with high memorability prediction accuracy\nhad higher brain similarity with the inferior temporal (IT) cortex, which is\nthe highest stage in the ventral visual pathway. Furthermore, fine-tuning the\n64 CNN models revealed that brain similarity with the IT cortex at the\npenultimate layer was positively correlated with memorability prediction\naccuracy. This analysis also showed that the best fine-tuned model provided\naccuracy comparable to the state-of-the-art CNN models developed specifically\nfor memorability prediction. Overall, this study's results indicated that the\nCNN models' great success in predicting memorability relies on feature\nrepresentation acquisition similar to the IT cortex. This study advanced our\nunderstanding of feature representations and its use for predicting image\nmemorability.\n","authors":["Takumi Harada","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2303.07679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00290v2","updated":"2023-03-14T07:30:03Z","published":"2022-12-01T05:31:07Z","title":"Component Segmentation of Engineering Drawings Using Graph Convolutional\n  Networks","summary":"  We present a data-driven framework to automate the vectorization and machine\ninterpretation of 2D engineering part drawings. In industrial settings, most\nmanufacturing engineers still rely on manual reads to identify the topological\nand manufacturing requirements from drawings submitted by designers. The\ninterpretation process is laborious and time-consuming, which severely inhibits\nthe efficiency of part quotation and manufacturing tasks. While recent advances\nin image-based computer vision methods have demonstrated great potential in\ninterpreting natural images through semantic segmentation approaches, the\napplication of such methods in parsing engineering technical drawings into\nsemantically accurate components remains a significant challenge. The severe\npixel sparsity in engineering drawings also restricts the effective\nfeaturization of image-based data-driven methods. To overcome these challenges,\nwe propose a deep learning based framework that predicts the semantic type of\neach vectorized component. Taking a raster image as input, we vectorize all\ncomponents through thinning, stroke tracing, and cubic bezier fitting. Then a\ngraph of such components is generated based on the connectivity between the\ncomponents. Finally, a graph convolutional neural network is trained on this\ngraph data to identify the semantic type of each component. We test our\nframework in the context of semantic segmentation of text, dimension and,\ncontour components in engineering drawings. Results show that our method yields\nthe best performance compared to recent image, and graph-based segmentation\nmethods.\n","authors":["Wentai Zhang","Joe Joseph","Yue Yin","Liuyue Xie","Tomotake Furuhata","Soji Yamakawa","Kenji Shimada","Levent Burak Kara"],"pdf_url":"https://arxiv.org/pdf/2212.00290v2.pdf","comment":"Preprint accepted to Computers in Industry"},{"id":"http://arxiv.org/abs/2210.10691v2","updated":"2023-03-14T07:27:43Z","published":"2022-10-19T16:06:12Z","title":"Provably Safe Reinforcement Learning via Action Projection using\n  Reachability Analysis and Polynomial Zonotopes","summary":"  While reinforcement learning produces very promising results for many\napplications, its main disadvantage is the lack of safety guarantees, which\nprevents its use in safety-critical systems. In this work, we address this\nissue by a safety shield for nonlinear continuous systems that solve\nreach-avoid tasks. Our safety shield prevents applying potentially unsafe\nactions from a reinforcement learning agent by projecting the proposed action\nto the closest safe action. This approach is called action projection and is\nimplemented via mixed-integer optimization. The safety constraints for action\nprojection are obtained by applying parameterized reachability analysis using\npolynomial zonotopes, which enables to accurately capture the nonlinear effects\nof the actions on the system. In contrast to other state-of-the-art approaches\nfor action projection, our safety shield can efficiently handle input\nconstraints and dynamic obstacles, eases incorporation of the spatial robot\ndimensions into the safety constraints, guarantees robust safety despite\nprocess noise and measurement errors, and is well suited for high-dimensional\nsystems, as we demonstrate on several challenging benchmark systems.\n","authors":["Niklas Kochdumper","Hanna Krasowski","Xiao Wang","Stanley Bak","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2210.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07675v1","updated":"2023-03-14T07:25:44Z","published":"2023-03-14T07:25:44Z","title":"Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems\n  Using Optimal Transport","summary":"  Predicting how distributions over discrete variables vary over time is a\ncommon task in time series forecasting. But whereas most approaches focus on\nmerely predicting the distribution at subsequent time steps, a crucial piece of\ninformation in many settings is to determine how this probability mass flows\nbetween the different elements over time. We propose a new approach to\npredicting such mass flow over time using optimal transport. Specifically, we\npropose a generic approach to predicting transport matrices in end-to-end deep\nlearning systems, replacing the standard softmax operation with Sinkhorn\niterations. We apply our approach to the task of predicting how communities\nwill evolve over time in social network settings, and show that the approach\nimproves substantially over alternative prediction methods. We specifically\nhighlight results on the task of predicting faction evolution in Ukrainian\nparliamentary voting.\n","authors":["Mukul Bhutani","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2303.07675v1.pdf","comment":"A prior version of the work appeared in the Optimal Transport\n  Workshop at NeurIPS 2019"},{"id":"http://arxiv.org/abs/2303.07674v1","updated":"2023-03-14T07:25:38Z","published":"2023-03-14T07:25:38Z","title":"Koos Classification of Vestibular Schwannoma via Image Translation-Based\n  Unsupervised Cross-Modality Domain Adaptation","summary":"  The Koos grading scale is a classification system for vestibular schwannoma\n(VS) used to characterize the tumor and its effects on adjacent brain\nstructures. The Koos classification captures many of the characteristics of\ntreatment deci-sions and is often used to determine treatment plans. Although\nboth contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2)\nscanning can be used for Koos Classification, hrT2 scanning is gaining interest\nbecause of its higher safety and cost-effectiveness. However, in the absence of\nannotations for hrT2 scans, deep learning methods often inevitably suffer from\nperformance deg-radation due to unsupervised learning. If ceT1 scans and their\nannotations can be used for unsupervised learning of hrT2 scans, the\nperformance of Koos classifi-cation using unlabeled hrT2 scans will be greatly\nimproved. In this regard, we propose an unsupervised cross-modality domain\nadaptation method based on im-age translation by transforming annotated ceT1\nscans into hrT2 modality and us-ing their annotations to achieve supervised\nlearning of hrT2 modality. Then, the VS and 7 adjacent brain structures related\nto Koos classification in hrT2 scans were segmented. Finally, handcrafted\nfeatures are extracted from the segmenta-tion results, and Koos grade is\nclassified using a random forest classifier. The proposed method received rank\n1 on the Koos classification task of the Cross-Modality Domain Adaptation\n(crossMoDA 2022) challenge, with Macro-Averaged Mean Absolute Error (MA-MAE) of\n0.2148 for the validation set and 0.26 for the test set.\n","authors":["Tao Yang","Lisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07674v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07669v1","updated":"2023-03-14T07:23:16Z","published":"2023-03-14T07:23:16Z","title":"AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph\n  Neural Networks","summary":"  AutoML has demonstrated remarkable success in finding an effective neural\narchitecture for a given machine learning task defined by a specific dataset\nand an evaluation metric. However, most present AutoML techniques consider each\ntask independently from scratch, which requires exploring many architectures,\nleading to high computational cost. Here we propose AutoTransfer, an AutoML\nsolution that improves search efficiency by transferring the prior\narchitectural design knowledge to the novel task of interest. Our key\ninnovation includes a task-model bank that captures the model performance over\na diverse set of GNN architectures and tasks, and a computationally efficient\ntask embedding that can accurately measure the similarity among different\ntasks. Based on the task-model bank and the task embeddings, we estimate the\ndesign priors of desirable models of the novel task, by aggregating a\nsimilarity-weighted sum of the top-K design distributions on tasks that are\nsimilar to the task of interest. The computed design priors can be used with\nany AutoML search algorithm. We evaluate AutoTransfer on six datasets in the\ngraph machine learning domain. Experiments demonstrate that (i) our proposed\ntask embedding can be computed efficiently, and that tasks with similar\nembeddings have similar best-performing architectures; (ii) AutoTransfer\nsignificantly improves search efficiency with the transferred design priors,\nreducing the number of explored architectures by an order of magnitude.\nFinally, we release GNN-Bank-101, a large-scale dataset of detailed GNN\ntraining information of 120,000 task-model combinations to facilitate and\ninspire future research.\n","authors":["Kaidi Cao","Jiaxuan You","Jiaju Liu","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2303.07669v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2206.07883v3","updated":"2023-03-14T07:16:43Z","published":"2022-06-16T02:19:37Z","title":"Combinatorial Pure Exploration of Causal Bandits","summary":"  The combinatorial pure exploration of causal bandits is the following online\nlearning task: given a causal graph with unknown causal inference\ndistributions, in each round we choose a subset of variables to intervene or do\nno intervention, and observe the random outcomes of all random variables, with\nthe goal that using as few rounds as possible, we can output an intervention\nthat gives the best (or almost best) expected outcome on the reward variable\n$Y$ with probability at least $1-\\delta$, where $\\delta$ is a given confidence\nlevel. We provide the first gap-dependent and fully adaptive pure exploration\nalgorithms on two types of causal models -- the binary generalized linear model\n(BGLM) and general graphs. For BGLM, our algorithm is the first to be designed\nspecifically for this setting and achieves polynomial sample complexity, while\nall existing algorithms for general graphs have either sample complexity\nexponential to the graph size or some unreasonable assumptions. For general\ngraphs, our algorithm provides a significant improvement on sample complexity,\nand it nearly matches the lower bound we prove. Our algorithms achieve such\nimprovement by a novel integration of prior causal bandit algorithms and prior\nadaptive pure exploration algorithms, the former of which utilize the rich\nobservational feedback in causal bandits but are not adaptive to reward gaps,\nwhile the latter of which have the issue in reverse.\n","authors":["Nuoya Xiong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2206.07883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07666v1","updated":"2023-03-14T07:15:41Z","published":"2023-03-14T07:15:41Z","title":"Relational Multi-Task Learning: Modeling Relations between Data and\n  Tasks","summary":"  A key assumption in multi-task learning is that at the inference time the\nmulti-task model only has access to a given data point but not to the data\npoint's labels from other tasks. This presents an opportunity to extend\nmulti-task learning to utilize data point's labels from other auxiliary tasks,\nand this way improves performance on the new task. Here we introduce a novel\nrelational multi-task learning setting where we leverage data point labels from\nauxiliary tasks to make more accurate predictions on the new task. We develop\nMetaLink, where our key innovation is to build a knowledge graph that connects\ndata points and tasks and thus allows us to leverage labels from auxiliary\ntasks. The knowledge graph consists of two types of nodes: (1) data nodes,\nwhere node features are data embeddings computed by the neural network, and (2)\ntask nodes, with the last layer's weights for each task as node features. The\nedges in this knowledge graph capture data-task relationships, and the edge\nlabel captures the label of a data point on a particular task. Under MetaLink,\nwe reformulate the new task as a link label prediction problem between a data\nnode and a task node. The MetaLink framework provides flexibility to model\nknowledge transfer from auxiliary task labels to the task of interest. We\nevaluate MetaLink on 6 benchmark datasets in both biochemical and vision\ndomains. Experiments demonstrate that MetaLink can successfully utilize the\nrelations among different tasks, outperforming the state-of-the-art methods\nunder the proposed relational multi-task learning setting, with up to 27%\nimprovement in ROC AUC.\n","authors":["Kaidi Cao","Jiaxuan You","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2303.07666v1.pdf","comment":"ICLR 2022 Spotlight"},{"id":"http://arxiv.org/abs/2207.03902v2","updated":"2023-03-14T07:13:04Z","published":"2022-07-08T13:42:54Z","title":"Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning","summary":"  Deep cooperative multi-agent reinforcement learning has demonstrated its\nremarkable success over a wide spectrum of complex control tasks. However,\nrecent advances in multi-agent learning mainly focus on value decomposition\nwhile leaving entity interactions still intertwined, which easily leads to\nover-fitting on noisy interactions between entities. In this work, we introduce\na novel interactiOn Pattern disenTangling (OPT) method, to disentangle not only\nthe joint value function into agent-wise value functions for decentralized\nexecution, but also the entity interactions into interaction prototypes, each\nof which represents an underlying interaction pattern within a subgroup of the\nentities. OPT facilitates filtering the noisy interactions between irrelevant\nentities and thus significantly improves generalizability as well as\ninterpretability. Specifically, OPT introduces a sparse disagreement mechanism\nto encourage sparsity and diversity among discovered interaction prototypes.\nThen the model selectively restructures these prototypes into a compact\ninteraction pattern by an aggregator with learnable weights. To alleviate the\ntraining instability issue caused by partial observability, we propose to\nmaximize the mutual information between the aggregation weights and the history\nbehaviors of each agent. Experiments on both single-task and multi-task\nbenchmarks demonstrate that the proposed method yields results superior to the\nstate-of-the-art counterparts. Our code is available at\nhttps://github.com/liushunyu/OPT.\n","authors":["Shunyu Liu","Jie Song","Yihe Zhou","Na Yu","Kaixuan Chen","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2207.03902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07651v1","updated":"2023-03-14T06:38:17Z","published":"2023-03-14T06:38:17Z","title":"Context Normalization for Robust Image Classification","summary":"  Normalization is a pre-processing step that converts the data into a more\nusable representation. As part of the deep neural networks (DNNs), the batch\nnormalization (BN) technique uses normalization to address the problem of\ninternal covariate shift. It can be packaged as general modules, which have\nbeen extensively integrated into various DNNs, to stabilize and accelerate\ntraining, presumably leading to improved generalization. However, the effect of\nBN is dependent on the mini-batch size and it does not take into account any\ngroups or clusters that may exist in the dataset when estimating population\nstatistics. This study proposes a new normalization technique, called context\nnormalization, for image data. This approach adjusts the scaling of features\nbased on the characteristics of each sample, which improves the model's\nconvergence speed and performance by adapting the data values to the context of\nthe target task. The effectiveness of context normalization is demonstrated on\nvarious datasets, and its performance is compared to other standard\nnormalization techniques.\n","authors":["Bilal Faye","Mohamed-Djallel Dilmi","Hanane Azzag","Mustapha Lebbah","Fangchen Feng"],"pdf_url":"https://arxiv.org/pdf/2303.07651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07647v1","updated":"2023-03-14T06:15:17Z","published":"2023-03-14T06:15:17Z","title":"Recent Advances and Applications of Machine Learning in Experimental\n  Solid Mechanics: A Review","summary":"  For many decades, experimental solid mechanics has played a crucial role in\ncharacterizing and understanding the mechanical properties of natural and novel\nmaterials. Recent advances in machine learning (ML) provide new opportunities\nfor the field, including experimental design, data analysis, uncertainty\nquantification, and inverse problems. As the number of papers published in\nrecent years in this emerging field is exploding, it is timely to conduct a\ncomprehensive and up-to-date review of recent ML applications in experimental\nsolid mechanics. Here, we first provide an overview of common ML algorithms and\nterminologies that are pertinent to this review, with emphasis placed on\nphysics-informed and physics-based ML methods. Then, we provide thorough\ncoverage of recent ML applications in traditional and emerging areas of\nexperimental mechanics, including fracture mechanics, biomechanics, nano- and\nmicro-mechanics, architected materials, and 2D material. Finally, we highlight\nsome current challenges of applying ML to multi-modality and multi-fidelity\nexperimental datasets and propose several future research directions. This\nreview aims to provide valuable insights into the use of ML methods as well as\na variety of examples for researchers in solid mechanics to integrate into\ntheir experiments.\n","authors":["Hanxun Jin","Enrui Zhang","Horacio D. Espinosa"],"pdf_url":"https://arxiv.org/pdf/2303.07647v1.pdf","comment":"76 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.07646v1","updated":"2023-03-14T06:11:57Z","published":"2023-03-14T06:11:57Z","title":"Clustering with Simplicial Complexes","summary":"  In this work, we propose a new clustering algorithm to group nodes in\nnetworks based on second-order simplices (aka filled triangles) to leverage\nhigher-order network interactions. We define a simplicial conductance function,\nwhich on minimizing, yields an optimal partition with a higher density of\nfilled triangles within the set while the density of filled triangles is\nsmaller across the sets. To this end, we propose a simplicial adjacency\noperator that captures the relation between the nodes through second-order\nsimplices. This allows us to extend the well-known Cheeger inequality to\ncluster a simplicial complex. Then, leveraging the Cheeger inequality, we\npropose the simplicial spectral clustering algorithm. We report results from\nnumerical experiments on synthetic and real-world network data to demonstrate\nthe efficacy of the proposed approach.\n","authors":["Thummaluru Siddartha Reddy","Sundeep Prabhakar Chepuri","Pierre Borgnat"],"pdf_url":"https://arxiv.org/pdf/2303.07646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14829v3","updated":"2023-03-14T06:05:01Z","published":"2023-02-22T07:56:45Z","title":"Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time\n  Series Forecasting","summary":"  The distribution shift in Time Series Forecasting (TSF), indicating series\ndistribution changes over time, largely hinders the performance of TSF models.\nExisting works towards distribution shift in time series are mostly limited in\nthe quantification of distribution and, more importantly, overlook the\npotential shift between lookback and horizon windows. To address above\nchallenges, we systematically summarize the distribution shift in TSF into two\ncategories. Regarding lookback windows as input-space and horizon windows as\noutput-space, there exist (i) intra-space shift, that the distribution within\nthe input-space keeps shifted over time, and (ii) inter-space shift, that the\ndistribution is shifted between input-space and output-space. Then we\nintroduce, Dish-TS, a general neural paradigm for alleviating distribution\nshift in TSF. Specifically, for better distribution estimation, we propose the\ncoefficient net (CONET), which can be any neural architectures, to map input\nsequences into learnable distribution coefficients. To relieve intra-space and\ninter-space shift, we organize Dish-TS as a Dual-CONET framework to separately\nlearn the distribution of input- and output-space, which naturally captures the\ndistribution difference of two spaces. In addition, we introduce a more\neffective training strategy for intractable CONET learning. Finally, we conduct\nextensive experiments on several datasets coupled with different\nstate-of-the-art forecasting models. Experimental results show Dish-TS\nconsistently boosts them with a more than 20% average improvement. Code is\navailable.\n","authors":["Wei Fan","Pengyang Wang","Dongkun Wang","Dongjie Wang","Yuanchun Zhou","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2302.14829v3.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2209.14609v4","updated":"2023-03-14T05:56:18Z","published":"2022-09-29T07:58:32Z","title":"Dataset Distillation Using Parameter Pruning","summary":"  In many fields, the acquisition of advanced models depends on large datasets,\nmaking data storage and model training expensive. As a solution, dataset\ndistillation can synthesize a small dataset that preserves most information of\nthe original large dataset. The recently proposed dataset distillation method\nby matching network parameters has been proven effective for several datasets.\nHowever, the dimensions of network parameters are typically large. Furthermore,\nsome parameters are difficult to match during the distillation process,\ndegrading distillation performance. Based on this observation, this study\nproposes a novel dataset distillation method based on parameter pruning that\nsolves the problem. The proposed method can synthesize more robust distilled\ndatasets and improve distillation performance by pruning difficult-to-match\nparameters during the distillation process. Experimental results on three\ndatasets show that the proposed method outperforms other state-of-the-art\ndataset distillation methods.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2209.14609v4.pdf","comment":"Submitted as a journal paper at IEEE SPL"},{"id":"http://arxiv.org/abs/2211.02247v2","updated":"2023-03-14T05:24:20Z","published":"2022-11-04T03:45:17Z","title":"Music Mixing Style Transfer: A Contrastive Learning Approach to\n  Disentangle Audio Effects","summary":"  We propose an end-to-end music mixing style transfer system that converts the\nmixing style of an input multitrack to that of a reference song. This is\nachieved with an encoder pre-trained with a contrastive objective to extract\nonly audio effects related information from a reference music recording. All\nour models are trained in a self-supervised manner from an already-processed\nwet multitrack dataset with an effective data preprocessing method that\nalleviates the data scarcity of obtaining unprocessed dry data. We analyze the\nproposed encoder for the disentanglement capability of audio effects and also\nvalidate its performance for mixing style transfer through both objective and\nsubjective evaluations. From the results, we show the proposed system not only\nconverts the mixing style of multitrack audio close to a reference but is also\nrobust with mixture-wise style transfer upon using a music source separation\nmodel.\n","authors":["Junghyun Koo","Marco A. Martínez-Ramírez","Wei-Hsiang Liao","Stefan Uhlich","Kyogu Lee","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2211.02247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10325v9","updated":"2023-03-14T05:23:24Z","published":"2021-10-20T00:47:31Z","title":"One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and\n  Its Application to Tumour Segmentation for Breast Cancer","summary":"  Recent studies have demonstrated the effectiveness of the combination of\nmachine learning and logical reasoning, including data-driven logical\nreasoning, knowledge driven machine learning and abductive learning, in\ninventing advanced artificial intelligence technologies. One-step abductive\nmulti-target learning (OSAMTL), an approach inspired by abductive learning, via\nsimply combining machine learning and logical reasoning in a one-step balanced\nway, has as well shown its effectiveness in handling complex noisy labels of a\nsingle noisy sample in medical histopathology whole slide image analysis\n(MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy\nsamples (DiNS) are provided for a learning task. In this paper, giving\ndefinition of DiNS, we propose one-step abductive multi-target learning with\nDiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels\nof DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in\nMHWSIA, we show that OSAMTL-DiNS is able to enable various state-of-the-art\napproaches for learning from noisy labels to achieve more rational predictions.\n","authors":["Yongquan Yang","Fengling Li","Yani Wei","Jie Chen","Ning Chen","Hong Bu"],"pdf_url":"https://arxiv.org/pdf/2110.10325v9.pdf","comment":"66 pages"},{"id":"http://arxiv.org/abs/2303.07627v1","updated":"2023-03-14T04:51:24Z","published":"2023-03-14T04:51:24Z","title":"Best arm identification in rare events","summary":"  We consider the best arm identification problem in the stochastic multi-armed\nbandit framework where each arm has a tiny probability of realizing large\nrewards while with overwhelming probability the reward is zero. A key\napplication of this framework is in online advertising where click rates of\nadvertisements could be a fraction of a single percent and final conversion to\nsales, while highly profitable, may again be a small fraction of the click\nrates. Lately, algorithms for BAI problems have been developed that minimise\nsample complexity while providing statistical guarantees on the correct arm\nselection. As we observe, these algorithms can be computationally prohibitive.\nWe exploit the fact that the reward process for each arm is well approximated\nby a Compound Poisson process to arrive at algorithms that are faster, with a\nsmall increase in sample complexity. We analyze the problem in an asymptotic\nregime as rarity of reward occurrence reduces to zero, and reward amounts\nincrease to infinity. This helps illustrate the benefits of the proposed\nalgorithm. It also sheds light on the underlying structure of the optimal BAI\nalgorithms in the rare event setting.\n","authors":["Anirban Bhattacharjee","Sushant Vijayan","Sandeep K Juneja"],"pdf_url":"https://arxiv.org/pdf/2303.07627v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2206.10696v3","updated":"2023-03-14T04:43:23Z","published":"2022-06-21T19:31:25Z","title":"Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting\n  Epidemics","summary":"  Infectious diseases remain among the top contributors to human illness and\ndeath worldwide, among which many diseases produce epidemic waves of infection.\nThe unavailability of specific drugs and ready-to-use vaccines to prevent most\nof these epidemics makes the situation worse. These force public health\nofficials and policymakers to rely on early warning systems generated by\nreliable and accurate forecasts of epidemics. Accurate forecasts of epidemics\ncan assist stakeholders in tailoring countermeasures, such as vaccination\ncampaigns, staff scheduling, and resource allocation, to the situation at hand,\nwhich could translate to reductions in the impact of a disease. Unfortunately,\nmost of these past epidemics exhibit nonlinear and non-stationary\ncharacteristics due to their spreading fluctuations based on seasonal-dependent\nvariability and the nature of these epidemics. We analyse a wide variety of\nepidemic time series datasets using a maximal overlap discrete wavelet\ntransform (MODWT) based autoregressive neural network and call it EWNet model.\nMODWT techniques effectively characterize non-stationary behavior and seasonal\ndependencies in the epidemic time series and improve the nonlinear forecasting\nscheme of the autoregressive neural network in the proposed ensemble wavelet\nnetwork framework. From a nonlinear time series viewpoint, we explore the\nasymptotic stationarity of the proposed EWNet model to show the asymptotic\nbehavior of the associated Markov Chain. We also theoretically investigate the\neffect of learning stability and the choice of hidden neurons in the proposal.\nFrom a practical perspective, we compare our proposed EWNet framework with\nseveral statistical, machine learning, and deep learning models. Experimental\nresults show that the proposed EWNet is highly competitive compared to the\nstate-of-the-art epidemic forecasting methods.\n","authors":["Madhurima Panja","Tanujit Chakraborty","Uttam Kumar","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2206.10696v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07622v1","updated":"2023-03-14T04:20:59Z","published":"2023-03-14T04:20:59Z","title":"RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via\n  Language-Based Feedback","summary":"  Reinforcement learning-based policies for continuous control robotic\nnavigation tasks often fail to adapt to changes in the environment during\nreal-time deployment, which may result in catastrophic failures. To address\nthis limitation, we propose a novel approach called RE-MOVE (\\textbf{RE}quest\nhelp and \\textbf{MOVE} on), which uses language-based feedback to adjust\ntrained policies to real-time changes in the environment. In this work, we\nenable the trained policy to decide \\emph{when to ask for feedback} and\n\\emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates\nepistemic uncertainty to determine the optimal time to request feedback from\nhumans and uses language-based feedback for real-time adaptation. We perform\nextensive synthetic and real-world evaluations to demonstrate the benefits of\nour proposed approach in several test-time dynamic navigation scenarios. Our\napproach enable robots to learn from human feedback and adapt to previously\nunseen adversarial situations.\n","authors":["Souradip Chakraborty","Kasun Weerakoon","Prithvi Poddar","Pratap Tokekar","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2303.07622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07074v3","updated":"2023-03-14T03:36:41Z","published":"2023-01-17T18:36:57Z","title":"SegViz: A federated-learning based framework for multi-organ\n  segmentation on heterogeneous data sets with partial annotations","summary":"  Segmentation is one of the most primary tasks in deep learning for medical\nimaging, owing to its multiple downstream clinical applications. However,\ngenerating manual annotations for medical images is time-consuming, requires\nhigh skill, and is an expensive effort, especially for 3D images. One potential\nsolution is to aggregate knowledge from partially annotated datasets from\nmultiple groups to collaboratively train global models using Federated\nLearning. To this end, we propose SegViz, a federated learning-based framework\nto train a segmentation model from distributed non-i.i.d datasets with partial\nannotations. The performance of SegViz was compared against training individual\nmodels separately on each dataset as well as centrally aggregating all the\ndatasets in one place and training a single model. The SegViz framework using\nFedBN as the aggregation strategy demonstrated excellent performance on the\nexternal BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for\nsegmentation of liver, spleen, pancreas, and kidneys, respectively,\nsignificantly ($p<0.05$) better (except spleen) than the dice scores of 0.87,\n0.83, 0.42, and 0.48 for the baseline models. In contrast, the central\naggregation model significantly ($p<0.05$) performed poorly on the test dataset\nwith dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the\npotential of the SegViz framework to train multi-task models from distributed\ndatasets with partial labels. All our implementations are open-source and\navailable at https://anonymous.4open.science/r/SegViz-B746\n","authors":["Adway U. Kanhere","Pranav Kulkarni","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.07074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00351v3","updated":"2023-03-14T03:12:12Z","published":"2023-01-01T05:26:33Z","title":"Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation","summary":"  An unbiased scene graph generation (SGG) algorithm referred to as Skew\nClass-balanced Re-weighting (SCR) is proposed for considering the unbiased\npredicate prediction caused by the long-tailed distribution. The prior works\nfocus mainly on alleviating the deteriorating performances of the minority\npredicate predictions, showing drastic dropping recall scores, i.e., losing the\nmajority predicate performances. It has not yet correctly analyzed the\ntrade-off between majority and minority predicate performances in the limited\nSGG datasets. In this paper, to alleviate the issue, the Skew Class-balanced\nRe-weighting (SCR) loss function is considered for the unbiased SGG models.\nLeveraged by the skewness of biased predicate predictions, the SCR estimates\nthe target predicate weight coefficient and then re-weights more to the biased\npredicates for better trading-off between the majority predicates and the\nminority ones. Extensive experiments conducted on the standard Visual Genome\ndataset and Open Image V4 \\& V6 show the performances and generality of the SCR\nwith the traditional SGG models.\n","authors":["Haeyong Kang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2301.00351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07608v1","updated":"2023-03-14T03:04:37Z","published":"2023-03-14T03:04:37Z","title":"On the Implicit Geometry of Cross-Entropy Parameterizations for\n  Label-Imbalanced Data","summary":"  Various logit-adjusted parameterizations of the cross-entropy (CE) loss have\nbeen proposed as alternatives to weighted CE for training large models on\nlabel-imbalanced data far beyond the zero train error regime. The driving force\nbehind those designs has been the theory of implicit bias, which for\nlinear(ized) models, explains why they successfully induce bias on the\noptimization path towards solutions that favor minorities. Aiming to extend\nthis theory to non-linear models, we investigate the implicit geometry of\nclassifiers and embeddings that are learned by different CE parameterizations.\nOur main result characterizes the global minimizers of a non-convex\ncost-sensitive SVM classifier for the unconstrained features model, which\nserves as an abstraction of deep nets. We derive closed-form formulas for the\nangles and norms of classifiers and embeddings as a function of the number of\nclasses, the imbalance and the minority ratios, and the loss hyperparameters.\nUsing these, we show that logit-adjusted parameterizations can be appropriately\ntuned to learn symmetric geometries irrespective of the imbalance ratio. We\ncomplement our analysis with experiments and an empirical study of convergence\naccuracy in deep-nets.\n","authors":["Tina Behnia","Ganesh Ramachandra Kini","Vala Vakilian","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2303.07608v1.pdf","comment":"Short version of this accepted at AISTATS 2023"},{"id":"http://arxiv.org/abs/2207.11659v3","updated":"2023-03-14T02:52:54Z","published":"2022-07-24T04:23:56Z","title":"Training Robust Spiking Neural Networks on Neuromorphic Data with\n  Spatiotemporal Fragments","summary":"  Neuromorphic vision sensors (event cameras) are inherently suitable for\nspiking neural networks (SNNs) and provide novel neuromorphic vision data for\nthis biomimetic model. Due to the spatiotemporal characteristics, novel data\naugmentations are required to process the unconventional visual signals of\nthese cameras. In this paper, we propose a novel Event SpatioTemporal Fragments\n(ESTF) augmentation method. It preserves the continuity of neuromorphic data by\ndrifting or inverting fragments of the spatiotemporal event stream to simulate\nthe disturbance of brightness variations, leading to more robust spiking neural\nnetworks. Extensive experiments are performed on prevailing neuromorphic\ndatasets. It turns out that ESTF provides substantial improvements over pure\ngeometric transformations and outperforms other event data augmentation\nmethods. It is worth noting that the SNNs with ESTF achieve the\nstate-of-the-art accuracy of 83.9\\% on the CIFAR10-DVS dataset.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11659v3.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2207.11670v2","updated":"2023-03-14T02:51:07Z","published":"2022-07-24T06:12:23Z","title":"Training Stronger Spiking Neural Networks with Biomimetic Adaptive\n  Internal Association Neurons","summary":"  As the third generation of neural networks, spiking neural networks (SNNs)\nare dedicated to exploring more insightful neural mechanisms to achieve\nnear-biological intelligence. Intuitively, biomimetic mechanisms are crucial to\nunderstanding and improving SNNs. For example, the associative long-term\npotentiation (ALTP) phenomenon suggests that in addition to learning mechanisms\nbetween neurons, there are associative effects within neurons. However, most\nexisting methods only focus on the former and lack exploration of the internal\nassociation effects. In this paper, we propose a novel Adaptive Internal\nAssociation~(AIA) neuron model to establish previously ignored influences\nwithin neurons. Consistent with the ALTP phenomenon, the AIA neuron model is\nadaptive to input stimuli, and internal associative learning occurs only when\nboth dendrites are stimulated at the same time. In addition, we employ weighted\nweights to measure internal associations and introduce intermediate caches to\nreduce the volatility of associations. Extensive experiments on prevailing\nneuromorphic datasets show that the proposed method can potentiate or depress\nthe firing of spikes more specifically, resulting in better performance with\nfewer spikes. It is worth noting that without adding any parameters at\ninference, the AIA model achieves state-of-the-art performance on\nDVS-CIFAR10~(83.9\\%) and N-CARS~(95.64\\%) datasets.\n","authors":["Haibo Shen","Yihao Luo","Xiang Cao","Liangqi Zhang","Juyu Xiao","Tianjiang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11670v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07122v2","updated":"2023-03-14T02:50:50Z","published":"2023-02-22T19:35:28Z","title":"Quantifying Causes of Arctic Amplification via Deep Learning based\n  Time-series Causal Inference","summary":"  The warming of the Arctic, also known as Arctic amplification, is led by\nseveral atmospheric and oceanic drivers, however, the details of its underlying\nthermodynamic causes are still unknown. Inferring the causal effects of\natmospheric processes on sea ice melt using fixed treatment effect strategies\nleads to unrealistic counterfactual estimations. Such models are also prone to\nbias due to time-varying confoundedness. In order to tackle these challenges,\nwe propose TCINet - time-series causal inference model to infer causation under\ncontinuous treatment using recurrent neural networks. Through experiments on\nsynthetic and observational data, we show how our research can substantially\nimprove the ability to quantify the leading causes of Arctic sea ice melt.\n","authors":["Sahara Ali","Omar Faruque","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04150v2","updated":"2023-03-14T02:48:42Z","published":"2022-10-09T02:57:32Z","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP","summary":"  Open-vocabulary semantic segmentation aims to segment an image into semantic\nregions according to text descriptions, which may not have been seen during\ntraining. Recent two-stage methods first generate class-agnostic mask proposals\nand then leverage pre-trained vision-language models, e.g., CLIP, to classify\nmasked regions. We identify the performance bottleneck of this paradigm to be\nthe pre-trained CLIP model, since it does not perform well on masked images. To\naddress this, we propose to finetune CLIP on a collection of masked image\nregions and their corresponding text descriptions. We collect training data by\nmining an existing image-caption dataset (e.g., COCO Captions), using CLIP to\nmatch masked image regions to nouns in the image captions. Compared with the\nmore precise and manually annotated segmentation labels with fixed classes\n(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain\nCLIP's generalization ability. Along with finetuning the entire model, we\nutilize the \"blank\" areas in masked images using a method we dub mask prompt\ntuning. Experiments demonstrate mask prompt tuning brings significant\nimprovement without modifying any weights of CLIP, and it can further improve a\nfully finetuned model. In particular, when trained on COCO and evaluated on\nADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the\nprevious state-of-the-art. For the first time, open-vocabulary generalist\nmodels match the performance of supervised specialist models in 2017 without\ndataset-specific adaptations.\n","authors":["Feng Liang","Bichen Wu","Xiaoliang Dai","Kunpeng Li","Yinan Zhao","Hang Zhang","Peizhao Zhang","Peter Vajda","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2210.04150v2.pdf","comment":"CVPR 2023. Project page: https://jeff-liangf.github.io/projects/ovseg"},{"id":"http://arxiv.org/abs/2303.07600v1","updated":"2023-03-14T02:46:42Z","published":"2023-03-14T02:46:42Z","title":"Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC)\n  Countries using Machine Learning","summary":"  COVID-19 has infected more than 68 million people worldwide since it was\nfirst detected about a year ago. Machine learning time series models have been\nimplemented to forecast COVID-19 infections. In this paper, we develop time\nseries models for the Gulf Cooperation Council (GCC) countries using the public\nCOVID-19 dataset from Johns Hopkins. The dataset set includes the one-year\ncumulative COVID-19 cases between 22/01/2020 to 22/01/2021. We developed\ndifferent models for the countries under study based on the spatial\ndistribution of the infection data. Our experimental results show that the\ndeveloped models can forecast COVID-19 infections with high precision.\n","authors":["Leila Ismail","Huned Materwala","Alain Hennebelle"],"pdf_url":"https://arxiv.org/pdf/2303.07600v1.pdf","comment":"9 pages, Proceedings of the 13th International Conference on Computer\n  Modeling and Simulation, ICCMS 2021, Autoregressive integrated moving\n  average, ARIMA, Coronavirus, COVID-19, Damped Trend, Holt Linear Trend,\n  Machine learning, Pandemic, Time series"},{"id":"http://arxiv.org/abs/2303.07599v1","updated":"2023-03-14T02:45:41Z","published":"2023-03-14T02:45:41Z","title":"A Contrastive Knowledge Transfer Framework for Model Compression and\n  Transfer Learning","summary":"  Knowledge Transfer (KT) achieves competitive performance and is widely used\nfor image classification tasks in model compression and transfer learning.\nExisting KT works transfer the information from a large model (\"teacher\") to\ntrain a small model (\"student\") by minimizing the difference of their\nconditionally independent output distributions. However, these works overlook\nthe high-dimension structural knowledge from the intermediate representations\nof the teacher, which leads to limited effectiveness, and they are motivated by\nvarious heuristic intuitions, which makes it difficult to generalize. This\npaper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which\nenables the transfer of sufficient structural knowledge from the teacher to the\nstudent by optimizing multiple contrastive objectives across the intermediate\nrepresentations between them. Also, CKTF provides a generalized agreement to\nexisting KT techniques and increases their performance significantly by\nderiving them as specific cases of CKTF. The extensive evaluation shows that\nCKTF consistently outperforms the existing KT works by 0.04% to 11.59% in model\ncompression and by 0.4% to 4.75% in transfer learning on various models and\ndatasets.\n","authors":["Kaiqi Zhao","Yitao Chen","Ming Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.07599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.07725v2","updated":"2023-03-14T02:45:14Z","published":"2021-02-15T18:19:07Z","title":"Neural Network Compression for Noisy Storage Devices","summary":"  Compression and efficient storage of neural network (NN) parameters is\ncritical for applications that run on resource-constrained devices. Despite the\nsignificant progress in NN model compression, there has been considerably less\ninvestigation in the actual \\textit{physical} storage of NN parameters.\nConventionally, model compression and physical storage are decoupled, as\ndigital storage media with error-correcting codes (ECCs) provide robust\nerror-free storage. However, this decoupled approach is inefficient as it\nignores the overparameterization present in most NNs and forces the memory\ndevice to allocate the same amount of resources to every bit of information\nregardless of its importance. In this work, we investigate analog memory\ndevices as an alternative to digital media -- one that naturally provides a way\nto add more protection for significant bits unlike its counterpart, but is\nnoisy and may compromise the stored model's performance if used naively. We\ndevelop a variety of robust coding strategies for NN weight storage on analog\ndevices, and propose an approach to jointly optimize model compression and\nmemory resource allocation. We then demonstrate the efficacy of our approach on\nmodels trained on MNIST, CIFAR-10 and ImageNet datasets for existing\ncompression techniques. Compared to conventional error-free digital storage,\nour method reduces the memory footprint by up to one order of magnitude,\nwithout significantly compromising the stored model's accuracy.\n","authors":["Berivan Isik","Kristy Choi","Xin Zheng","Tsachy Weissman","Stefano Ermon","H. -S. Philip Wong","Armin Alaghi"],"pdf_url":"https://arxiv.org/pdf/2102.07725v2.pdf","comment":"Published at the ACM Transactions on Embedded Computing Systems\n  (TECS), 2023"},{"id":"http://arxiv.org/abs/2303.00320v3","updated":"2023-03-14T02:43:29Z","published":"2023-03-01T08:33:16Z","title":"TimeMAE: Self-Supervised Representations of Time Series with Decoupled\n  Masked Autoencoders","summary":"  Enhancing the expressive capacity of deep learning-based time series models\nwith self-supervised pre-training has become ever-increasingly prevalent in\ntime series classification. Even though numerous efforts have been devoted to\ndeveloping self-supervised models for time series data, we argue that the\ncurrent methods are not sufficient to learn optimal time series representations\ndue to solely unidirectional encoding over sparse point-wise input units. In\nthis work, we propose TimeMAE, a novel self-supervised paradigm for learning\ntransferrable time series representations based on transformer networks. The\ndistinct characteristics of the TimeMAE lie in processing each time series into\na sequence of non-overlapping sub-series via window-slicing partitioning,\nfollowed by random masking strategies over the semantic units of localized\nsub-series. Such a simple yet effective setting can help us achieve the goal of\nkilling three birds with one stone, i.e., (1) learning enriched contextual\nrepresentations of time series with a bidirectional encoding scheme; (2)\nincreasing the information density of basic semantic units; (3) efficiently\nencoding representations of time series using transformer networks.\nNevertheless, it is a non-trivial to perform reconstructing task over such a\nnovel formulated modeling paradigm. To solve the discrepancy issue incurred by\nnewly injected masked embeddings, we design a decoupled autoencoder\narchitecture, which learns the representations of visible (unmasked) positions\nand masked ones with two different encoder modules, respectively. Furthermore,\nwe construct two types of informative targets to accomplish the corresponding\npretext tasks. One is to create a tokenizer module that assigns a codeword to\neach masked region, allowing the masked codeword classification (MCC) task to\nbe completed effectively...\n","authors":["Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.00320v3.pdf","comment":"Submitted to IEEE TRANSACTIONS ON KNOWLEDGE AND DATA\n  ENGINEERING(TKDE), under review"},{"id":"http://arxiv.org/abs/2303.07598v1","updated":"2023-03-14T02:42:01Z","published":"2023-03-14T02:42:01Z","title":"AdPE: Adversarial Positional Embeddings for Pretraining Vision\n  Transformers via MAE+","summary":"  Unsupervised learning of vision transformers seeks to pretrain an encoder via\npretext tasks without labels. Among them is the Masked Image Modeling (MIM)\naligned with pretraining of language transformers by predicting masked patches\nas a pretext task. A criterion in unsupervised pretraining is the pretext task\nneeds to be sufficiently hard to prevent the transformer encoder from learning\ntrivial low-level features not generalizable well to downstream tasks. For this\npurpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It\ndistorts the local visual structures by perturbing the position encodings so\nthat the learned transformer cannot simply use the locally correlated patches\nto predict the missing ones. We hypothesize that it forces the transformer\nencoder to learn more discriminative features in a global context with stronger\ngeneralizability to downstream tasks. We will consider both absolute and\nrelative positional encodings, where adversarial positions can be imposed both\nin the embedding mode and the coordinate mode. We will also present a new MAE+\nbaseline that brings the performance of the MIM pretraining to a new level with\nthe AdPE. The experiments demonstrate that our approach can improve the\nfine-tuning accuracy of MAE by $0.8\\%$ and $0.4\\%$ over 1600 epochs of\npretraining ViT-B and ViT-L on Imagenet1K. For the transfer learning task, it\noutperforms the MAE with the ViT-B backbone by $2.6\\%$ in mIoU on ADE20K, and\nby $3.2\\%$ in AP$^{bbox}$ and $1.6\\%$ in AP$^{mask}$ on COCO, respectively.\nThese results are obtained with the AdPE being a pure MIM approach that does\nnot use any extra models or external datasets for pretraining. The code is\navailable at https://github.com/maple-research-lab/AdPE.\n","authors":["Xiao Wang","Ying Wang","Ziwei Xuan","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2303.07598v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.07597v1","updated":"2023-03-14T02:40:46Z","published":"2023-03-14T02:40:46Z","title":"Fast Regularized Discrete Optimal Transport with Group-Sparse\n  Regularizers","summary":"  Regularized discrete optimal transport (OT) is a powerful tool to measure the\ndistance between two discrete distributions that have been constructed from\ndata samples on two different domains. While it has a wide range of\napplications in machine learning, in some cases the sampled data from only one\nof the domains will have class labels such as unsupervised domain adaptation.\nIn this kind of problem setting, a group-sparse regularizer is frequently\nleveraged as a regularization term to handle class labels. In particular, it\ncan preserve the label structure on the data samples by corresponding the data\nsamples with the same class label to one group-sparse regularization term. As a\nresult, we can measure the distance while utilizing label information by\nsolving the regularized optimization problem with gradient-based algorithms.\nHowever, the gradient computation is expensive when the number of classes or\ndata samples is large because the number of regularization terms and their\nrespective sizes also turn out to be large. This paper proposes fast discrete\nOT with group-sparse regularizers. Our method is based on two ideas. The first\nis to safely skip the computations of the gradients that must be zero. The\nsecond is to efficiently extract the gradients that are expected to be nonzero.\nOur method is guaranteed to return the same value of the objective function as\nthat of the original method. Experiments show that our method is up to 8.6\ntimes faster than the original method without degrading accuracy.\n","authors":["Yasutoshi Ida","Sekitoshi Kanai","Kazuki Adachi","Atsutoshi Kumagai","Yasuhiro Fujiwara"],"pdf_url":"https://arxiv.org/pdf/2303.07597v1.pdf","comment":"This is an extended version of the paper accepted by the 37th AAAI\n  Conference on Artificial Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2303.07589v1","updated":"2023-03-14T02:26:34Z","published":"2023-03-14T02:26:34Z","title":"Sequential three-way decisions with a single hidden layer feedforward\n  neural network","summary":"  The three-way decisions strategy has been employed to construct network\ntopology in a single hidden layer feedforward neural network (SFNN). However,\nthis model has a general performance, and does not consider the process costs,\nsince it has fixed threshold parameters. Inspired by the sequential three-way\ndecisions (STWD), this paper proposes STWD with an SFNN (STWD-SFNN) to enhance\nthe performance of networks on structured datasets. STWD-SFNN adopts\nmulti-granularity levels to dynamically learn the number of hidden layer nodes\nfrom coarse to fine, and set the sequential threshold parameters. Specifically,\nat the coarse granular level, STWD-SFNN handles easy-to-classify instances by\napplying strict threshold conditions, and with the increasing number of hidden\nlayer nodes at the fine granular level, STWD-SFNN focuses more on disposing of\nthe difficult-to-classify instances by applying loose threshold conditions,\nthereby realizing the classification of instances. Moreover, STWD-SFNN\nconsiders and reports the process cost produced from each granular level. The\nexperimental results verify that STWD-SFNN has a more compact network on\nstructured datasets than other SFNN models, and has better generalization\nperformance than the competitive models. All models and datasets can be\ndownloaded from https://github.com/wuc567/Machine-learning/tree/main/STWD-SFNN.\n","authors":["Youxi Wu","Shuhui Cheng","Yan Li","Rongjie Lv","Fan Min"],"pdf_url":"https://arxiv.org/pdf/2303.07589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07586v1","updated":"2023-03-14T02:12:00Z","published":"2023-03-14T02:12:00Z","title":"Teacher-Student Knowledge Distillation for Radar Perception on Embedded\n  Accelerators","summary":"  Many radar signal processing methodologies are being developed for critical\nroad safety perception tasks. Unfortunately, these signal processing algorithms\nare often poorly suited to run on embedded hardware accelerators used in\nautomobiles. Conversely, end-to-end machine learning (ML) approaches better\nexploit the performance gains brought by specialized accelerators. In this\npaper, we propose a teacher-student knowledge distillation approach for\nlow-level radar perception tasks. We utilize a hybrid model for stationary\nobject detection as a teacher to train an end-to-end ML student model. The\nstudent can efficiently harness embedded compute for real-time deployment. We\ndemonstrate that the proposed student model runs at speeds 100x faster than the\nteacher model.\n","authors":["Steven Shaw","Kanishka Tyagi","Shan Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07586v1.pdf","comment":"submitted at ASILOMAR,2023"},{"id":"http://arxiv.org/abs/2211.12712v2","updated":"2023-03-14T02:07:45Z","published":"2022-11-23T05:18:42Z","title":"Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition","summary":"  Value Decomposition (VD) aims to deduce the contributions of agents for\ndecentralized policies in the presence of only global rewards, and has recently\nemerged as a powerful credit assignment paradigm for tackling cooperative\nMulti-Agent Reinforcement Learning (MARL) problems. One of the main challenges\nin VD is to promote diverse behaviors among agents, while existing methods\ndirectly encourage the diversity of learned agent networks with various\nstrategies. However, we argue that these dedicated designs for agent networks\nare still limited by the indistinguishable VD network, leading to homogeneous\nagent behaviors and thus downgrading the cooperation capability. In this paper,\nwe propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly\nboosting the credit-level distinguishability of the VD network to break the\nbottleneck of multi-agent diversity. Specifically, our approach leverages\ncontrastive learning to maximize the mutual information between the temporal\ncredits and identity representations of different agents, encouraging the full\nexpressiveness of credit assignment and further the emergence of\nindividualities. The algorithm implementation of the proposed CIA module is\nsimple yet effective that can be readily incorporated into various VD\narchitectures. Experiments on the SMAC benchmarks and across different VD\nbackbones demonstrate that the proposed method yields results superior to the\nstate-of-the-art counterparts. Our code is available at\nhttps://github.com/liushunyu/CIA.\n","authors":["Shunyu Liu","Yihe Zhou","Jie Song","Tongya Zheng","Kaixuan Chen","Tongtian Zhu","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2211.12712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07580v1","updated":"2023-03-14T01:56:15Z","published":"2023-03-14T01:56:15Z","title":"Sensitive Region-based Metamorphic Testing Framework using Explainable\n  AI","summary":"  Deep Learning (DL) is one of the most popular research topics in machine\nlearning and DL-driven image recognition systems have developed rapidly. Recent\nresearch has used metamorphic testing (MT) to detect misclassified images. Most\nof them discuss metamorphic relations (MR), with little discussion on which\nregions should be transformed. We focus on the fact that there are sensitive\nregions where even a small transformation can easily change the prediction\nresults and propose an MT framework that efficiently tests for regions prone to\nmisclassification by transforming the sensitive regions. Our evaluation showed\nthat the sensitive regions can be specified by Explainable AI (XAI) and our\nframework effectively detects faults.\n","authors":["Yuma Torikoshi","Yasuharu Nishi","Juichi Takahashi"],"pdf_url":"https://arxiv.org/pdf/2303.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07578v1","updated":"2023-03-14T01:55:41Z","published":"2023-03-14T01:55:41Z","title":"VANI: Very-lightweight Accent-controllable TTS for Native and Non-native\n  speakers with Identity Preservation","summary":"  We introduce VANI, a very lightweight multi-lingual accent controllable\nspeech synthesis system. Our model builds upon disentanglement strategies\nproposed in RADMMM and supports explicit control of accent, language, speaker\nand fine-grained $F_0$ and energy features for speech synthesis. We utilize the\nIndic languages dataset, released for LIMMITS 2023 as part of ICASSP Signal\nProcessing Grand Challenge, to synthesize speech in 3 different languages. Our\nmodel supports transferring the language of a speaker while retaining their\nvoice and the native accent of the target language. We utilize the\nlarge-parameter RADMMM model for Track $1$ and lightweight VANI model for Track\n$2$ and $3$ of the competition.\n","authors":["Rohan Badlani","Akshit Arora","Subhankar Ghosh","Rafael Valle","Kevin J. Shih","João Felipe Santos","Boris Ginsburg","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2303.07578v1.pdf","comment":"Presentation accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07560v1","updated":"2023-03-14T00:57:11Z","published":"2023-03-14T00:57:11Z","title":"Machine Learning Computer Vision Applications for Spatial AI Object\n  Recognition in Orange County, California","summary":"  We provide an integrated and systematic automation approach to spatial object\nrecognition and positional detection using AI machine learning and computer\nvision algorithms for Orange County, California. We describe a comprehensive\nmethodology for multi-sensor, high-resolution field data acquisition, along\nwith post-field processing and pre-analysis processing tasks. We developed a\nseries of algorithmic formulations and workflows that integrate convolutional\ndeep neural network learning with detected object positioning estimation in\n360{\\deg} equirectancular photosphere imagery. We provide examples of\napplication processing more than 800 thousand cardinal directions in\nphotosphere images across two areas in Orange County, and present detection\nresults for stop-sign and fire hydrant object recognition. We discuss the\nefficiency and effectiveness of our approach, along with broader inferences\nrelated to the performance and implications of this approach for future\ntechnological innovations, including automation of spatial data and public\nasset inventories, and near real-time AI field data systems.\n","authors":["Kostas Alexandridis"],"pdf_url":"https://arxiv.org/pdf/2303.07560v1.pdf","comment":"24 pages, 15 figures, 8 tables"},{"id":"http://arxiv.org/abs/2303.07557v1","updated":"2023-03-14T00:49:09Z","published":"2023-03-14T00:49:09Z","title":"Lifelong Learning for Anomaly Detection: New Challenges, Perspectives,\n  and Insights","summary":"  Anomaly detection is of paramount importance in many real-world domains,\ncharacterized by evolving behavior. Lifelong learning represents an emerging\ntrend, answering the need for machine learning models that continuously adapt\nto new challenges in dynamic environments while retaining past knowledge.\nHowever, limited efforts are dedicated to building foundations for lifelong\nanomaly detection, which provides intrinsically different challenges compared\nto the more widely explored classification setting. In this paper, we face this\nissue by exploring, motivating, and discussing lifelong anomaly detection,\ntrying to build foundations for its wider adoption. First, we explain why\nlifelong anomaly detection is relevant, defining challenges and opportunities\nto design anomaly detection methods that deal with lifelong learning\ncomplexities. Second, we characterize learning settings and a scenario\ngeneration procedure that enables researchers to experiment with lifelong\nanomaly detection using existing datasets. Third, we perform experiments with\npopular anomaly detection methods on proposed lifelong scenarios, emphasizing\nthe gap in performance that could be gained with the adoption of lifelong\nlearning. Overall, we conclude that the adoption of lifelong anomaly detection\nis important to design more robust models that provide a comprehensive view of\nthe environment, as well as simultaneous adaptation and knowledge retention.\n","authors":["Kamil Faber","Roberto Corizzo","Bartlomiej Sniezynski","Nathalie Japkowicz"],"pdf_url":"https://arxiv.org/pdf/2303.07557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07551v1","updated":"2023-03-14T00:43:48Z","published":"2023-03-14T00:43:48Z","title":"Merging Decision Transformers: Weight Averaging for Forming Multi-Task\n  Policies","summary":"  Recent work has shown the promise of creating generalist, transformer-based,\npolicies for language, vision, and sequential decision-making problems. To\ncreate such models, we generally require centralized training objectives, data,\nand compute. It is of interest if we can more flexibly create generalist\npolicies, by merging together multiple, task-specific, individually trained\npolicies. In this work, we take a preliminary step in this direction through\nmerging, or averaging, subsets of Decision Transformers in weight space trained\non different MuJoCo locomotion problems, forming multi-task models without\ncentralized training. We also propose that when merging policies, we can obtain\nbetter results if all policies start from common, pre-trained initializations,\nwhile also co-training on shared auxiliary tasks during problem-specific\nfinetuning. In general, we believe research in this direction can help\ndemocratize and distribute the process of which forms generally capable agents.\n","authors":["Daniel Lawson","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2303.07551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07546v1","updated":"2023-03-14T00:27:33Z","published":"2023-03-14T00:27:33Z","title":"Constrained Adversarial Learning and its applicability to Automated\n  Software Testing: a systematic review","summary":"  Every novel technology adds hidden vulnerabilities ready to be exploited by a\ngrowing number of cyber-attacks. Automated software testing can be a promising\nsolution to quickly analyze thousands of lines of code by generating and\nslightly modifying function-specific testing data to encounter a multitude of\nvulnerabilities and attack vectors. This process draws similarities to the\nconstrained adversarial examples generated by adversarial learning methods, so\nthere could be significant benefits to the integration of these methods in\nautomated testing tools. Therefore, this systematic review is focused on the\ncurrent state-of-the-art of constrained data generation methods applied for\nadversarial learning and software testing, aiming to guide researchers and\ndevelopers to enhance testing tools with adversarial learning methods and\nimprove the resilience and robustness of their digital systems. The found\nconstrained data generation applications for adversarial machine learning were\nsystematized, and the advantages and limitations of approaches specific for\nsoftware testing were thoroughly analyzed, identifying research gaps and\nopportunities to improve testing tools with adversarial attack methods.\n","authors":["João Vitorino","Tiago Dias","Tiago Fonseca","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2303.07546v1.pdf","comment":"32 pages, 5 tables, 2 figures, Information and Software Technology\n  journal"},{"id":"http://arxiv.org/abs/2211.01324v5","updated":"2023-03-14T00:22:14Z","published":"2022-11-02T17:43:04Z","title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert\n  Denoisers","summary":"  Large-scale diffusion-based generative models have led to breakthroughs in\ntext-conditioned high-resolution image synthesis. Starting from random noise,\nsuch text-to-image diffusion models gradually synthesize images in an iterative\nfashion while conditioning on text prompts. We find that their synthesis\nbehavior qualitatively changes throughout this process: Early in sampling,\ngeneration strongly relies on the text prompt to generate text-aligned content,\nwhile later, the text conditioning is almost entirely ignored. This suggests\nthat sharing model parameters throughout the entire generation process may not\nbe ideal. Therefore, in contrast to existing works, we propose to train an\nensemble of text-to-image diffusion models specialized for different synthesis\nstages. To maintain training efficiency, we initially train a single model,\nwhich is then split into specialized models that are trained for the specific\nstages of the iterative generation process. Our ensemble of diffusion models,\ncalled eDiff-I, results in improved text alignment while maintaining the same\ninference computation cost and preserving high visual quality, outperforming\nprevious large-scale text-to-image diffusion models on the standard benchmark.\nIn addition, we train our model to exploit a variety of embeddings for\nconditioning, including the T5 text, CLIP text, and CLIP image embeddings. We\nshow that these different embeddings lead to different behaviors. Notably, the\nCLIP image embedding allows an intuitive way of transferring the style of a\nreference image to the target text-to-image output. Lastly, we show a technique\nthat enables eDiff-I's \"paint-with-words\" capability. A user can select the\nword in the input text and paint it in a canvas to control the output, which is\nvery handy for crafting the desired image in mind. The project page is\navailable at https://deepimagination.cc/eDiff-I/\n","authors":["Yogesh Balaji","Seungjun Nah","Xun Huang","Arash Vahdat","Jiaming Song","Qinsheng Zhang","Karsten Kreis","Miika Aittala","Timo Aila","Samuli Laine","Bryan Catanzaro","Tero Karras","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2211.01324v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07543v1","updated":"2023-03-14T00:13:57Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear\n  Discriminative Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminative Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07540v1","updated":"2023-03-14T00:05:08Z","published":"2023-03-14T00:05:08Z","title":"Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial\n  Wedge Pressure from Cardiac MRI","summary":"  Heart failure is a serious and life-threatening condition that can lead to\nelevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure\n(PAWP) is an important surrogate marker indicating high pressure in the left\nventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an\ninvasive procedure. A non-invasive method is useful in quickly identifying\nhigh-risk patients from a large population. In this work, we develop a tensor\nlearning-based pipeline for identifying PAWP from multimodal cardiac Magnetic\nResonance Imaging (MRI). This pipeline extracts spatial and temporal features\nfrom high-dimensional scans. For quality control, we incorporate an epistemic\nuncertainty-based binning strategy to identify poor-quality training samples.\nTo improve the performance, we learn complementary information by integrating\nfeatures from multimodal data: cardiac MRI with short-axis and four-chamber\nviews, and Electronic Health Records. The experimental analysis on a large\ncohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation\nindicates that the proposed pipeline has a diagnostic value and can produce\npromising performance with significant improvement over the baseline in\nclinical practice (i.e., $\\Delta$AUC $=0.10$, $\\Delta$Accuracy $=0.06$, and\n$\\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical\nutility of our method.\n","authors":["Prasun C. Tripathi","Mohammod N. I. Suvon","Lawrence Schobs","Shuo Zhou","Samer Alabed","Andrew J. Swift","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2303.07540v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2211.16198v2","updated":"2023-03-14T16:13:03Z","published":"2022-11-28T16:48:41Z","title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n","authors":["Vishaal Udandarao","Ankush Gupta","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2211.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07863v1","updated":"2023-03-14T12:53:27Z","published":"2023-03-14T12:53:27Z","title":"You Can Ground Earlier than See: An Effective and Efficient Pipeline for\n  Temporal Sentence Grounding in Compressed Videos","summary":"  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a\ntarget moment semantically according to a sentence query. Although previous\nrespectable works have made decent success, they only focus on high-level\nvisual features extracted from the consecutive decoded frames and fail to\nhandle the compressed videos for query modelling, suffering from insufficient\nrepresentation capability and significant computational complexity during\ntraining and testing. In this paper, we pose a new setting, compressed-domain\nTSG, which directly utilizes compressed videos rather than fully-decompressed\nframes as the visual input. To handle the raw video bit-stream input, we\npropose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)\nframework, which extracts and aggregates three kinds of low-level visual\nfeatures (I-frame, motion vector and residual features) for effective and\nefficient grounding. Particularly, instead of encoding the whole decoded frames\nlike previous works, we capture the appearance representation by only learning\nthe I-frame feature to reduce delay or latency. Besides, we explore the motion\ninformation not only by learning the motion vector feature, but also by\nexploring the relations of neighboring frames via the residual feature. In this\nway, a three-branch spatial-temporal attention layer with an adaptive\nmotion-appearance fusion module is further designed to extract and aggregate\nboth appearance and motion information for the final grounding. Experiments on\nthree challenging datasets shows that our TCSF achieves better performance than\nother state-of-the-art methods with lower complexity.\n","authors":["Xiang Fang","Daizong Liu","Pan Zhou","Guoshun Nan"],"pdf_url":"https://arxiv.org/pdf/2303.07863v1.pdf","comment":"Accepted by CVPR-23"},{"id":"http://arxiv.org/abs/2302.10657v2","updated":"2023-03-14T11:35:27Z","published":"2023-02-21T13:19:19Z","title":"DasFormer: Deep Alternating Spectrogram Transformer for\n  Multi/Single-Channel Speech Separation","summary":"  For the task of speech separation, previous study usually treats\nmulti-channel and single-channel scenarios as two research tracks with\nspecialized solutions developed respectively. Instead, we propose a simple and\nunified architecture - DasFormer (Deep alternating spectrogram transFormer) to\nhandle both of them in the challenging reverberant environments. Unlike\nframe-wise sequence modeling, each TF-bin in the spectrogram is assigned with\nan embedding encoding spectral and spatial information. With such input,\nDasFormer is then formed by multiple repetition of simple blocks each of which\nintegrates 1) two multi-head self-attention (MHSA) modules alternately\nprocessing within each frequency bin & temporal frame of the spectrogram 2)\nMBConv before each MHSA for modeling local features on the spectrogram.\nExperiments show that DasFormer has a powerful ability to model the\ntime-frequency representation, whose performance far exceeds the current SOTA\nmodels in multi-channel speech separation, and also achieves single-channel\nSOTA in the more challenging yet realistic reverberation scenario.\n","authors":["Shuo Wang","Xiangyu Kong","Xiulian Peng","Mahmood Movassagh","Vinod Prakash","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.10657v2.pdf","comment":"5 pages, accepted by ICASSP2023"},{"id":"http://arxiv.org/abs/2303.07794v1","updated":"2023-03-14T11:09:12Z","published":"2023-03-14T11:09:12Z","title":"DiffuseRoll: Multi-track multi-category music generation based on\n  diffusion model","summary":"  Recent advancements in generative models have shown remarkable progress in\nmusic generation. However, most existing methods focus on generating monophonic\nor homophonic music, while the generation of polyphonic and multi-track music\nwith rich attributes is still a challenging task. In this paper, we propose a\nnovel approach for multi-track, multi-attribute symphonic music generation\nusing the diffusion model. Specifically, we generate piano-roll representations\nwith a diffusion model and map them to MIDI format for output. To capture rich\nattribute information, we introduce a color coding scheme to encode note\nsequences into color and position information that represents pitch,velocity,\nand instrument. This scheme enables a seamless mapping between discrete music\nsequences and continuous images. We also propose a post-processing method to\noptimize the generated scores for better performance. Experimental results show\nthat our method outperforms state-of-the-art methods in terms of polyphonic\nmusic generation with rich attribute information compared to the figure\nmethods.\n","authors":["Hongfei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07748v1","updated":"2023-03-14T09:48:59Z","published":"2023-03-14T09:48:59Z","title":"Generation-Guided Multi-Level Unified Network for Video Grounding","summary":"  Video grounding aims to locate the timestamps best matching the query\ndescription within an untrimmed video. Prevalent methods can be divided into\nmoment-level and clip-level frameworks. Moment-level approaches directly\npredict the probability of each transient moment to be the boundary in a global\nperspective, and they usually perform better in coarse grounding. On the other\nhand, clip-level ones aggregate the moments in different time windows into\nproposals and then deduce the most similar one, leading to its advantage in\nfine-grained grounding. In this paper, we propose a multi-level unified\nframework to enhance performance by leveraging the merits of both moment-level\nand clip-level methods. Moreover, a novel generation-guided paradigm in both\nlevels is adopted. It introduces a multi-modal generator to produce the\nimplicit boundary feature and clip feature, later regarded as queries to\ncalculate the boundary scores by a discriminator. The generation-guided\nsolution enhances video grounding from a two-unique-modals' match task to a\ncross-modal attention task, which steps out of the previous framework and\nobtains notable gains. The proposed Generation-guided Multi-level Unified\nnetwork (GMU) surpasses previous methods and reaches State-Of-The-Art on\nvarious benchmarks with disparate features, e.g., Charades-STA, ActivityNet\ncaptions.\n","authors":["Xing Cheng","Xiangyu Wu","Dong Shen","Hezheng Lin","Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2303.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07626v1","updated":"2023-03-14T04:50:52Z","published":"2023-03-14T04:50:52Z","title":"CAT: Causal Audio Transformer for Audio Classification","summary":"  The attention-based Transformers have been increasingly applied to audio\nclassification because of their global receptive field and ability to handle\nlong-term dependency. However, the existing frameworks which are mainly\nextended from the Vision Transformers are not perfectly compatible with audio\nsignals. In this paper, we introduce a Causal Audio Transformer (CAT)\nconsisting of a Multi-Resolution Multi-Feature (MRMF) feature extraction with\nan acoustic attention block for more optimized audio modeling. In addition, we\npropose a causal module that alleviates over-fitting, helps with knowledge\ntransfer, and improves interpretability. CAT obtains higher or comparable\nstate-of-the-art classification performance on ESC50, AudioSet and UrbanSound8K\ndatasets, and can be easily generalized to other Transformer-based models.\n","authors":["Xiaoyu Liu","Hanlin Lu","Jianbo Yuan","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2303.07626v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.14402v3","updated":"2023-03-14T01:41:44Z","published":"2023-02-28T08:35:50Z","title":"Neural Video Compression with Diverse Contexts","summary":"  For any video codecs, the coding efficiency highly relies on whether the\ncurrent signal to be encoded can find the relevant contexts from the previous\nreconstructed signals. Traditional codec has verified more contexts bring\nsubstantial coding gain, but in a time-consuming manner. However, for the\nemerging neural video codec (NVC), its contexts are still limited, leading to\nlow compression ratio. To boost NVC, this paper proposes increasing the context\ndiversity in both temporal and spatial dimensions. First, we guide the model to\nlearn hierarchical quality patterns across frames, which enriches long-term and\nyet high-quality temporal contexts. Furthermore, to tap the potential of\noptical flow-based coding framework, we introduce a group-based offset\ndiversity where the cross-group interaction is proposed for better context\nmining. In addition, this paper also adopts a quadtree-based partition to\nincrease spatial context diversity when encoding the latent representation in\nparallel. Experiments show that our codec obtains 23.5% bitrate saving over\nprevious SOTA NVC. Better yet, our codec has surpassed the under-developing\nnext generation traditional codec/ECM in both RGB and YUV420 colorspaces, in\nterms of PSNR. The codes are at https://github.com/microsoft/DCVC.\n","authors":["Jiahao Li","Bin Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2302.14402v3.pdf","comment":"Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC"}]},"2023-03-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.07522v1","updated":"2023-03-13T23:17:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v1.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2303.07519v1","updated":"2023-03-13T23:11:05Z","published":"2023-03-13T23:11:05Z","title":"Architext: Language-Driven Generative Architecture Design","summary":"  Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100\\%\nrate. Accuracy shows great improvement when scaling the models, with the\nlargest model (GPT-J) yielding impressive accuracy ranging between 25% to over\n80% for different prompt categories. We open source the finetuned Architext\nmodels and our synthetic dataset, hoping to inspire experimentation in this\nexciting area of design research.\n","authors":["Theodoros Galanos","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2303.07519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01860v2","updated":"2023-03-13T21:41:39Z","published":"2023-02-03T17:07:23Z","title":"GLADIS: A General and Large Acronym Disambiguation Benchmark","summary":"  Acronym Disambiguation (AD) is crucial for natural language understanding on\nvarious sources, including biomedical reports, scientific papers, and search\nengine queries. However, existing acronym disambiguation benchmarks and tools\nare limited to specific domains, and the size of prior benchmarks is rather\nsmall. To accelerate the research on acronym disambiguation, we construct a new\nbenchmark named GLADIS with three components: (1) a much larger acronym\ndictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus\nwith 160 million sentences; (3) three datasets that cover the general,\nscientific, and biomedical domains. We then pre-train a language model,\n\\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,\nand show the challenges and values of our new benchmark.\n","authors":["Lihu Chen","Gaël Varoquaux","Fabian M. Suchanek"],"pdf_url":"https://arxiv.org/pdf/2302.01860v2.pdf","comment":"Long paper at EACL 23"},{"id":"http://arxiv.org/abs/2303.07457v1","updated":"2023-03-13T20:34:56Z","published":"2023-03-13T20:34:56Z","title":"AMOM: Adaptive Masking over Masking for Conditional Masked Language\n  Model","summary":"  Transformer-based autoregressive (AR) methods have achieved appealing\nperformance for varied sequence-to-sequence generation tasks, e.g., neural\nmachine translation, summarization, and code generation, but suffer from low\ninference efficiency. To speed up the inference stage, many non-autoregressive\n(NAR) strategies have been proposed in the past few years. Among them, the\nconditional masked language model (CMLM) is one of the most versatile\nframeworks, as it can support many different sequence generation scenarios and\nachieve very competitive performance on these tasks. In this paper, we further\nintroduce a simple yet effective adaptive masking over masking strategy to\nenhance the refinement capability of the decoder and make the encoder\noptimization easier. Experiments on \\textbf{3} different tasks (neural machine\ntranslation, summarization, and code generation) with \\textbf{15} datasets in\ntotal confirm that our proposed simple method achieves significant performance\nimprovement over the strong CMLM model. Surprisingly, our proposed model yields\nstate-of-the-art performance on neural machine translation (\\textbf{34.62} BLEU\non WMT16 EN$\\to$RO, \\textbf{34.82} BLEU on WMT16 RO$\\to$EN, and \\textbf{34.84}\nBLEU on IWSLT De$\\to$En) and even better performance than the \\textbf{AR}\nTransformer on \\textbf{7} benchmark datasets with at least \\textbf{2.2$\\times$}\nspeedup. Our code is available at GitHub.\n","authors":["Yisheng Xiao","Ruiyang Xu","Lijun Wu","Juntao Li","Tao Qin","Yan-Tie Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07457v1.pdf","comment":"Accepted by AAAI2023"},{"id":"http://arxiv.org/abs/2212.02623v3","updated":"2023-03-13T17:42:44Z","published":"2022-12-05T22:14:49Z","title":"Unifying Vision, Text, and Layout for Universal Document Processing","summary":"  We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n","authors":["Zineng Tang","Ziyi Yang","Guoxin Wang","Yuwei Fang","Yang Liu","Chenguang Zhu","Michael Zeng","Cha Zhang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2212.02623v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.07320v1","updated":"2023-03-13T17:41:57Z","published":"2023-03-13T17:41:57Z","title":"Model-tuning Via Prompts Makes NLP Models Adversarially Robust","summary":"  In recent years, NLP practitioners have converged on the following practice:\n(i) import an off-the-shelf pretrained (masked) language model; (ii) append a\nmultilayer perceptron atop the CLS token's hidden representation (with randomly\ninitialized weights); and (iii) fine-tune the entire model on a downstream task\n(MLP). This procedure has produced massive gains on standard NLP benchmarks,\nbut these models remain brittle, even to mild adversarial perturbations, such\nas word-level synonym substitutions. In this work, we demonstrate surprising\ngains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an\nalternative method of adapting to downstream tasks. Rather than modifying the\nmodel (by appending an MLP head), MVP instead modifies the input (by appending\na prompt template). Across three classification datasets, MVP improves\nperformance against adversarial word-level synonym substitutions by an average\nof 8% over standard methods and even outperforms adversarial training-based\nstate-of-art defenses by 3.5%. By combining MVP with adversarial training, we\nachieve further improvements in robust accuracy while maintaining clean\naccuracy. Finally, we conduct ablations to investigate the mechanism underlying\nthese gains. Notably, we find that the main causes of vulnerability of MLP can\nbe attributed to the misalignment between pre-training and fine-tuning tasks,\nand the randomly initialized MLP parameters. Code is available at\nhttps://github.com/acmi-lab/mvp\n","authors":["Mrigank Raman","Pratyush Maini","J. Zico Kolter","Zachary C. Lipton","Danish Pruthi"],"pdf_url":"https://arxiv.org/pdf/2303.07320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07295v1","updated":"2023-03-13T17:17:11Z","published":"2023-03-13T17:17:11Z","title":"Meet in the Middle: A New Pre-training Paradigm","summary":"  Most language models (LMs) are trained and applied in an autoregressive\nleft-to-right fashion, assuming that the next token only depends on the\npreceding ones. However, this assumption ignores the potential benefits of\nusing the full sequence information during training, and the possibility of\nhaving context from both sides during inference. In this paper, we propose a\nnew pre-training paradigm with techniques that jointly improve the training\ndata efficiency and the capabilities of the LMs in the infilling task. The\nfirst is a training objective that aligns the predictions of a left-to-right LM\nwith those of a right-to-left LM, trained on the same data but in reverse\norder. The second is a bidirectional inference procedure that enables both LMs\nto meet in the middle. We show the effectiveness of our pre-training paradigm\nwith extensive experiments on both programming and natural language models,\noutperforming strong baselines.\n","authors":["Anh Nguyen","Nikos Karampatziakis","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.07295v1.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.07292v1","updated":"2023-03-13T17:12:03Z","published":"2023-03-13T17:12:03Z","title":"Transformer-based approaches to Sentiment Detection","summary":"  The use of transfer learning methods is largely responsible for the present\nbreakthrough in Natural Learning Processing (NLP) tasks across multiple\ndomains. In order to solve the problem of sentiment detection, we examined the\nperformance of four different types of well-known state-of-the-art transformer\nmodels for text classification. Models such as Bidirectional Encoder\nRepresentations from Transformers (BERT), Robustly Optimized BERT Pre-training\nApproach (RoBERTa), a distilled version of BERT (DistilBERT), and a large\nbidirectional neural network architecture (XLNet) were proposed. The\nperformance of the four models that were used to detect disaster in the text\nwas compared. All the models performed well enough, indicating that\ntransformer-based models are suitable for the detection of disaster in text.\nThe RoBERTa transformer model performs best on the test dataset with a score of\n82.6% and is highly recommended for quality predictions. Furthermore, we\ndiscovered that the learning algorithms' performance was influenced by the\npre-processing techniques, the nature of words in the vocabulary, unbalanced\nlabeling, and the model parameters.\n","authors":["Olumide Ebenezer Ojo","Hoang Thang Ta","Alexander Gelbukh","Hiram Calvo","Olaronke Oluwayemisi Adebanji","Grigori Sidorov"],"pdf_url":"https://arxiv.org/pdf/2303.07292v1.pdf","comment":"Publisher: Springer Nature Switzerland AG, Gewerbestrasse 11, 6330\n  Cham, Switzerland Published in Book Titled: Recent Developments and the New\n  Directions of Research, Foundations, and Applications: Selected Papers of the\n  8th World Conference on Soft Computing, February 03-05, 2022, Baku,\n  Azerbaijan"},{"id":"http://arxiv.org/abs/2303.07274v1","updated":"2023-03-13T16:49:43Z","published":"2023-03-13T16:49:43Z","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of\n  Synthetic and Compositional Images","summary":"  Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n","authors":["Nitzan Bitton-Guetta","Yonatan Bitton","Jack Hessel","Ludwig Schmidt","Yuval Elovici","Gabriel Stanovsky","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2303.07274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05737v2","updated":"2023-03-13T16:19:43Z","published":"2023-03-10T06:46:23Z","title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition\n  Performance in Clinical Settings","summary":"  Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 13 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n","authors":["Joel Shor","Ruyue Agnes Bi","Subhashini Venugopalan","Steven Ibara","Roman Goldenberg","Ehud Rivlin"],"pdf_url":"https://arxiv.org/pdf/2303.05737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07027v2","updated":"2023-03-13T16:14:52Z","published":"2023-02-14T13:09:23Z","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained\n  Language Models","summary":"  Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n","authors":["Alexandra Chronopoulou","Matthew E. Peters","Alexander Fraser","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2302.07027v2.pdf","comment":"Accepted at EACL 2023; camera-ready version"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2210.10488v4","updated":"2023-03-13T16:05:11Z","published":"2022-10-19T11:53:13Z","title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining\n  Perspective","summary":"  Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2210.10488v4.pdf","comment":"Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023"},{"id":"http://arxiv.org/abs/2303.07226v1","updated":"2023-03-13T16:00:31Z","published":"2023-03-13T16:00:31Z","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","summary":"  The field of natural language processing (NLP) has made significant strides\nin recent years, particularly in the development of large-scale vision-language\nmodels (VLMs). These models aim to bridge the gap between text and visual\ninformation, enabling a more comprehensive understanding of multimedia data.\nHowever, as these models become larger and more complex, they also become more\nchallenging to train and deploy. One approach to addressing this challenge is\nthe use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the\nmodel into smaller, specialized sub-models that can jointly solve a task. In\nthis paper, we explore the effectiveness of MoE in scaling vision-language\nmodels, demonstrating its potential to achieve state-of-the-art performance on\na range of benchmarks over dense models of equivalent computational cost. Our\nresearch offers valuable insights into stabilizing the training of MoE models,\nunderstanding the impact of MoE on model interpretability, and balancing the\ntrade-offs between compute performance when scaling VLMs. We hope our work will\ninspire further research into the use of MoE for scaling large-scale\nvision-language models and other multimodal machine learning applications.\n","authors":["Sheng Shen","Zhewei Yao","Chunyuan Li","Trevor Darrell","Kurt Keutzer","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2303.07226v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2211.05100v3","updated":"2023-03-13T15:55:30Z","published":"2022-11-09T18:48:09Z","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","summary":"  Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n","authors":["BigScience Workshop"," :","Teven Le Scao","Angela Fan","Christopher Akiki","Ellie Pavlick","Suzana Ilić","Daniel Hesslow","Roman Castagné","Alexandra Sasha Luccioni","François Yvon","Matthias Gallé","Jonathan Tow","Alexander M. Rush","Stella Biderman","Albert Webson","Pawan Sasanka Ammanamanchi","Thomas Wang","Benoît Sagot","Niklas Muennighoff","Albert Villanova del Moral","Olatunji Ruwase","Rachel Bawden","Stas Bekman","Angelina McMillan-Major","Iz Beltagy","Huu Nguyen","Lucile Saulnier","Samson Tan","Pedro Ortiz Suarez","Victor Sanh","Hugo Laurençon","Yacine Jernite","Julien Launay","Margaret Mitchell","Colin Raffel","Aaron Gokaslan","Adi Simhi","Aitor Soroa","Alham Fikri Aji","Amit Alfassy","Anna Rogers","Ariel Kreisberg Nitzav","Canwen Xu","Chenghao Mou","Chris Emezue","Christopher Klamm","Colin Leong","Daniel van Strien","David Ifeoluwa Adelani","Dragomir Radev","Eduardo González Ponferrada","Efrat Levkovizh","Ethan Kim","Eyal Bar Natan","Francesco De Toni","Gérard Dupont","Germán Kruszewski","Giada Pistilli","Hady Elsahar","Hamza Benyamina","Hieu Tran","Ian Yu","Idris Abdulmumin","Isaac Johnson","Itziar Gonzalez-Dios","Javier de la Rosa","Jenny Chim","Jesse Dodge","Jian Zhu","Jonathan Chang","Jörg Frohberg","Joseph Tobing","Joydeep Bhattacharjee","Khalid Almubarak","Kimbo Chen","Kyle Lo","Leandro Von Werra","Leon Weber","Long Phan","Loubna Ben allal","Ludovic Tanguy","Manan Dey","Manuel Romero Muñoz","Maraim Masoud","María Grandury","Mario Šaško","Max Huang","Maximin Coavoux","Mayank Singh","Mike Tian-Jian Jiang","Minh Chien Vu","Mohammad A. Jauhar","Mustafa Ghaleb","Nishant Subramani","Nora Kassner","Nurulaqilla Khamis","Olivier Nguyen","Omar Espejel","Ona de Gibert","Paulo Villegas","Peter Henderson","Pierre Colombo","Priscilla Amuok","Quentin Lhoest","Rheza Harliman","Rishi Bommasani","Roberto Luis López","Rui Ribeiro","Salomey Osei","Sampo Pyysalo","Sebastian Nagel","Shamik Bose","Shamsuddeen Hassan Muhammad","Shanya Sharma","Shayne Longpre","Somaieh Nikpoor","Stanislav Silberberg","Suhas Pai","Sydney Zink","Tiago Timponi Torrent","Timo Schick","Tristan Thrush","Valentin Danchev","Vassilina Nikoulina","Veronika Laippala","Violette Lepercq","Vrinda Prabhu","Zaid Alyafeai","Zeerak Talat","Arun Raja","Benjamin Heinzerling","Chenglei Si","Davut Emre Taşar","Elizabeth Salesky","Sabrina J. Mielke","Wilson Y. Lee","Abheesht Sharma","Andrea Santilli","Antoine Chaffin","Arnaud Stiegler","Debajyoti Datta","Eliza Szczechla","Gunjan Chhablani","Han Wang","Harshit Pandey","Hendrik Strobelt","Jason Alan Fries","Jos Rozen","Leo Gao","Lintang Sutawika","M Saiful Bari","Maged S. Al-shaibani","Matteo Manica","Nihal Nayak","Ryan Teehan","Samuel Albanie","Sheng Shen","Srulik Ben-David","Stephen H. Bach","Taewoon Kim","Tali Bers","Thibault Fevry","Trishala Neeraj","Urmish Thakker","Vikas Raunak","Xiangru Tang","Zheng-Xin Yong","Zhiqing Sun","Shaked Brody","Yallow Uri","Hadar Tojarieh","Adam Roberts","Hyung Won Chung","Jaesung Tae","Jason Phang","Ofir Press","Conglong Li","Deepak Narayanan","Hatim Bourfoune","Jared Casper","Jeff Rasley","Max Ryabinin","Mayank Mishra","Minjia Zhang","Mohammad Shoeybi","Myriam Peyrounette","Nicolas Patry","Nouamane Tazi","Omar Sanseviero","Patrick von Platen","Pierre Cornette","Pierre François Lavallée","Rémi Lacroix","Samyam Rajbhandari","Sanchit Gandhi","Shaden Smith","Stéphane Requena","Suraj Patil","Tim Dettmers","Ahmed Baruwa","Amanpreet Singh","Anastasia Cheveleva","Anne-Laure Ligozat","Arjun Subramonian","Aurélie Névéol","Charles Lovering","Dan Garrette","Deepak Tunuguntla","Ehud Reiter","Ekaterina Taktasheva","Ekaterina Voloshina","Eli Bogdanov","Genta Indra Winata","Hailey Schoelkopf","Jan-Christoph Kalo","Jekaterina Novikova","Jessica Zosa Forde","Jordan Clive","Jungo Kasai","Ken Kawamura","Liam Hazan","Marine Carpuat","Miruna Clinciu","Najoung Kim","Newton Cheng","Oleg Serikov","Omer Antverg","Oskar van der Wal","Rui Zhang","Ruochen Zhang","Sebastian Gehrmann","Shachar Mirkin","Shani Pais","Tatiana Shavrina","Thomas Scialom","Tian Yun","Tomasz Limisiewicz","Verena Rieser","Vitaly Protasov","Vladislav Mikhailov","Yada Pruksachatkun","Yonatan Belinkov","Zachary Bamberger","Zdeněk Kasner","Alice Rueda","Amanda Pestana","Amir Feizpour","Ammar Khan","Amy Faranak","Ana Santos","Anthony Hevia","Antigona Unldreaj","Arash Aghagol","Arezoo Abdollahi","Aycha Tammour","Azadeh HajiHosseini","Bahareh Behroozi","Benjamin Ajibade","Bharat Saxena","Carlos Muñoz Ferrandis","Danish Contractor","David Lansky","Davis David","Douwe Kiela","Duong A. Nguyen","Edward Tan","Emi Baylor","Ezinwanne Ozoani","Fatima Mirza","Frankline Ononiwu","Habib Rezanejad","Hessie Jones","Indrani Bhattacharya","Irene Solaiman","Irina Sedenko","Isar Nejadgholi","Jesse Passmore","Josh Seltzer","Julio Bonis Sanz","Livia Dutra","Mairon Samagaio","Maraim Elbadri","Margot Mieskes","Marissa Gerchick","Martha Akinlolu","Michael McKenna","Mike Qiu","Muhammed Ghauri","Mykola Burynok","Nafis Abrar","Nazneen Rajani","Nour Elkott","Nour Fahmy","Olanrewaju Samuel","Ran An","Rasmus Kromann","Ryan Hao","Samira Alizadeh","Sarmad Shubber","Silas Wang","Sourav Roy","Sylvain Viguier","Thanh Le","Tobi Oyebade","Trieu Le","Yoyo Yang","Zach Nguyen","Abhinav Ramesh Kashyap","Alfredo Palasciano","Alison Callahan","Anima Shukla","Antonio Miranda-Escalada","Ayush Singh","Benjamin Beilharz","Bo Wang","Caio Brito","Chenxi Zhou","Chirag Jain","Chuxin Xu","Clémentine Fourrier","Daniel León Periñán","Daniel Molano","Dian Yu","Enrique Manjavacas","Fabio Barth","Florian Fuhrimann","Gabriel Altay","Giyaseddin Bayrak","Gully Burns","Helena U. Vrabec","Imane Bello","Ishani Dash","Jihyun Kang","John Giorgi","Jonas Golde","Jose David Posada","Karthik Rangasai Sivaraman","Lokesh Bulchandani","Lu Liu","Luisa Shinzato","Madeleine Hahn de Bykhovetz","Maiko Takeuchi","Marc Pàmies","Maria A Castillo","Marianna Nezhurina","Mario Sänger","Matthias Samwald","Michael Cullan","Michael Weinberg","Michiel De Wolf","Mina Mihaljcic","Minna Liu","Moritz Freidank","Myungsun Kang","Natasha Seelam","Nathan Dahlberg","Nicholas Michio Broad","Nikolaus Muellner","Pascale Fung","Patrick Haller","Ramya Chandrasekhar","Renata Eisenberg","Robert Martin","Rodrigo Canalli","Rosaline Su","Ruisi Su","Samuel Cahyawijaya","Samuele Garda","Shlok S Deshmukh","Shubhanshu Mishra","Sid Kiblawi","Simon Ott","Sinee Sang-aroonsiri","Srishti Kumar","Stefan Schweter","Sushil Bharati","Tanmay Laud","Théo Gigant","Tomoya Kainuma","Wojciech Kusa","Yanis Labrak","Yash Shailesh Bajaj","Yash Venkatraman","Yifan Xu","Yingxin Xu","Yu Xu","Zhe Tan","Zhongli Xie","Zifan Ye","Mathilde Bras","Younes Belkada","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2211.05100v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07196v1","updated":"2023-03-13T15:34:19Z","published":"2023-03-13T15:34:19Z","title":"A Comprehensive Empirical Evaluation of Existing Word Embedding\n  Approaches","summary":"  Vector-based word representations help countless Natural Language Processing\n(NLP) tasks capture both semantic and syntactic regularities of the language.\nIn this paper, we present the characteristics of existing word embedding\napproaches and analyze them with regards to many classification tasks. We\ncategorize the methods into two main groups - Traditional approaches mostly use\nmatrix factorization to produce word representations, and they are not able to\ncapture the semantic and syntactic regularities of the language very well.\nNeural-Network based approaches, on the other hand, can capture sophisticated\nregularities of the language and preserve the word relationships in the\ngenerated word representations. We report experimental results on multiple\nclassification tasks and highlight the scenarios where one approach performs\nbetter than the rest.\n","authors":["Obaidullah Zaland","Muhammad Abulaish","Mohd. Fazil"],"pdf_url":"https://arxiv.org/pdf/2303.07196v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2211.16270v2","updated":"2023-03-13T14:18:42Z","published":"2022-11-29T14:57:23Z","title":"Neural Transducer Training: Reduced Memory Consumption with Sample-wise\n  Computation","summary":"  The neural transducer is an end-to-end model for automatic speech recognition\n(ASR). While the model is well-suited for streaming ASR, the training process\nremains challenging. During training, the memory requirements may quickly\nexceed the capacity of state-of-the-art GPUs, limiting batch size and sequence\nlengths. In this work, we analyze the time and space complexity of a typical\ntransducer training setup. We propose a memory-efficient training method that\ncomputes the transducer loss and gradients sample by sample. We present\noptimizations to increase the efficiency and parallelism of the sample-wise\nmethod. In a set of thorough benchmarks, we show that our sample-wise method\nsignificantly reduces memory usage, and performs at competitive speed when\ncompared to the default batched computation. As a highlight, we manage to\ncompute the transducer loss and gradients for a batch size of 1024, and audio\nlength of 40 seconds, using only 6 GB of memory.\n","authors":["Stefan Braun","Erik McDermott","Roger Hsiao"],"pdf_url":"https://arxiv.org/pdf/2211.16270v2.pdf","comment":"5 pages, 4 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2303.07146v1","updated":"2023-03-13T14:16:59Z","published":"2023-03-13T14:16:59Z","title":"NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective\n  Reasoning","summary":"  We present a new AI task and baseline solution for Inter-Subjective\nReasoning. We define inter-subjective information, to be a mixture of objective\nand subjective information possibly shared by different parties. Examples may\ninclude commodities and their objective properties as reported by IR\n(Information Retrieval) systems, that need to be cross-referenced with\nsubjective user reviews from an online forum. For an AI system to successfully\nreason about both, it needs to be able to combine symbolic reasoning of\nobjective facts with the shared consensus found on subjective user reviews. To\nthis end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as\na baseline solution for this problem. NeuroQL is a neuro-symbolic language that\nextends logical unification with neural primitives for extraction and\nretrieval. It can function as a target for automatic translation of\ninter-subjective questions (posed in natural language) into the neuro-symbolic\ncode that can answer them.\n","authors":["Nick Papoulias"],"pdf_url":"https://arxiv.org/pdf/2303.07146v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2211.05103v2","updated":"2023-03-13T13:35:10Z","published":"2022-11-09T18:53:59Z","title":"Accidental Learners: Spoken Language Identification in Multilingual\n  Self-Supervised Models","summary":"  In this paper, we extend previous self-supervised approaches for language\nidentification by experimenting with Conformer based architecture in a\nmultilingual pre-training paradigm. We find that pre-trained speech models\noptimally encode language discriminatory information in lower layers. Further,\nwe demonstrate that the embeddings obtained from these layers are significantly\nrobust to classify unseen languages and different acoustic environments without\nadditional training. After fine-tuning a pre-trained Conformer model on the\nVoxLingua107 dataset, we achieve results similar to current state-of-the-art\nsystems for language identification. More, our model accomplishes this with 5x\nless parameters. We open-source the model through the NVIDIA NeMo toolkit.\n","authors":["Travis M. Bartley","Fei Jia","Krishna C. Puvvada","Samuel Kriman","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2211.05103v2.pdf","comment":"Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.04029v2","updated":"2023-03-13T13:18:44Z","published":"2022-10-08T14:09:58Z","title":"EDU-level Extractive Summarization with Varying Summary Lengths","summary":"  Extractive models usually formulate text summarization as extracting fixed\ntop-$k$ salient sentences from the document as a summary. Few works exploited\nextracting finer-grained Elementary Discourse Unit (EDU) with little analysis\nand justification for the extractive unit selection. Further, the selection\nstrategy of the fixed top-$k$ salient sentences fits the summarization need\npoorly, as the number of salient sentences in different documents varies and\ntherefore a common or best $k$ does not exist in reality. To fill these gaps,\nthis paper first conducts the comparison analysis of oracle summaries based on\nEDUs and sentences, which provides evidence from both theoretical and\nexperimental perspectives to justify and quantify that EDUs make summaries with\nhigher automatic evaluation scores than sentences. Then, considering this merit\nof EDUs, this paper further proposes an EDU-level extractive model with Varying\nsummary Lengths and develops the corresponding learning algorithm. EDU-VL\nlearns to encode and predict probabilities of EDUs in the document, generate\nmultiple candidate summaries with varying lengths based on various $k$ values,\nand encode and score candidate summaries, in an end-to-end training manner.\nFinally, EDU-VL is experimented on single and multi-document benchmark datasets\nand shows improved performances on ROUGE scores in comparison with\nstate-of-the-art extractive models, and further human evaluation suggests that\nEDU-constituent summaries maintain good grammaticality and readability.\n","authors":["Yuping Wu","Ching-Hsun Tseng","Jiayu Shang","Shengzhong Mao","Goran Nenadic","Xiao-Jun Zeng"],"pdf_url":"https://arxiv.org/pdf/2210.04029v2.pdf","comment":"Accepted to EACL 2023 Findings"},{"id":"http://arxiv.org/abs/2303.07069v1","updated":"2023-03-13T12:45:01Z","published":"2023-03-13T12:45:01Z","title":"Generating multiple-choice questions for medical question answering with\n  distractors and cue-masking","summary":"  Medical multiple-choice question answering (MCQA) is particularly difficult.\nQuestions may describe patient symptoms and ask for the correct diagnosis,\nwhich requires domain knowledge and complex reasoning. Standard language\nmodeling pretraining alone is not sufficient to achieve the best results.\n\\citet{jin2020disease} showed that focusing masked language modeling on disease\nname prediction when using medical encyclopedic paragraphs as input leads to\nconsiderable MCQA accuracy improvement. In this work, we show that (1)\nfine-tuning on generated MCQA dataset outperforms the masked language modeling\nbased objective and (2) correctly masking the cues to the answers is critical\nfor good performance. We release new pretraining datasets and achieve\nstate-of-the-art results on 4 MCQA datasets, notably +5.7\\% with base-size\nmodel on MedQA-USMLE.\n","authors":["Damien Sileo","Kanimozhi Uma","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2303.07069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00258v2","updated":"2023-03-13T12:40:23Z","published":"2022-04-30T13:03:53Z","title":"EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language\n  Processing","summary":"  The success of Pre-Trained Models (PTMs) has reshaped the development of\nNatural Language Processing (NLP). Yet, it is not easy to obtain\nhigh-performing models and deploy them online for industrial practitioners. To\nbridge this gap, EasyNLP is designed to make it easy to build NLP applications,\nwhich supports a comprehensive suite of NLP algorithms. It further features\nknowledge-enhanced pre-training, knowledge distillation and few-shot learning\nfunctionalities for large-scale PTMs, and provides a unified framework of model\ntraining, inference and deployment for real-world applications. Currently,\nEasyNLP has powered over ten business units within Alibaba Group and is\nseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.\nThe source code of our EasyNLP toolkit is released at GitHub\n(https://github.com/alibaba/EasyNLP).\n","authors":["Chengyu Wang","Minghui Qiu","Chen Shi","Taolin Zhang","Tingting Liu","Lei Li","Jianing Wang","Ming Wang","Jun Huang","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2205.00258v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.00969v2","updated":"2023-03-13T12:14:30Z","published":"2023-03-02T05:06:44Z","title":"Rethinking the Reasonability of the Test Set for Simultaneous Machine\n  Translation","summary":"  Simultaneous machine translation (SimulMT) models start translation before\nthe end of the source sentence, making the translation monotonically aligned\nwith the source sentence. However, the general full-sentence translation test\nset is acquired by offline translation of the entire source sentence, which is\nnot designed for SimulMT evaluation, making us rethink whether this will\nunderestimate the performance of SimulMT models. In this paper, we manually\nannotate a monotonic test set based on the MuST-C English-Chinese test set,\ndenoted as SiMuST-C. Our human evaluation confirms the acceptability of our\nannotated test set. Evaluations on three different SimulMT models verify that\nthe underestimation problem can be alleviated on our test set. Further\nexperiments show that finetuning on an automatically extracted monotonic\ntraining set improves SimulMT models by up to 3 BLEU points.\n","authors":["Mengge Liu","Wen Zhang","Xiang Li","Jian Luan","Bin Wang","Yuhang Guo","Shuoying Chen"],"pdf_url":"https://arxiv.org/pdf/2303.00969v2.pdf","comment":"Accepted by 48th IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2023)"},{"id":"http://arxiv.org/abs/2303.07024v1","updated":"2023-03-13T11:41:28Z","published":"2023-03-13T11:41:28Z","title":"Addressing Biases in the Texts using an End-to-End Pipeline Approach","summary":"  The concept of fairness is gaining popularity in academia and industry.\nSocial media is especially vulnerable to media biases and toxic language and\ncomments. We propose a fair ML pipeline that takes a text as input and\ndetermines whether it contains biases and toxic content. Then, based on\npre-trained word embeddings, it suggests a set of new words by substituting the\nbi-ased words, the idea is to lessen the effects of those biases by replacing\nthem with alternative words. We compare our approach to existing fairness\nmodels to determine its effectiveness. The results show that our proposed\npipeline can de-tect, identify, and mitigate biases in social media data\n","authors":["Shaina Raza","Syed Raza Bashir"," Sneha","Urooj Qamar"],"pdf_url":"https://arxiv.org/pdf/2303.07024v1.pdf","comment":"Accepted in Bias @ ECIR 2023"},{"id":"http://arxiv.org/abs/2303.06944v1","updated":"2023-03-13T09:22:48Z","published":"2023-03-13T09:22:48Z","title":"A Human Subject Study of Named Entity Recognition (NER) in\n  Conversational Music Recommendation Queries","summary":"  We conducted a human subject study of named entity recognition on a noisy\ncorpus of conversational music recommendation queries, with many irregular and\nnovel named entities. We evaluated the human NER linguistic behaviour in these\nchallenging conditions and compared it with the most common NER systems\nnowadays, fine-tuned transformers. Our goal was to learn about the task to\nguide the design of better evaluation methods and NER algorithms. The results\nshowed that NER in our context was quite hard for both human and algorithms\nunder a strict evaluation schema; humans had higher precision, while the model\nhigher recall because of entity exposure especially during pre-training; and\nentity types had different error patterns (e.g. frequent typing errors for\nartists). The released corpus goes beyond predefined frames of interaction and\ncan support future work in conversational music recommendation.\n","authors":["Elena V. Epure","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2303.06944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04426v2","updated":"2023-03-13T08:43:27Z","published":"2023-03-08T08:08:57Z","title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker","summary":"  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n","authors":["Nicolas Heist","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2303.04426v2.pdf","comment":"Preprint of a paper in the research track of the 20th Extended\n  Semantic Web Conference (ESWC'23)"},{"id":"http://arxiv.org/abs/2303.06904v1","updated":"2023-03-13T07:46:41Z","published":"2023-03-13T07:46:41Z","title":"Contextually-rich human affect perception using multimodal scene\n  information","summary":"  The process of human affect understanding involves the ability to infer\nperson specific emotional states from various sources including images, speech,\nand language. Affect perception from images has predominantly focused on\nexpressions extracted from salient face crops. However, emotions perceived by\nhumans rely on multiple contextual cues including social settings, foreground\ninteractions, and ambient visual scenes. In this work, we leverage pretrained\nvision-language (VLN) models to extract descriptions of foreground context from\nimages. Further, we propose a multimodal context fusion (MCF) module to combine\nforeground cues with the visual scene and person-based contextual information\nfor emotion prediction. We show the effectiveness of our proposed modular\ndesign on two datasets associated with natural scenes and TV shows.\n","authors":["Digbalay Bose","Rajat Hebbar","Krishna Somandepalli","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2303.06904v1.pdf","comment":"Accepted to IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2023"},{"id":"http://arxiv.org/abs/2301.13294v2","updated":"2023-03-13T06:43:18Z","published":"2023-01-30T21:17:15Z","title":"Adaptive Machine Translation with Large Language Models","summary":"  Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, GPT-3.5 can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\n","authors":["Yasmin Moslem","Rejwanul Haque","John D. Kelleher","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2301.13294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07354v1","updated":"2023-03-13T06:39:38Z","published":"2023-03-13T06:39:38Z","title":"MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer\n  Adapters","summary":"  State-sponsored trolls are the main actors of influence campaigns on social\nmedia and automatic troll detection is important to combat misinformation at\nscale. Existing troll detection models are developed based on training data for\nknown campaigns (e.g.\\ the influence campaign by Russia's Internet Research\nAgency on the 2016 US Election), and they fall short when dealing with {\\em\nnovel} campaigns with new targets. We propose MetaTroll, a text-based troll\ndetection model based on the meta-learning framework that enables high\nportability and parameter-efficient adaptation to new campaigns using only a\nhandful of labelled samples for few-shot transfer. We introduce\n\\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''\ncampaign-specific knowledge so as to tackle catastrophic forgetting, where a\nmodel ``forgets'' how to detect trolls from older campaigns due to continual\nadaptation. Our experiments demonstrate that MetaTroll substantially\noutperforms baselines and state-of-the-art few-shot text classification models.\nLastly, we explore simple approaches to extend MetaTroll to multilingual and\nmultimodal detection. Source code for MetaTroll is available at:\nhttps://github.com/ltian678/metatroll-code.git.\n","authors":["Lin Tian","Xiuzhen Zhang","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2303.07354v1.pdf","comment":"11 pages, 2 figures, Accepted by the Web Conference 2023 (WWW 2023)"},{"id":"http://arxiv.org/abs/2303.06878v1","updated":"2023-03-13T05:53:42Z","published":"2023-03-13T05:53:42Z","title":"The System Description of dun_oscar team for The ICPR MSR Challenge","summary":"  This paper introduces the system submitted by dun_oscar team for the ICPR MSR\nChallenge. Three subsystems for task1-task3 are descripted respectively. In\ntask1, we develop a visual system which includes a OCR model, a text tracker,\nand a NLP classifier for distinguishing subtitles and non-subtitles. In task2,\nwe employ an ASR system which includes an AM with 18 layers and a 4-gram LM.\nSemi-supervised learning on unlabeled data is also vital. In task3, we employ\nthe ASR system to improve the visual system, some false subtitles can be\ncorrected by a fusion module.\n","authors":["Binbin Du","Rui Deng","Yingxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06574v3","updated":"2023-03-13T05:51:27Z","published":"2022-02-14T09:36:50Z","title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image\n  Captioning","summary":"  Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.\n","authors":["Ziyang Luo","Zhipeng Hu","Yadong Xi","Rongsheng Zhang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2202.06574v3.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06854v1","updated":"2023-03-13T04:49:46Z","published":"2023-03-13T04:49:46Z","title":"Robust Contrastive Language-Image Pretraining against Adversarial\n  Attacks","summary":"  Contrastive vision-language representation learning has achieved\nstate-of-the-art performance for zero-shot classification, by learning from\nmillions of image-caption pairs crawled from the internet. However, the massive\ndata that powers large multimodal models such as CLIP, makes them extremely\nvulnerable to various types of adversarial attacks, including targeted and\nbackdoor data poisoning attacks. Despite this vulnerability, robust contrastive\nvision-language pretraining against adversarial attacks has remained\nunaddressed. In this work, we propose RoCLIP, the first effective method for\nrobust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP\neffectively breaks the association between poisoned image-caption pairs by\nconsidering a pool of random examples, and (1) matching every image with the\ntext that is most similar to its caption in the pool, and (2) matching every\ncaption with the image that is most similar to its image in the pool. Our\nextensive experiments show that our method renders state-of-the-art targeted\ndata poisoning and backdoor attacks ineffective during pre-training or\nfine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor\nattack success rates down to 0\\% during pre-training and 1\\%-4\\% during\nfine-tuning, and effectively improves the model's performance.\n","authors":["Wenhan Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2303.06854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06841v1","updated":"2023-03-13T04:15:33Z","published":"2023-03-13T04:15:33Z","title":"Learning Transductions and Alignments with RNN Seq2seq Models","summary":"  The paper studies the capabilities of Recurrent-Neural-Network sequence to\nsequence (RNN seq2seq) models in learning four string-to-string transduction\ntasks: identity, reversal, total reduplication, and input-specified\nreduplication. These transductions are traditionally well studied under finite\nstate transducers and attributed with varying complexity. We find that RNN\nseq2seq models are only able to approximate a mapping that fits the training or\nin-distribution data. Attention helps significantly, but does not solve the\nout-of-distribution generalization limitation. Task complexity and RNN variants\nalso play a role in the results. Our results are best understood in terms of\nthe complexity hierarchy of formal languages as opposed to that of string\ntransductions.\n","authors":["Zhengxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06841v1.pdf","comment":"24 pages; 9 figures; 7 tables"},{"id":"http://arxiv.org/abs/2207.01063v3","updated":"2023-03-13T02:13:38Z","published":"2022-07-03T15:07:41Z","title":"DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech","summary":"  The majority of current Text-to-Speech (TTS) datasets, which are collections\nof individual utterances, contain few conversational aspects. In this paper, we\nintroduce DailyTalk, a high-quality conversational speech dataset designed for\nconversational TTS. We sampled, modified, and recorded 2,541 dialogues from the\nopen-domain dialogue dataset DailyDialog inheriting its annotated attributes.\nOn top of our dataset, we extend prior work as our baseline, where a\nnon-autoregressive TTS is conditioned on historical information in a dialogue.\nFrom the baseline experiment with both general and our novel metrics, we show\nthat DailyTalk can be used as a general TTS dataset, and more than that, our\nbaseline can represent contextual information from DailyTalk. The DailyTalk\ndataset and baseline code are freely available for academic use with CC-BY-SA\n4.0 license.\n","authors":["Keon Lee","Kyumin Park","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2207.01063v3.pdf","comment":"5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2204.00175v2","updated":"2023-03-13T01:49:33Z","published":"2022-04-01T02:51:22Z","title":"Alternate Intermediate Conditioning with Syllable-level and\n  Character-level Targets for Japanese ASR","summary":"  End-to-end automatic speech recognition directly maps input speech to\ncharacters. However, the mapping can be problematic when several different\npronunciations should be mapped into one character or when one pronunciation is\nshared among many different characters. Japanese ASR suffers the most from such\nmany-to-one and one-to-many mapping problems due to Japanese kanji characters.\nTo alleviate the problems, we introduce explicit interaction between characters\nand syllables using Self-conditioned connectionist temporal classification\n(CTC), in which the upper layers are ``self-conditioned'' on the intermediate\npredictions from the lower layers. The proposed method utilizes character-level\nand syllable-level intermediate predictions as conditioning features to deal\nwith mutual dependency between characters and syllables. Experimental results\non Corpus of Spontaneous Japanese show that the proposed method outperformed\nthe conventional multi-task and Self-conditioned CTC methods.\n","authors":["Yusuke Fujita","Tatsuya Komatsu","Yusuke Kida"],"pdf_url":"https://arxiv.org/pdf/2204.00175v2.pdf","comment":"SLT 2022"},{"id":"http://arxiv.org/abs/2105.11115v3","updated":"2023-03-13T01:47:55Z","published":"2021-05-24T06:42:58Z","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages","summary":"  Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.\n","authors":["Shunyu Yao","Binghui Peng","Christos Papadimitriou","Karthik Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2105.11115v3.pdf","comment":"ACL 2021. 19 pages with extended appendix. Fixed a small typo in the\n  formula at the end of page 5 (thank to Gabriel Faria). Code:\n  https://github.com/princeton-nlp/dyck-transformer"},{"id":"http://arxiv.org/abs/2303.06806v1","updated":"2023-03-13T01:28:55Z","published":"2023-03-13T01:28:55Z","title":"Neural Diarization with Non-autoregressive Intermediate Attractors","summary":"  End-to-end neural diarization (EEND) with encoder-decoder-based attractors\n(EDA) is a promising method to handle the whole speaker diarization problem\nsimultaneously with a single neural network. While the EEND model can produce\nall frame-level speaker labels simultaneously, it disregards output label\ndependency. In this work, we propose a novel EEND model that introduces the\nlabel dependency between frames. The proposed method generates\nnon-autoregressive intermediate attractors to produce speaker labels at the\nlower layers and conditions the subsequent layers with these labels. While the\nproposed model works in a non-autoregressive manner, the speaker labels are\nrefined by referring to the whole sequence of intermediate labels. The\nexperiments with the two-speaker CALLHOME dataset show that the intermediate\nlabels with the proposed non-autoregressive intermediate attractors boost the\ndiarization performance. The proposed method with the deeper network benefits\nmore from the intermediate labels, resulting in better performance and training\nthroughput than EEND-EDA.\n","authors":["Yusuke Fujita","Tatsuya Komatsu","Robin Scheibler","Yusuke Kida","Tetsuji Ogawa"],"pdf_url":"https://arxiv.org/pdf/2303.06806v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06791v1","updated":"2023-03-13T00:39:04Z","published":"2023-03-13T00:39:04Z","title":"Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset","summary":"  Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.\n","authors":["Arun Tejasvi Chaganty","Megan Leszczynski","Shu Zhang","Ravi Ganti","Krisztian Balog","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2303.06791v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.07530v1","updated":"2023-03-13T23:35:14Z","published":"2023-03-13T23:35:14Z","title":"Towards Unsupervised Learning based Denoising of Cyber Physical System\n  Data to Mitigate Security Concerns","summary":"  A dataset, collected under an industrial setting, often contains a\nsignificant portion of noises. In many cases, using trivial filters is not\nenough to retrieve useful information i.e., accurate value without the noise.\nOne such data is time-series sensor readings collected from moving vehicles\ncontaining fuel information. Due to the noisy dynamics and mobile environment,\nthe sensor readings can be very noisy. Denoising such a dataset is a\nprerequisite for any useful application and security issues. Security is a\nprimitive concern in present vehicular schemes. The server side for retrieving\nthe fuel information can be easily hacked. Providing the accurate and noise\nfree fuel information via vehicular networks become crutial. Therefore, it has\nled us to develop a system that can remove noise and keep the original value.\nThe system is also helpful for vehicle industry, fuel station, and power-plant\nstation that require fuel. In this work, we have only considered the value of\nfuel level, and we have come up with a unique solution to filter out the noise\nof high magnitudes using several algorithms such as interpolation,\nextrapolation, spectral clustering, agglomerative clustering, wavelet analysis,\nand median filtering. We have also employed peak detection and peak validation\nalgorithms to detect fuel refill and consumption in charge-discharge cycles. We\nhave used the R-squared metric to evaluate our model, and it is 98 percent In\nmost cases, the difference between detected value and real value remains within\nthe range of 1L.\n","authors":["Mst Shapna Akter","Hossain Shahriar"],"pdf_url":"https://arxiv.org/pdf/2303.07530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07527v1","updated":"2023-03-13T23:30:48Z","published":"2023-03-13T23:30:48Z","title":"Domain Generalization via Nuclear Norm Regularization","summary":"  The ability to generalize to unseen domains is crucial for machine learning\nsystems deployed in the real world, especially when we only have data from\nlimited training domains. In this paper, we propose a simple and effective\nregularization method based on the nuclear norm of the learned features for\ndomain generalization. Intuitively, the proposed regularizer mitigates the\nimpacts of environmental features and encourages learning domain-invariant\nfeatures. Theoretically, we provide insights into why nuclear norm\nregularization is more effective compared to ERM and alternative regularization\nmethods. Empirically, we conduct extensive experiments on both synthetic and\nreal datasets. We show that nuclear norm regularization achieves strong\nperformance compared to baselines in a wide range of domain generalization\ntasks. Moreover, our regularizer is broadly applicable with various methods\nsuch as ERM and SWAD with consistently improved performance, e.g., 1.7% and\n0.9% test accuracy improvements respectively on the DomainBed benchmark.\n","authors":["Zhenmei Shi","Yifei Ming","Ying Fan","Frederic Sala","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2303.07527v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.07525v1","updated":"2023-03-13T23:27:42Z","published":"2023-03-13T23:27:42Z","title":"Automated Vulnerability Detection in Source Code Using Quantum Natural\n  Language Processing","summary":"  One of the most important challenges in the field of software code audit is\nthe presence of vulnerabilities in software source code. These flaws are highly\nlikely ex-ploited and lead to system compromise, data leakage, or denial of\nser-vice. C and C++ open source code are now available in order to create a\nlarge-scale, classical machine-learning and quantum machine-learning system for\nfunction-level vulnerability identification. We assembled a siz-able dataset of\nmillions of open-source functions that point to poten-tial exploits. We created\nan efficient and scalable vulnerability detection method based on a deep neural\nnetwork model Long Short Term Memory (LSTM), and quantum machine learning model\nLong Short Term Memory (QLSTM), that can learn features extracted from the\nsource codes. The source code is first converted into a minimal intermediate\nrepresentation to remove the pointless components and shorten the de-pendency.\nTherefore, We keep the semantic and syntactic information using state of the\nart word embedding algorithms such as Glove and fastText. The embedded vectors\nare subsequently fed into the classical and quantum convolutional neural\nnetworks to classify the possible vulnerabilities. To measure the performance,\nwe used evaluation metrics such as F1 score, precision, re-call, accuracy, and\ntotal execution time. We made a comparison between the results derived from the\nclassical LSTM and quantum LSTM using basic feature representation as well as\nsemantic and syntactic represen-tation. We found that the QLSTM with semantic\nand syntactic features detects significantly accurate vulnerability and runs\nfaster than its classical counterpart.\n","authors":["Mst Shapna Akter","Hossain Shahriar","Zakirul Alam Bhuiya"],"pdf_url":"https://arxiv.org/pdf/2303.07525v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.12230v2","updated":"2023-03-13T18:13:44Z","published":"2022-12-23T10:01:14Z","title":"Recommending on graphs: a comprehensive review from a data perspective","summary":"  Recent advances in graph-based learning approaches have demonstrated their\neffectiveness in modelling users' preferences and items' characteristics for\nRecommender Systems (RSS). Most of the data in RSS can be organized into graphs\nwhere various objects (e.g., users, items, and attributes) are explicitly or\nimplicitly connected and influence each other via various relations. Such a\ngraph-based organization brings benefits to exploiting potential properties in\ngraph learning (e.g., random walk and network embedding) techniques to enrich\nthe representations of the user and item nodes, which is an essential factor\nfor successful recommendations. In this paper, we provide a comprehensive\nsurvey of Graph Learning-based Recommender Systems (GLRSs). Specifically, we\nstart from a data-driven perspective to systematically categorize various\ngraphs in GLRSs and analyze their characteristics. Then, we discuss the\nstate-of-the-art frameworks with a focus on the graph learning module and how\nthey address practical recommendation challenges such as scalability, fairness,\ndiversity, explainability and so on. Finally, we share some potential research\ndirections in this rapidly growing area.\n","authors":["Lemei Zhang","Peng Liu","Jon Atle Gulla"],"pdf_url":"https://arxiv.org/pdf/2212.12230v2.pdf","comment":"Accepted by UMUAI"},{"id":"http://arxiv.org/abs/2303.07001v1","updated":"2023-03-13T10:59:38Z","published":"2023-03-13T10:59:38Z","title":"Neural Group Recommendation Based on a Probabilistic Semantic\n  Aggregation","summary":"  Recommendation to groups of users is a challenging subfield of recommendation\nsystems. Its key concept is how and where to make the aggregation of each set\nof user information into an individual entity, such as a ranked recommendation\nlist, a virtual user, or a multi-hot input vector encoding. This paper proposes\nan innovative strategy where aggregation is made in the multi-hot vector that\nfeeds the neural network model. The aggregation provides a probabilistic\nsemantic, and the resulting input vectors feed a model that is able to\nconveniently generalize the group recommendation from the individual\npredictions. Furthermore, using the proposed architecture, group\nrecommendations can be obtained by simply feedforwarding the pre-trained model\nwith individual ratings; that is, without the need to obtain datasets\ncontaining group of user information, and without the need of running two\nseparate trainings (individual and group). This approach also avoids\nmaintaining two different models to support both individual and group learning.\nExperiments have tested the proposed architecture using three representative\ncollaborative filtering datasets and a series of baselines; results show\nsuitable accuracy improvements compared to the state-of-the-art.\n","authors":["Jorge Dueñas-Lerín","Raúl Lara-Cabrera","Fernando Ortega","Jesús Bobadilla"],"pdf_url":"https://arxiv.org/pdf/2303.07001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04426v2","updated":"2023-03-13T08:43:27Z","published":"2023-03-08T08:08:57Z","title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker","summary":"  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n","authors":["Nicolas Heist","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2303.04426v2.pdf","comment":"Preprint of a paper in the research track of the 20th Extended\n  Semantic Web Conference (ESWC'23)"},{"id":"http://arxiv.org/abs/2303.06791v1","updated":"2023-03-13T00:39:04Z","published":"2023-03-13T00:39:04Z","title":"Beyond Single Items: Exploring User Preferences in Item Sets with the\n  Conversational Playlist Curation Dataset","summary":"  Users in consumption domains, like music, are often able to more efficiently\nprovide preferences over a set of items (e.g. a playlist or radio) than over\nsingle items (e.g. songs). Unfortunately, this is an underexplored area of\nresearch, with most existing recommendation systems limited to understanding\npreferences over single items. Curating an item set exponentiates the search\nspace that recommender systems must consider (all subsets of items!): this\nmotivates conversational approaches-where users explicitly state or refine\ntheir preferences and systems elicit preferences in natural language-as an\nefficient way to understand user needs. We call this task conversational item\nset curation and present a novel data collection methodology that efficiently\ncollects realistic preferences about item sets in a conversational setting by\nobserving both item-level and set-level feedback. We apply this methodology to\nmusic recommendation to build the Conversational Playlist Curation Dataset\n(CPCD), where we show that it leads raters to express preferences that would\nnot be otherwise expressed. Finally, we propose a wide range of conversational\nretrieval models as baselines for this task and evaluate them on the dataset.\n","authors":["Arun Tejasvi Chaganty","Megan Leszczynski","Shu Zhang","Ravi Ganti","Krisztian Balog","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2303.06791v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.07538v1","updated":"2023-03-13T23:49:09Z","published":"2023-03-13T23:49:09Z","title":"HiSSNet: Sound Event Detection and Speaker Identification via\n  Hierarchical Prototypical Networks for Low-Resource Headphones","summary":"  Modern noise-cancelling headphones have significantly improved users'\nauditory experiences by removing unwanted background noise, but they can also\nblock out sounds that matter to users. Machine learning (ML) models for sound\nevent detection (SED) and speaker identification (SID) can enable headphones to\nselectively pass through important sounds; however, implementing these models\nfor a user-centric experience presents several unique challenges. First, most\npeople spend limited time customizing their headphones, so the sound detection\nshould work reasonably well out of the box. Second, the models should be able\nto learn over time the specific sounds that are important to users based on\ntheir implicit and explicit interactions. Finally, such models should have a\nsmall memory footprint to run on low-power headphones with limited on-chip\nmemory. In this paper, we propose addressing these challenges using HiSSNet\n(Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model that\nuses a hierarchical prototypical network to detect both general and specific\nsounds of interest and characterize both alarm-like and speech sounds. We show\nthat HiSSNet outperforms an SEID model trained using non-hierarchical\nprototypical networks by 6.9 - 8.6 percent. When compared to state-of-the-art\n(SOTA) models trained specifically for SED or SID alone, HiSSNet achieves\nsimilar or better performance while reducing the memory footprint required to\nsupport multiple capabilities on-device.\n","authors":["N Shashaank","Berker Banar","Mohammad Rasool Izadi","Jeremy Kemmerer","Shuo Zhang"," Chuan-Che"," Huang"],"pdf_url":"https://arxiv.org/pdf/2303.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07537v1","updated":"2023-03-13T23:47:46Z","published":"2023-03-13T23:47:46Z","title":"Fractional dynamics foster deep learning of COPD stage prediction","summary":"  Chronic obstructive pulmonary disease (COPD) is one of the leading causes of\ndeath worldwide. Current COPD diagnosis (i.e., spirometry) could be unreliable\nbecause the test depends on an adequate effort from the tester and testee.\nMoreover, the early diagnosis of COPD is challenging. We address COPD detection\nby constructing two novel physiological signals datasets (4432 records from 54\npatients in the WestRo COPD dataset and 13824 medical records from 534 patients\nin the WestRo Porti COPD dataset). The authors demonstrate their complex\ncoupled fractal dynamical characteristics and perform a fractional-order\ndynamics deep learning analysis to diagnose COPD. The authors found that the\nfractional-order dynamical modeling can extract distinguishing signatures from\nthe physiological signals across patients with all COPD stages from stage 0\n(healthy) to stage 4 (very severe). They use the fractional signatures to\ndevelop and train a deep neural network that predicts COPD stages based on the\ninput features (such as thorax breathing effort, respiratory rate, or oxygen\nsaturation). The authors show that the fractional dynamic deep learning model\n(FDDLM) achieves a COPD prediction accuracy of 98.66% and can serve as a robust\nalternative to spirometry. The FDDLM also has high accuracy when validated on a\ndataset with different physiological signals.\n","authors":["Chenzhong Yin","Mihai Udrescu","Gaurav Gupta","Mingxi Cheng","Andrei Lihu","Lucretia Udrescu","Paul Bogdan","David M Mannino","Stefan Mihaicuta"],"pdf_url":"https://arxiv.org/pdf/2303.07537v1.pdf","comment":"Published on Advanced Science"},{"id":"http://arxiv.org/abs/2303.07535v1","updated":"2023-03-13T23:44:40Z","published":"2023-03-13T23:44:40Z","title":"Path Planning using Reinforcement Learning: A Policy Iteration Approach","summary":"  With the impact of real-time processing being realized in the recent past,\nthe need for efficient implementations of reinforcement learning algorithms has\nbeen on the rise. Albeit the numerous advantages of Bellman equations utilized\nin RL algorithms, they are not without the large search space of design\nparameters.\n  This research aims to shed light on the design space exploration associated\nwith reinforcement learning parameters, specifically that of Policy Iteration.\nGiven the large computational expenses of fine-tuning the parameters of\nreinforcement learning algorithms, we propose an auto-tuner-based ordinal\nregression approach to accelerate the process of exploring these parameters\nand, in return, accelerate convergence towards an optimal policy. Our approach\nprovides 1.82x peak speedup with an average of 1.48x speedup over the previous\nstate-of-the-art.\n","authors":["Saumil Shivdikar","Jagannath Nirmal"],"pdf_url":"https://arxiv.org/pdf/2303.07535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.09127v3","updated":"2023-03-13T23:43:41Z","published":"2022-06-18T06:10:13Z","title":"Nonparametric Multi-shape Modeling with Uncertainty Quantification","summary":"  The modeling and uncertainty quantification of closed curves is an important\nproblem in the field of shape analysis, and can have significant ramifications\nfor subsequent statistical tasks. Many of these tasks involve collections of\nclosed curves, which often exhibit structural similarities at multiple levels.\nModeling multiple closed curves in a way that efficiently incorporates such\nbetween-curve dependence remains a challenging problem. In this work, we\npropose and investigate a multiple-output (a.k.a. multi-output),\nmulti-dimensional Gaussian process modeling framework. We illustrate the\nproposed methodological advances, and demonstrate the utility of meaningful\nuncertainty quantification, on several curve and shape-related tasks. This\nmodel-based approach not only addresses the problem of inference on closed\ncurves (and their shapes) with kernel constructions, but also opens doors to\nnonparametric modeling of multi-level dependence for functional objects in\ngeneral.\n","authors":["Hengrui Luo","Justin D. Strait"],"pdf_url":"https://arxiv.org/pdf/2206.09127v3.pdf","comment":"66 pages, 20 figures"},{"id":"http://arxiv.org/abs/2303.07527v1","updated":"2023-03-13T23:30:48Z","published":"2023-03-13T23:30:48Z","title":"Domain Generalization via Nuclear Norm Regularization","summary":"  The ability to generalize to unseen domains is crucial for machine learning\nsystems deployed in the real world, especially when we only have data from\nlimited training domains. In this paper, we propose a simple and effective\nregularization method based on the nuclear norm of the learned features for\ndomain generalization. Intuitively, the proposed regularizer mitigates the\nimpacts of environmental features and encourages learning domain-invariant\nfeatures. Theoretically, we provide insights into why nuclear norm\nregularization is more effective compared to ERM and alternative regularization\nmethods. Empirically, we conduct extensive experiments on both synthetic and\nreal datasets. We show that nuclear norm regularization achieves strong\nperformance compared to baselines in a wide range of domain generalization\ntasks. Moreover, our regularizer is broadly applicable with various methods\nsuch as ERM and SWAD with consistently improved performance, e.g., 1.7% and\n0.9% test accuracy improvements respectively on the DomainBed benchmark.\n","authors":["Zhenmei Shi","Yifei Ming","Ying Fan","Frederic Sala","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2303.07527v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.07525v1","updated":"2023-03-13T23:27:42Z","published":"2023-03-13T23:27:42Z","title":"Automated Vulnerability Detection in Source Code Using Quantum Natural\n  Language Processing","summary":"  One of the most important challenges in the field of software code audit is\nthe presence of vulnerabilities in software source code. These flaws are highly\nlikely ex-ploited and lead to system compromise, data leakage, or denial of\nser-vice. C and C++ open source code are now available in order to create a\nlarge-scale, classical machine-learning and quantum machine-learning system for\nfunction-level vulnerability identification. We assembled a siz-able dataset of\nmillions of open-source functions that point to poten-tial exploits. We created\nan efficient and scalable vulnerability detection method based on a deep neural\nnetwork model Long Short Term Memory (LSTM), and quantum machine learning model\nLong Short Term Memory (QLSTM), that can learn features extracted from the\nsource codes. The source code is first converted into a minimal intermediate\nrepresentation to remove the pointless components and shorten the de-pendency.\nTherefore, We keep the semantic and syntactic information using state of the\nart word embedding algorithms such as Glove and fastText. The embedded vectors\nare subsequently fed into the classical and quantum convolutional neural\nnetworks to classify the possible vulnerabilities. To measure the performance,\nwe used evaluation metrics such as F1 score, precision, re-call, accuracy, and\ntotal execution time. We made a comparison between the results derived from the\nclassical LSTM and quantum LSTM using basic feature representation as well as\nsemantic and syntactic represen-tation. We found that the QLSTM with semantic\nand syntactic features detects significantly accurate vulnerability and runs\nfaster than its classical counterpart.\n","authors":["Mst Shapna Akter","Hossain Shahriar","Zakirul Alam Bhuiya"],"pdf_url":"https://arxiv.org/pdf/2303.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07522v1","updated":"2023-03-13T23:17:51Z","published":"2023-03-13T23:17:51Z","title":"Audio Visual Language Maps for Robot Navigation","summary":"  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n","authors":["Chenguang Huang","Oier Mees","Andy Zeng","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2303.07522v1.pdf","comment":"Project page: https://avlmaps.github.io/"},{"id":"http://arxiv.org/abs/2303.07519v1","updated":"2023-03-13T23:11:05Z","published":"2023-03-13T23:11:05Z","title":"Architext: Language-Driven Generative Architecture Design","summary":"  Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100\\%\nrate. Accuracy shows great improvement when scaling the models, with the\nlargest model (GPT-J) yielding impressive accuracy ranging between 25% to over\n80% for different prompt categories. We open source the finetuned Architext\nmodels and our synthetic dataset, hoping to inspire experimentation in this\nexciting area of design research.\n","authors":["Theodoros Galanos","Antonios Liapis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2303.07519v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.07523v1","updated":"2023-03-13T23:18:17Z","published":"2023-03-13T23:18:17Z","title":"Investigating the Characteristics and Performance of Augmented Reality\n  Applications on Head-Mounted Displays: A Study of the Hololens Application\n  Store","summary":"  Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained\nsignificant traction over the recent years. Nevertheless, it remains unclear\nwhat AR HMD-based applications have been developed over the years and what\ntheir system performance is when they are run on HMDs. In this paper, we aim to\nshed light into this direction. Our study focuses on the applications available\non the Microsoft Hololens application store given the wide use of the Hololens\nheadset. Our study has two major parts: (i) we collect metadata about the\napplications available on the Microsoft Hololens application store to\nunderstand their characteristics (e.g., categories, pricing, permissions\nrequested, hardware and software compatibility); and (ii) we interact with\nthese applications while running on a Hololens 2 headset and collect data about\nsystems-related metrics (e.g., memory and storage usage, time spent on CPU and\nGPU related operations) to investigate the systems performance of applications.\nOur study has resulted in several interesting findings, which we share with the\nresearch community.\n","authors":["Pubudu Wijesooriya","Sheikh Muhammad Farjad","Nikolaos Stergiou","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2303.07523v1.pdf","comment":"This paper has been accepted for publication by IEEE ICC workshops\n  2023"},{"id":"http://arxiv.org/abs/2303.07347v1","updated":"2023-03-13T17:59:59Z","published":"2023-03-13T17:59:59Z","title":"TriDet: Temporal Action Detection with Relative Boundary Modeling","summary":"  In this paper, we present a one-stage framework TriDet for temporal action\ndetection. Existing methods often suffer from imprecise boundary predictions\ndue to the ambiguous action boundaries in videos. To alleviate this problem, we\npropose a novel Trident-head to model the action boundary via an estimated\nrelative probability distribution around the boundary. In the feature pyramid\nof TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer\nto mitigate the rank loss problem of self-attention that takes place in the\nvideo features and aggregate information across different temporal\ngranularities. Benefiting from the Trident-head and the SGP-based feature\npyramid, TriDet achieves state-of-the-art performance on three challenging\nbenchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational\ncosts, compared to previous methods. For example, TriDet hits an average mAP of\n$69.3\\%$ on THUMOS14, outperforming the previous best by $2.5\\%$, but with only\n$74.6\\%$ of its latency. The code is released to\nhttps://github.com/sssste/TriDet.\n","authors":["Dingfeng Shi","Yujie Zhong","Qiong Cao","Lin Ma","Jia Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.07347v1.pdf","comment":"CVPR2023; Temporal Action Detection; Temporal Action Localization"},{"id":"http://arxiv.org/abs/2303.07240v1","updated":"2023-03-13T16:13:16Z","published":"2023-03-13T16:13:16Z","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical\n  Documents","summary":"  Foundation models trained on large-scale dataset gain a recent surge in CV\nand NLP. In contrast, development in biomedical domain lags far behind due to\ndata scarcity. To address this issue, we build and release PMC-OA, a biomedical\ndataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess\nsubset, which is 8 times larger than before. PMC-OA covers diverse modalities\nor diseases, with majority of the image-caption samples aligned at\nfiner-grained level, i.e., subfigure and subcaption. While pretraining a\nCLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art\nresults on various downstream tasks, including image-text retrieval on ROCO,\nMedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text\nretrieval, +3.9% accuracy on image classification.\n","authors":["Weixiong Lin","Ziheng Zhao","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2303.07240v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2201.08071v3","updated":"2023-03-13T13:55:30Z","published":"2022-01-20T09:10:20Z","title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions","summary":"  Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n","authors":["Hao Zhang","Aixin Sun","Wei Jing","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.08071v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2202.03861v4","updated":"2023-03-13T07:29:12Z","published":"2022-02-08T13:46:42Z","title":"Towards Making a Trojan-horse Attack on Text-to-Image Retrieval","summary":"  While deep learning based image retrieval is reported to be vulnerable to\nadversarial attacks, existing works are mainly on image-to-image retrieval with\ntheir attacks performed at the front end via query modification. By contrast,\nwe present in this paper the first study about a threat that occurs at the back\nend of a text-to-image retrieval (T2IR) system. Our study is motivated by the\nfact that the image collection indexed by the system will be regularly updated\ndue to the arrival of new images from various sources such as web crawlers and\nadvertisers. With malicious images indexed, it is possible for an attacker to\nindirectly interfere with the retrieval process, letting users see certain\nimages that are completely irrelevant w.r.t. their queries. We put this thought\ninto practice by proposing a novel Trojan-horse attack (THA). In particular, we\nconstruct a set of Trojan-horse images by first embedding word-specific\nadversarial information into a QR code and then putting the code on benign\nadvertising images. A proof-of-concept evaluation, conducted on two popular\nT2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed\nTHA in a white-box mode.\n","authors":["Fan Hu","Aozhu Chen","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2202.03861v4.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2206.07331v2","updated":"2023-03-13T06:52:04Z","published":"2022-06-15T07:26:27Z","title":"ETMA: Efficient Transformer Based Multilevel Attention framework for\n  Multimodal Fake News Detection","summary":"  In this new digital era, social media has created a severe impact on the\nlives of people. In recent times, fake news content on social media has become\none of the major challenging problems for society. The dissemination of\nfabricated and false news articles includes multimodal data in the form of text\nand images. The previous methods have mainly focused on unimodal analysis.\nMoreover, for multimodal analysis, researchers fail to keep the unique\ncharacteristics corresponding to each modality. This paper aims to overcome\nthese limitations by proposing an Efficient Transformer based Multilevel\nAttention (ETMA) framework for multimodal fake news detection, which comprises\nthe following components: visual attention-based encoder, textual\nattention-based encoder, and joint attention-based learning. Each component\nutilizes the different forms of attention mechanism and uniquely deals with\nmultimodal data to detect fraudulent content. The efficacy of the proposed\nnetwork is validated by conducting several experiments on four real-world fake\nnews datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,\nand Risdal Fake News Dataset using multiple evaluation metrics. The results\nshow that the proposed method outperforms the baseline methods on all four\ndatasets. Further, the computation time of the model is also lower than the\nstate-of-the-art methods.\n","authors":["Ashima Yadav","Shivani Gaba","Haneef Khan","Ishan Budhiraja","Akansha Singh","Krishan Kant Singh"],"pdf_url":"https://arxiv.org/pdf/2206.07331v2.pdf","comment":"Accepted for publication in IEEE Transactions on Computational Social\n  Systems"},{"id":"http://arxiv.org/abs/2303.06859v1","updated":"2023-03-13T05:04:18Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Casual-IRDIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v1.pdf","comment":"Accepted by CVPR2023"}]},"2023-03-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2210.15173v2","updated":"2023-03-12T20:28:46Z","published":"2022-10-27T05:07:04Z","title":"Articulation GAN: Unsupervised modeling of articulatory learning","summary":"  Generative deep neural networks are widely used for speech synthesis, but\nmost existing models directly generate waveforms or spectral outputs. Humans,\nhowever, produce speech by controlling articulators, which results in the\nproduction of speech sounds through physical properties of sound propagation.\nWe introduce the Articulatory Generator to the Generative Adversarial Network\nparadigm, a new unsupervised generative model of speech production/synthesis.\nThe Articulatory Generator more closely mimics human speech production by\nlearning to generate articulatory representations (electromagnetic\narticulography or EMA) in a fully unsupervised manner. A separate pre-trained\nphysical model (ema2wav) then transforms the generated EMA representations to\nspeech waveforms, which get sent to the Discriminator for evaluation.\nArticulatory analysis suggests that the network learns to control articulators\nin a similar manner to humans during speech production. Acoustic analysis of\nthe outputs suggests that the network learns to generate words that are both\npresent and absent in the training distribution. We additionally discuss\nimplications of articulatory representations for cognitive models of human\nlanguage and speech technology in general.\n","authors":["Gašper Beguš","Alan Zhou","Peter Wu","Gopala K Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2210.15173v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2110.05006v3","updated":"2023-03-12T17:56:44Z","published":"2021-10-11T05:30:30Z","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey","summary":"  Pre-trained language models (PLMs) have been the de facto paradigm for most\nnatural language processing (NLP) tasks. This also benefits biomedical domain:\nresearchers from informatics, medicine, and computer science (CS) communities\npropose various PLMs trained on biomedical datasets, e.g., biomedical text,\nelectronic health records, protein, and DNA sequences for various biomedical\ntasks. However, the cross-discipline characteristics of biomedical PLMs hinder\ntheir spreading among communities; some existing works are isolated from each\nother without comprehensive comparison and discussions. It expects a survey\nthat not only systematically reviews recent advances of biomedical PLMs and\ntheir applications but also standardizes terminology and benchmarks. In this\npaper, we summarize the recent progress of pre-trained language models in the\nbiomedical domain and their applications in biomedical downstream tasks.\nParticularly, we discuss the motivations and propose a taxonomy of existing\nbiomedical PLMs. Their applications in biomedical downstream tasks are\nexhaustively discussed. At last, we illustrate various limitations and future\ntrends, which we hope can provide inspiration for the future research of the\nresearch community.\n","authors":["Benyou Wang","Qianqian Xie","Jiahuan Pei","Zhihong Chen","Prayag Tiwari","Zhao Li","Jie fu"],"pdf_url":"https://arxiv.org/pdf/2110.05006v3.pdf","comment":"An improved version"},{"id":"http://arxiv.org/abs/2302.07267v3","updated":"2023-03-12T17:32:22Z","published":"2023-02-13T17:57:50Z","title":"\"Correct answers\" from the psychology of artificial intelligence","summary":"  Large Language Models have vastly grown in capabilities. One proposed\napplication of such AI systems is to support data collection in the social and\ncognitive sciences, where perfect experimental control is currently unfeasible\nand the collection of large, representative datasets is generally expensive. In\nthis paper, we re-replicate 14 studies from the Many Labs 2 replication project\nwith OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We\ncollected responses from the default setting of GPT3.5 by inputting each\nstudy's survey as text. Among the eight studies we could analyse, our GPT\nsample replicated 37.5% of the original results as well as 37.5% of the Many\nLabs 2 results. Unexpectedly, we could not analyse the remaining six studies as\nwe had planned in our pre-registration. This was because for each of these six\nstudies, GPT3.5 answered at least one of the survey questions (either a\ndependent variable or a condition variable) in an extremely predetermined way:\nan unexpected phenomenon we call the \"correct answer\" effect. Different runs of\nGPT3.5 answered nuanced questions probing political orientation, economic\npreference, judgement, and moral philosophy with zero or near-zero variation in\nresponses: with the supposedly \"correct answer.\" For example, our survey\nquestions found the default setting of GPT3.5 to almost always self-identify as\na maximally strong conservative (99.6%, N=1,030), and to always be morally\ndeontological in opposing the hypothetical pushing of a large man in front of\nan incoming trolley to save the lives of five people (100%, N=1,030). Since AI\nmodels of the future may be trained on much of the same data as GPT3.5,\ntraining data from which GPT3.5 may have learned its supposedly \"correct\nanswers,\" our results raise concerns that a hypothetical AI-led future may in\ncertain ways be subject to a diminished diversity of thought.\n","authors":["Peter S. Park","Philipp Schoenegger","Chongyang Zhu"],"pdf_url":"https://arxiv.org/pdf/2302.07267v3.pdf","comment":"52 pages (31-page main text, 21-page SI); nine visualizations (three\n  tables and two figures in the main text, four figures in the SI); added\n  corrections regarding the previously erroneous survey for Study 4's\n  replication of Graham et al. (2009); preregistered OSF database is available\n  at https://osf.io/dzp8t/"},{"id":"http://arxiv.org/abs/2303.02846v2","updated":"2023-03-12T14:51:17Z","published":"2023-03-06T02:52:37Z","title":"Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with\n  Variational Information Bottleneck and Contrastive Learning","summary":"  Deep learning techniques have dominated the literature on aspect-based\nsentiment analysis (ABSA), yielding state-of-the-art results. However, these\ndeep models generally suffer from spurious correlation problems between input\nfeatures and output labels, which creates significant barriers to robustness\nand generalization capability. In this paper, we propose a novel Contrastive\nVariational Information Bottleneck framework (called CVIB) to reduce spurious\ncorrelations for ABSA. The proposed CVIB framework is composed of an original\nnetwork and a self-pruned network, and these two networks are optimized\nsimultaneously via contrastive learning. Concretely, we employ the Variational\nInformation Bottleneck (VIB) principle to learn an informative and compressed\nnetwork (self-pruned network) from the original network, which discards the\nsuperfluous patterns or spurious correlations between input features and\nprediction labels. Then, self-pruning contrastive learning is devised to pull\ntogether semantically similar positive pairs and push away dissimilar pairs,\nwhere the representations of the anchor learned by the original and self-pruned\nnetworks respectively are regarded as a positive pair while the representations\nof two different sentences within a mini-batch are treated as a negative pair.\nTo verify the effectiveness of our CVIB method, we conduct extensive\nexperiments on five benchmark ABSA datasets and the experimental results show\nthat our approach achieves better performance than the strong competitors in\nterms of overall prediction performance, robustness, and generalization.\n","authors":["Mingshan Chang","Min Yang","Qingshan Jiang","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2303.02846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06675v1","updated":"2023-03-12T14:31:44Z","published":"2023-03-12T14:31:44Z","title":"LUKE-Graph: A Transformer-based Approach with Gated Relational Graph\n  Attention for Cloze-style Reading Comprehension","summary":"  Incorporating prior knowledge can improve existing pre-training models in\ncloze-style machine reading and has become a new trend in recent studies.\nNotably, most of the existing models have integrated external knowledge graphs\n(KG) and transformer-based models, such as BERT into a unified data structure.\nHowever, selecting the most relevant ambiguous entities in KG and extracting\nthe best subgraph remains a challenge. In this paper, we propose the\nLUKE-Graph, a model that builds a heterogeneous graph based on the intuitive\nrelationships between entities in a document without using any external KG. We\nthen use a Relational Graph Attention (RGAT) network to fuse the graph's\nreasoning information and the contextual representation encoded by the\npre-trained LUKE model. In this way, we can take advantage of LUKE, to derive\nan entity-aware representation; and a graph model - to exploit relation-aware\nrepresentation. Moreover, we propose Gated-RGAT by augmenting RGAT with a\ngating mechanism that regulates the question information for the graph\nconvolution operation. This is very similar to human reasoning processing\nbecause they always choose the best entity candidate based on the question\ninformation. Experimental results demonstrate that the LUKE-Graph achieves\nstate-of-the-art performance on the ReCoRD dataset with commonsense reasoning.\n","authors":["Shima Foolad","Kourosh Kiani"],"pdf_url":"https://arxiv.org/pdf/2303.06675v1.pdf","comment":"submitted for neurocomputing journal"},{"id":"http://arxiv.org/abs/2303.06662v1","updated":"2023-03-12T13:51:38Z","published":"2023-03-12T13:51:38Z","title":"Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive\n  Machine Translation","summary":"  Non-autoregressive translation (NAT) reduces the decoding latency but suffers\nfrom performance degradation due to the multi-modality problem. Recently, the\nstructure of directed acyclic graph has achieved great success in NAT, which\ntackles the multi-modality problem by introducing dependency between vertices.\nHowever, training it with negative log-likelihood loss implicitly requires a\nstrict alignment between reference tokens and vertices, weakening its ability\nto handle multiple translation modalities. In this paper, we hold the view that\nall paths in the graph are fuzzily aligned with the reference sentence. We do\nnot require the exact alignment but train the model to maximize a fuzzy\nalignment score between the graph and reference, which takes captured\ntranslations in all modalities into account. Extensive experiments on major WMT\nbenchmarks show that our method substantially improves translation performance\nand increases prediction confidence, setting a new state of the art for NAT on\nthe raw training data.\n","authors":["Zhengrui Ma","Chenze Shao","Shangtong Gui","Min Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.06662v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06623v1","updated":"2023-03-12T09:35:42Z","published":"2023-03-12T09:35:42Z","title":"MWE as WSD: Solving Multiword Expression Identification with Word Sense\n  Disambiguation","summary":"  Recent work in word sense disambiguation (WSD) utilizes encodings of the\nsense gloss (definition text), in addition to the input words and context, to\nimprove performance. In this work we demonstrate that this approach can be\nadapted for use in multiword expression (MWE) identification by training a\nBi-encoder model which uses gloss and context information to filter MWE\ncandidates produced from a simple rule-based extraction pipeline. We achieve\nstate-of-the-art results in MWE identification on the DiMSUM dataset, and\ncompetitive results on the PARSEME 1.1 English dataset using this method. Our\nmodel also retains most of its ability to perform WSD, demonstrating that a\nsingle model can successfully be applied to both of these tasks. Additionally,\nwe experiment with applying Poly-encoder models to MWE identification and WSD,\nintroducing a modified Poly-encoder architecture which outperforms the standard\nPoly-encoder on these tasks.\n","authors":["Joshua Tanner","Jacob Hoffman"],"pdf_url":"https://arxiv.org/pdf/2303.06623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06585v1","updated":"2023-03-12T06:11:44Z","published":"2023-03-12T06:11:44Z","title":"Improving the Intent Classification accuracy in Noisy Environment","summary":"  Intent classification is a fundamental task in the spoken language\nunderstanding field that has recently gained the attention of the scientific\ncommunity, mainly because of the feasibility of approaching it with end-to-end\nneural models. In this way, avoiding using intermediate steps, i.e. automatic\nspeech recognition, is possible, thus the propagation of errors due to\nbackground noise, spontaneous speech, speaking styles of users, etc. Towards\nthe development of solutions applicable in real scenarios, it is interesting to\ninvestigate how environmental noise and related noise reduction techniques to\naddress the intent classification task with end-to-end neural models. In this\npaper, we experiment with a noisy version of the fluent speech command data\nset, combining the intent classifier with a time-domain speech enhancement\nsolution based on Wave-U-Net and considering different training strategies.\nExperimental results reveal that, for this task, the use of speech enhancement\ngreatly improves the classification accuracy in noisy conditions, in particular\nwhen the classification model is trained on enhanced signals.\n","authors":["Mohamed Nabih Ali","Alessio Brutti","Daniele Falavigna"],"pdf_url":"https://arxiv.org/pdf/2303.06585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06580v1","updated":"2023-03-12T05:27:22Z","published":"2023-03-12T05:27:22Z","title":"Towards General Purpose Medical AI: Continual Learning Medical\n  Foundation Model","summary":"  Inevitable domain and task discrepancies in real-world scenarios can impair\nthe generalization performance of the pre-trained deep models for medical data.\nTherefore, we audaciously propose that we should build a general-purpose\nmedical AI system that can be seamlessly adapted to downstream domains/tasks.\nSince the domain/task adaption procedures usually involve additional labeling\nwork for the target data, designing a data-efficient adaption algorithm is\ndesired to save the cost of transferring the learned knowledge. Our recent work\nfound that vision-language models (VLMs) are efficient learners with\nextraordinary cross-domain ability. Therefore, in this work, we further explore\nthe possibility of leveraging pre-trained VLMs as medical foundation models for\nbuilding general-purpose medical AI, where we thoroughly investigate three\nmachine-learning paradigms, i.e., domain/task-specialized learning, joint\nlearning, and continual learning, for training the VLMs and evaluate their\ngeneralization performance on cross-domain and cross-task test sets. To\nalleviate the catastrophic forgetting during sequential training, we employ\nrehearsal learning and receive a sharp boost in terms of generalization\ncapability. In a nutshell, our empirical evidence suggests that continual\nlearning may be a practical and efficient learning paradigm for the medical\nfoundation model. And we hope researchers can use our empirical evidence as\nbasement to further explore the path toward medical foundation model.\n","authors":["Huahui Yi","Ziyuan Qin","Qicheng Lao","Wei Xu","Zekun Jiang","Dequan Wang","Shaoting Zhang","Kang Li"],"pdf_url":"https://arxiv.org/pdf/2303.06580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06574v1","updated":"2023-03-12T05:11:09Z","published":"2023-03-12T05:11:09Z","title":"Diffusion Models for Non-autoregressive Text Generation: A Survey","summary":"  Non-autoregressive (NAR) text generation has attracted much attention in the\nfield of natural language processing, which greatly reduces the inference\nlatency but has to sacrifice the generation accuracy. Recently, diffusion\nmodels, a class of latent variable generative models, have been introduced into\nNAR text generation, showing improved generation quality. In this survey, we\nreview the recent progress in diffusion models for NAR text generation. As the\nbackground, we first present the general definition of diffusion models and the\ntext diffusion models, and then discuss their merits for NAR generation. As the\ncore content, we further introduce two mainstream diffusion models in existing\ntext diffusion works, and review the key designs of the diffusion process.\nMoreover, we discuss the utilization of pre-trained language models (PLMs) for\ntext diffusion models and introduce optimization techniques for text data.\nFinally, we discuss several promising directions and conclude this paper. Our\nsurvey aims to provide researchers with a systematic reference of related\nresearch on text diffusion models for NAR generation.\n","authors":["Yifan Li","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2303.06574v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2209.05735v3","updated":"2023-03-12T04:46:00Z","published":"2022-09-13T05:14:08Z","title":"Learning ASR pathways: A sparse multilingual ASR model","summary":"  Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.\n","authors":["Mu Yang","Andros Tjandra","Chunxi Liu","David Zhang","Duc Le","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2209.05735v3.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06565v1","updated":"2023-03-12T04:23:54Z","published":"2023-03-12T04:23:54Z","title":"Compressed Heterogeneous Graph for Abstractive Multi-Document\n  Summarization","summary":"  Multi-document summarization (MDS) aims to generate a summary for a number of\nrelated documents. We propose HGSUM, an MDS model that extends an\nencoder-decoder architecture, to incorporate a heterogeneous graph to represent\ndifferent semantic units (e.g., words and sentences) of the documents. This\ncontrasts with existing MDS models which do not consider different edge types\nof graphs and as such do not capture the diversity of relationships in the\ndocuments. To preserve only key information and relationships of the documents\nin the heterogeneous graph, HGSUM uses graph pooling to compress the input\ngraph. And to guide HGSUM to learn compression, we introduce an additional\nobjective that maximizes the similarity between the compressed graph and the\ngraph constructed from the ground-truth summary during training. HGSUM is\ntrained end-to-end with graph similarity and standard cross-entropy objectives.\nExperimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM\noutperforms state-of-the-art MDS models. The code for our model and experiments\nis available at: https://github.com/oaimli/HGSum.\n","authors":["Miao Li","Jianzhong Qi","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2303.06565v1.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2211.00171v2","updated":"2023-03-12T00:12:42Z","published":"2022-10-31T22:32:36Z","title":"Using Emotion Embeddings to Transfer Knowledge Between Emotions,\n  Languages, and Annotation Formats","summary":"  The need for emotional inference from text continues to diversify as more and\nmore disciplines integrate emotions into their theories and applications. These\nneeds include inferring different emotion types, handling multiple languages,\nand different annotation formats. A shared model between different\nconfigurations would enable the sharing of knowledge and a decrease in training\ncosts, and would simplify the process of deploying emotion recognition models\nin novel environments. In this work, we study how we can build a single model\nthat can transition between these different configurations by leveraging\nmultilingual models and Demux, a transformer-based model whose input includes\nthe emotions of interest, enabling us to dynamically change the emotions\npredicted by the model. Demux also produces emotion embeddings, and performing\noperations on them allows us to transition to clusters of emotions by pooling\nthe embeddings of each cluster. We show that Demux can simultaneously transfer\nknowledge in a zero-shot manner to a new language, to a novel annotation format\nand to unseen emotions. Code is available at\nhttps://github.com/gchochla/Demux-MEmo .\n","authors":["Georgios Chochlakis","Gireesh Mahajan","Sabyasachee Baruah","Keith Burghardt","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2211.00171v2.pdf","comment":"Accepted at ICASSP'23, 5 pages"},{"id":"http://arxiv.org/abs/2210.15842v2","updated":"2023-03-12T00:10:51Z","published":"2022-10-28T02:27:18Z","title":"Leveraging Label Correlations in a Multi-label Setting: A Case Study in\n  Emotion","summary":"  Detecting emotions expressed in text has become critical to a range of\nfields. In this work, we investigate ways to exploit label correlations in\nmulti-label emotion recognition models to improve emotion detection. First, we\ndevelop two modeling approaches to the problem in order to capture word\nassociations of the emotion words themselves, by either including the emotions\nin the input, or by leveraging Masked Language Modeling (MLM). Second, we\nintegrate pairwise constraints of emotion representations as regularization\nterms alongside the classification loss of the models. We split these terms\ninto two categories, local and global. The former dynamically change based on\nthe gold labels, while the latter remain static during training. We demonstrate\nstate-of-the-art performance across Spanish, English, and Arabic in SemEval\n2018 Task 1 E-c using monolingual BERT-based models. On top of better\nperformance, we also demonstrate improved robustness. Code is available at\nhttps://github.com/gchochla/Demux-MEmo.\n","authors":["Georgios Chochlakis","Gireesh Mahajan","Sabyasachee Baruah","Keith Burghardt","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2210.15842v2.pdf","comment":"Accepted at ICASSP'23, 5 pages, 1 figure"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2201.05333v3","updated":"2023-03-12T17:54:21Z","published":"2022-01-14T08:21:28Z","title":"Attention over Self-attention:Intention-aware Re-ranking with Dynamic\n  Transformer Encoders for Recommendation","summary":"  Re-ranking models refine item recommendation lists generated by the prior\nglobal ranking model, which have demonstrated their effectiveness in improving\nthe recommendation quality. However, most existing re-ranking solutions only\nlearn from implicit feedback with a shared prediction model, which regrettably\nignore inter-item relationships under diverse user intentions. In this paper,\nwe propose a novel Intention-aware Re-ranking Model with Dynamic Transformer\nEncoder (RAISE), aiming to perform user-specific prediction for each individual\nuser based on her intentions. Specifically, we first propose to mine latent\nuser intentions from text reviews with an intention discovering module (IDM).\nBy differentiating the importance of review information with a co-attention\nnetwork, the latent user intention can be explicitly modeled for each user-item\npair. We then introduce a dynamic transformer encoder (DTE) to capture\nuser-specific inter-item relationships among item candidates by seamlessly\naccommodating the learned latent user intentions via IDM. As such, one can not\nonly achieve more personalized recommendations but also obtain corresponding\nexplanations by constructing RAISE upon existing recommendation engines.\nEmpirical study on four public datasets shows the superiority of our proposed\nRAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by\nPrecision@5, MAP@5, and NDCG@5 respectively.\n","authors":["Zhuoyi Lin","Sheng Zang","Rundong Wang","Zhu Sun","J. Senthilnath","Chi Xu","Chee-Keong Kwoh"],"pdf_url":"https://arxiv.org/pdf/2201.05333v3.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2007.14129v6","updated":"2023-03-12T16:39:57Z","published":"2020-07-28T11:18:36Z","title":"COMET: Convolutional Dimension Interaction for Collaborative Filtering","summary":"  Representation learning-based recommendation models play a dominant role\namong recommendation techniques. However, most of the existing methods assume\nboth historical interactions and embedding dimensions are independent of each\nother, and thus regrettably ignore the high-order interaction information among\nhistorical interactions and embedding dimensions. In this paper, we propose a\nnovel representation learning-based model called COMET (COnvolutional diMEnsion\ninTeraction), which simultaneously models the high-order interaction patterns\namong historical interactions and embedding dimensions. To be specific, COMET\nstacks the embeddings of historical interactions horizontally at first, which\nresults in two \"embedding maps\". In this way, internal interactions and\ndimensional interactions can be exploited by convolutional neural networks\n(CNN) with kernels of different sizes simultaneously. A fully-connected\nmulti-layer perceptron (MLP) is then applied to obtain two interaction vectors.\nLastly, the representations of users and items are enriched by the learnt\ninteraction vectors, which can further be used to produce the final prediction.\nExtensive experiments and ablation studies on various public implicit feedback\ndatasets clearly demonstrate the effectiveness and rationality of our proposed\nmethod.\n","authors":["Zhuoyi Lin","Lei Feng","Xingzhi Guo","Yu Zhang","Rui Yin","Chee Keong Kwoh","Chi Xu"],"pdf_url":"https://arxiv.org/pdf/2007.14129v6.pdf","comment":"Accepted by ACM TIST"},{"id":"http://arxiv.org/abs/2302.14096v2","updated":"2023-03-12T15:44:41Z","published":"2023-02-27T19:14:37Z","title":"A Dataset for Learning Graph Representations to Predict Customer Returns\n  in Fashion Retail","summary":"  We present a novel dataset collected by ASOS (a major online fashion\nretailer) to address the challenge of predicting customer returns in a fashion\nretail ecosystem. With the release of this substantial dataset we hope to\nmotivate further collaboration between research communities and the fashion\nindustry. We first explore the structure of this dataset with a focus on the\napplication of Graph Representation Learning in order to exploit the natural\ndata structure and provide statistical insights into particular features within\nthe data. In addition to this, we show examples of a return prediction\nclassification task with a selection of baseline models (i.e. with no\nintermediate representation learning step) and a graph representation based\nmodel. We show that in a downstream return prediction classification task, an\nF1-score of 0.792 can be found using a Graph Neural Network (GNN), improving\nupon other models discussed in this work. Alongside this increased F1-score, we\nalso present a lower cross-entropy loss by recasting the data into a graph\nstructure, indicating more robust predictions from a GNN based solution. These\nresults provide evidence that GNNs could provide more impactful and usable\nclassifications than other baseline models on the presented dataset and with\nthis motivation, we hope to encourage further research into graph-based\napproaches using the ASOS GraphReturns dataset.\n","authors":["Jamie McGowan","Elizabeth Guest","Ziyang Yan","Cong Zheng","Neha Patel","Mason Cusack","Charlie Donaldson","Sofie de Cnudde","Gabriel Facini","Fabon Dzogang"],"pdf_url":"https://arxiv.org/pdf/2302.14096v2.pdf","comment":"The ASOS GraphReturns dataset can be found at https://osf.io/c793h/.\n  Accepted at FashionXRecSys 2022 workshop. Published Version"},{"id":"http://arxiv.org/abs/2303.06660v1","updated":"2023-03-12T13:42:50Z","published":"2023-03-12T13:42:50Z","title":"P-MMF: Provider Max-min Fairness Re-ranking in Recommender System","summary":"  In this paper, we address the issue of recommending fairly from the aspect of\nproviders, which has become increasingly essential in multistakeholder\nrecommender systems. Existing studies on provider fairness usually focused on\ndesigning proportion fairness (PF) metrics that first consider systematic\nfairness. However, sociological researches show that to make the market more\nstable, max-min fairness (MMF) is a better metric. The main reason is that MMF\naims to improve the utility of the worst ones preferentially, guiding the\nsystem to support the providers in weak market positions. When applying MMF to\nrecommender systems, how to balance user preferences and provider fairness in\nan online recommendation scenario is still a challenging problem. In this\npaper, we proposed an online re-ranking model named Provider Max-min Fairness\nRe-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates\nprovider fair recommendation as a resource allocation problem, where the\nexposure slots are considered the resources to be allocated to providers and\nthe max-min fairness is used as the regularizer during the process. We show\nthat the problem can be further represented as a regularized online optimizing\nproblem and solved efficiently in its dual space. During the online re-ranking\nphase, a momentum gradient descent method is designed to conduct the dynamic\nre-ranking. Theoretical analysis showed that the regret of P-MMF can be\nbounded. Experimental results on four public recommender datasets demonstrated\nthat P-MMF can outperformed the state-of-the-art baselines. Experimental\nresults also show that P-MMF can retain small computationally costs on a corpus\nwith the large number of items.\n","authors":["Chen Xu","Sirui Chen","Jun Xu","Weiran Shen","Xiao Zhang","Gang Wang","Zhenghua Dong"],"pdf_url":"https://arxiv.org/pdf/2303.06660v1.pdf","comment":"Accepted in WWW23"},{"id":"http://arxiv.org/abs/2303.06611v1","updated":"2023-03-12T08:36:15Z","published":"2023-03-12T08:36:15Z","title":"AutoDenoise: Automatic Data Instance Denoising for Recommendations","summary":"  Historical user-item interaction datasets are essential in training modern\nrecommender systems for predicting user preferences. However, the arbitrary\nuser behaviors in most recommendation scenarios lead to a large volume of noisy\ndata instances being recorded, which cannot fully represent their true\ninterests. While a large number of denoising studies are emerging in the\nrecommender system community, all of them suffer from highly dynamic data\ndistributions. In this paper, we propose a Deep Reinforcement Learning (DRL)\nbased framework, AutoDenoise, with an Instance Denoising Policy Network, for\ndenoising data instances with an instance selection manner in deep recommender\nsystems. To be specific, AutoDenoise serves as an agent in DRL to adaptively\nselect noise-free and predictive data instances, which can then be utilized\ndirectly in training representative recommendation models. In addition, we\ndesign an alternate two-phase optimization strategy to train and validate the\nAutoDenoise properly. In the searching phase, we aim to train the policy\nnetwork with the capacity of instance denoising; in the validation phase, we\nfind out and evaluate the denoised subset of data instances selected by the\ntrained policy network, so as to validate its denoising ability. We conduct\nextensive experiments to validate the effectiveness of AutoDenoise combined\nwith multiple representative recommender system models.\n","authors":["Weilin Lin","Xiangyu Zhao","Yejing Wang","Yuanshao Zhu","Wanyu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06611v1.pdf","comment":"9 pages, 4 figures, 5 tables, conference"},{"id":"http://arxiv.org/abs/2303.06588v1","updated":"2023-03-12T06:39:40Z","published":"2023-03-12T06:39:40Z","title":"MobileRec: A Large-Scale Dataset for Mobile Apps Recommendation","summary":"  Recommender systems have become ubiquitous in our digital lives, from\nrecommending products on e-commerce websites to suggesting movies and music on\nstreaming platforms. Existing recommendation datasets, such as Amazon Product\nReviews and MovieLens, greatly facilitated the research and development of\nrecommender systems in their respective domains. While the number of mobile\nusers and applications (aka apps) has increased exponentially over the past\ndecade, research in mobile app recommender systems has been significantly\nconstrained, primarily due to the lack of high-quality benchmark datasets, as\nopposed to recommendations for products, movies, and news. To facilitate\nresearch for app recommendation systems, we introduce a large-scale dataset,\ncalled MobileRec. We constructed MobileRec from users' activity on the Google\nplay store. MobileRec contains 19.3 million user interactions (i.e., user\nreviews on apps) with over 10K unique apps across 48 categories. MobileRec\nrecords the sequential activity of a total of 0.7 million distinct users. Each\nof these users has interacted with no fewer than five distinct apps, which\nstands in contrast to previous datasets on mobile apps that recorded only a\nsingle interaction per user. Furthermore, MobileRec presents users' ratings as\nwell as sentiments on installed apps, and each app contains rich metadata such\nas app name, category, description, and overall rating, among others. We\ndemonstrate that MobileRec can serve as an excellent testbed for app\nrecommendation through a comparative study of several state-of-the-art\nrecommendation approaches. The quantitative results can act as a baseline for\nother researchers to compare their results against. The MobileRec dataset is\navailable at https://huggingface.co/datasets/recmeapp/mobilerec.\n","authors":["M. H. Maqbool","Umar Farooq","Adib Mosharrof","A. B. Siddique","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2303.06588v1.pdf","comment":"10 pages, 4 tables, 4 figures, Under submission at SIGIR'23"},{"id":"http://arxiv.org/abs/2303.06586v1","updated":"2023-03-12T06:23:10Z","published":"2023-03-12T06:23:10Z","title":"Proactive Prioritization of App Issues via Contrastive Learning","summary":"  Mobile app stores produce a tremendous amount of data in the form of user\nreviews, which is a huge source of user requirements and sentiments; such\nreviews allow app developers to proactively address issues in their apps.\nHowever, only a small number of reviews capture common issues and sentiments\nwhich creates a need for automatically identifying prominent reviews.\nUnfortunately, most existing work in text ranking and popularity prediction\nfocuses on social contexts where other signals are available, which renders\nsuch works ineffective in the context of app reviews. In this work, we propose\na new framework, PPrior, that enables proactive prioritization of app issues\nthrough identifying prominent reviews (ones predicted to receive a large number\nof votes in a given time window). Predicting highly-voted reviews is\nchallenging given that, unlike social posts, social network features of users\nare not available. Moreover, there is an issue of class imbalance, since a\nlarge number of user reviews receive little to no votes. PPrior employs a\npre-trained T5 model and works in three phases. Phase one adapts the\npre-trained T5 model to the user reviews data in a self-supervised fashion. In\nphase two, we leverage contrastive training to learn a generic and\ntask-independent representation of user reviews. Phase three uses radius\nneighbors classifier t o m ake t he final predictions. This phase also uses\nFAISS index for scalability and efficient search. To conduct extensive\nexperiments, we acquired a large dataset of over 2.1 million user reviews from\nGoogle Play. Our experimental results demonstrate the effectiveness of the\nproposed framework when compared against several state-of-the-art approaches.\nMoreover, the accuracy of PPrior in predicting prominent reviews is comparable\nto that of experienced app developers.\n","authors":["Moghis Fereidouni","Adib Mosharrof","Umar Farooq","AB Siddique"],"pdf_url":"https://arxiv.org/pdf/2303.06586v1.pdf","comment":"10 pages, 2022 IEEE International Conference on Big Data (Big Data)"},{"id":"http://arxiv.org/abs/2303.06573v1","updated":"2023-03-12T05:08:16Z","published":"2023-03-12T05:08:16Z","title":"Large Language Models Know Your Contextual Search Intent: A Prompting\n  Framework for Conversational Search","summary":"  In this paper, we present a prompting framework called LLMCS that leverages\nlarge language models, such as code-davinci-002 of GPT-3, to perform few-shot\nconversational query rewriting for conversational search. We explore three\nprompting methods to generate multiple query rewrites and hypothetical\nresponses, and propose aggregating them into an integrated representation that\ncan robustly represent the user's real contextual search intent. Experimental\nresults on two conversational search datasets, including CAst-19 and CAsT-20,\nshow that our approach achieves significant improvements in search\neffectiveness over existing baselines and manual rewrites. Notably, LLMCS can\nsignificantly outperform the state-of-the-art baselines by up to +5.9\\% and\n+32.9\\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential\nof large language models for conversational search. Our code will be released\nat https://github.com/kyriemao/LLMCS.\n","authors":["Kelong Mao","Zhicheng Dou","Haonan Chen","Fengran Mo","Hongjin Qian"],"pdf_url":"https://arxiv.org/pdf/2303.06573v1.pdf","comment":"Work in progress"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.02665v2","updated":"2023-03-12T10:26:48Z","published":"2023-03-05T13:06:53Z","title":"Heterogeneous Graph Learning for Acoustic Event Classification","summary":"  Heterogeneous graphs provide a compact, efficient, and scalable way to model\ndata involving multiple disparate modalities. This makes modeling audiovisual\ndata using heterogeneous graphs an attractive option. However, graph structure\ndoes not appear naturally in audiovisual data. Graphs for audiovisual data are\nconstructed manually which is both difficult and sub-optimal. In this work, we\naddress this problem by (i) proposing a parametric graph construction strategy\nfor the intra-modal edges, and (ii) learning the crossmodal edges. To this end,\nwe develop a new model, heterogeneous graph crossmodal network (HGCN) that\nlearns the crossmodal edges. Our proposed model can adapt to various spatial\nand temporal scales owing to its parametric construction, while the learnable\ncrossmodal edges effectively connect the relevant nodes across modalities.\nExperiments on a large benchmark dataset (AudioSet) show that our model is\nstate-of-the-art (0.53 mean average precision), outperforming transformer-based\nmodels and other graph-based models.\n","authors":["Amir Shirian","Mona Ahmadian","Krishna Somandepalli","Tanaya Guha"],"pdf_url":"https://arxiv.org/pdf/2303.02665v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2207.07935"},{"id":"http://arxiv.org/abs/2302.08774v2","updated":"2023-03-12T07:53:23Z","published":"2023-02-17T09:20:51Z","title":"Vision, Deduction and Alignment: An Empirical Study on Multi-modal\n  Knowledge Graph Alignment","summary":"  Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in\nknowledge engineering. Existing EA methods mostly focus on utilizing the graph\nstructures and entity attributes (including literals), but ignore images that\nare common in modern multi-modal KGs. In this study we first constructed\nMulti-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then\nevaluated some existing embedding-based methods for utilizing images. In view\nof the complementary nature of visual modal information and logical deduction,\nwe further developed a new multi-modal EA method named LODEME using logical\ndeduction and multi-modal KG embedding, with state-of-the-art performance\nachieved on Multi-OpenEA and other existing multi-modal EA benchmarks.\n","authors":["Yangning Li","Jiaoyan Chen","Yinghui Li","Yuejia Xiang","Xi Chen","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2302.08774v2.pdf","comment":"Accepted by ICASSP2023"}]},"2023-03-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2211.15544v2","updated":"2023-03-11T23:13:21Z","published":"2022-11-28T16:49:13Z","title":"Automatically Extracting Information in Medical Dialogue: Expert System\n  And Attention for Labelling","summary":"  Medical dialogue information extraction is becoming an increasingly\nsignificant problem in modern medical care. It is difficult to extract key\ninformation from electronic medical records (EMRs) due to their large numbers.\nPreviously, researchers proposed attention-based models for retrieving features\nfrom EMRs, but their limitations were reflected in their inability to recognize\ndifferent categories in medical dialogues. In this paper, we propose a novel\nmodel, Expert System and Attention for Labelling (ESAL). We use mixture of\nexperts and pre-trained BERT to retrieve the semantics of different categories,\nenabling the model to fuse the differences between them. In our experiment,\nESAL was applied to a public dataset and the experimental results indicated\nthat ESAL significantly improved the performance of Medical Information\nClassification.\n","authors":["Xinshi Wang","Daniel Tang"],"pdf_url":"https://arxiv.org/pdf/2211.15544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06475v1","updated":"2023-03-11T18:17:03Z","published":"2023-03-11T18:17:03Z","title":"Transcription free filler word detection with Neural semi-CRFs","summary":"  Non-linguistic filler words, such as \"uh\" or \"um\", are prevalent in\nspontaneous speech and serve as indicators for expressing hesitation or\nuncertainty. Previous works for detecting certain non-linguistic filler words\nare highly dependent on transcriptions from a well-established commercial\nautomatic speech recognition (ASR) system. However, certain ASR systems are not\nuniversally accessible from many aspects, e.g., budget, target languages, and\ncomputational power. In this work, we investigate filler word detection system\nthat does not depend on ASR systems. We show that, by using the structured\nstate space sequence model (S4) and neural semi-Markov conditional random\nfields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment\nlevel) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a\nqualitative analysis on the detected results to analyze the limitations of our\nproposed system.\n","authors":["Ge Zhu","Yujia Yan","Juan-Pablo Caceres","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2303.06475v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.06458v1","updated":"2023-03-11T17:14:33Z","published":"2023-03-11T17:14:33Z","title":"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and\n  Multilingual Natural Language Generation","summary":"  Natural Language Generation (NLG) accepts input data in the form of images,\nvideos, or text and generates corresponding natural language text as output.\nExisting NLG methods mainly adopt a supervised approach and rely heavily on\ncoupled data-to-text pairs. However, for many targeted scenarios and for\nnon-English languages, sufficient quantities of labeled data are often not\navailable. To relax the dependency on labeled data of downstream tasks, we\npropose an intuitive and effective zero-shot learning framework, ZeroNLG, which\ncan deal with multiple NLG tasks, including image-to-text (image captioning),\nvideo-to-text (video captioning), and text-to-text (neural machine\ntranslation), across English, Chinese, German, and French within a unified\nframework. ZeroNLG does not require any labeled downstream pairs for training.\nDuring training, ZeroNLG (i) projects different domains (across modalities and\nlanguages) to corresponding coordinates in a shared common latent space; (ii)\nbridges different domains by aligning their corresponding coordinates in this\nspace; and (iii) builds an unsupervised multilingual auto-encoder to learn to\ngenerate text by reconstructing the input text given its coordinate in shared\nlatent space. Consequently, during inference, based on the data-to-text\npipeline, ZeroNLG can generate target sentences across different languages\ngiven the coordinate of input data in the common space. Within this unified\nframework, given visual (imaging or video) data as input, ZeroNLG can perform\nzero-shot visual captioning; given textual sentences as input, ZeroNLG can\nperform zero-shot machine translation. We present the results of extensive\nexperiments on twelve NLG tasks, showing that, without using any labeled\ndownstream pairs for training, ZeroNLG generates high-quality and believable\noutputs and significantly outperforms existing zero-shot methods.\n","authors":["Bang Yang","Fenglin Liu","Yuexian Zou","Xian Wu","Yaowei Wang","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2303.06458v1.pdf","comment":"We will release the codes and models at\n  https://github.com/yangbang18/ZeroNLG soon. Without any labeled downstream\n  pairs for training, the ZeroNLG can deal with multiple NLG tasks, including\n  image-to-text, video-to-text, and text-to-text, across English, Chinese,\n  German, and French within a unified framework"},{"id":"http://arxiv.org/abs/2210.00465v3","updated":"2023-03-11T16:04:32Z","published":"2022-10-02T09:04:47Z","title":"Assessing the impact of contextual information in hate speech detection","summary":"  In recent years, hate speech has gained great relevance in social networks\nand other virtual media because of its intensity and its relationship with\nviolent acts against members of protected groups. Due to the great amount of\ncontent generated by users, great effort has been made in the research and\ndevelopment of automatic tools to aid the analysis and moderation of this\nspeech, at least in its most threatening forms. One of the limitations of\ncurrent approaches to automatic hate speech detection is the lack of context.\nMost studies and resources are performed on data without context; that is,\nisolated messages without any type of conversational context or the topic being\ndiscussed. This restricts the available information to define if a post on a\nsocial network is hateful or not. In this work, we provide a novel corpus for\ncontextualized hate speech detection based on user responses to news posts from\nmedia outlets on Twitter. This corpus was collected in the Rioplatense\ndialectal variety of Spanish and focuses on hate speech associated with the\nCOVID-19 pandemic. Classification experiments using state-of-the-art techniques\nshow evidence that adding contextual information improves hate speech detection\nperformance for two proposed tasks (binary and multi-label prediction). We make\nour code, models, and corpus available for further research.\n","authors":["Juan Manuel Pérez","Franco Luque","Demian Zayat","Martín Kondratzky","Agustín Moro","Pablo Serrati","Joaquín Zajac","Paula Miguel","Natalia Debandi","Agustín Gravano","Viviana Cotik"],"pdf_url":"https://arxiv.org/pdf/2210.00465v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09231v4","updated":"2023-03-11T15:12:50Z","published":"2021-12-16T22:36:17Z","title":"Two-view Graph Neural Networks for Knowledge Graph Completion","summary":"  We present an effective graph neural network (GNN)-based knowledge graph\nembedding model, which we name WGE, to capture entity- and relation-focused\ngraph structures. Given a knowledge graph, WGE builds a single undirected\nentity-focused graph that views entities as nodes. WGE also constructs another\nsingle undirected graph from relation-focused constraints, which views entities\nand relations as nodes. WGE then proposes a GNN-based architecture to better\nlearn vector representations of entities and relations from these two single\nentity- and relation-focused graphs. WGE feeds the learned entity and relation\nrepresentations into a weighted score function to return the triple scores for\nknowledge graph completion. Experimental results show that WGE outperforms\nstrong baselines on seven benchmark datasets for knowledge graph completion.\n","authors":["Vinh Tong","Dai Quoc Nguyen","Dinh Phung","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2112.09231v4.pdf","comment":"To appear in Proceedings of ESWC 2023; 17 pages; 4 tables; 4 figures"},{"id":"http://arxiv.org/abs/2204.04916v2","updated":"2023-03-11T15:09:08Z","published":"2022-04-11T07:33:26Z","title":"A Token-level Contrastive Framework for Sign Language Translation","summary":"  Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.\n","authors":["Biao Fu","Peigen Ye","Liang Zhang","Pei Yu","Cong Hu","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2204.04916v2.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.02563v2","updated":"2023-03-11T13:51:21Z","published":"2023-03-05T03:18:56Z","title":"FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis","summary":"  This paper presents a novel approach for explainability in financial analysis\nby utilizing the Pearson correlation coefficient to establish a relationship\nbetween aspect-based sentiment analysis and stock prices. The proposed\nmethodology involves constructing an aspect list from financial news articles\nand analyzing sentiment intensity scores for each aspect. These scores are then\ncompared to the stock prices for the relevant companies using the Pearson\ncoefficient to determine any significant correlations. The results indicate\nthat the proposed approach provides a more detailed and accurate understanding\nof the relationship between sentiment analysis and stock prices, which can be\nuseful for investors and financial analysts in making informed decisions.\nAdditionally, this methodology offers a transparent and interpretable way to\nexplain the sentiment analysis results and their impact on stock prices.\nOverall, the findings of this paper demonstrate the importance of\nexplainability in financial analysis and highlight the potential benefits of\nutilizing the Pearson coefficient for analyzing aspect-based sentiment analysis\nand stock prices. The proposed approach offers a valuable tool for\nunderstanding the complex relationships between financial news sentiment and\nstock prices, providing a new perspective on the financial market and aiding in\nmaking informed investment decisions.\n","authors":["Keane Ong","Wihan van der Heever","Ranjan Satapathy","Gianmarco Mengaldo","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2303.02563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11981v2","updated":"2023-03-11T11:31:12Z","published":"2022-10-21T14:16:51Z","title":"Named Entity Detection and Injection for Direct Speech Translation","summary":"  In a sentence, certain words are critical for its semantic. Among them, named\nentities (NEs) are notoriously challenging for neural models. Despite their\nimportance, their accurate handling has been neglected in speech-to-text (S2T)\ntranslation research, and recent work has shown that S2T models perform poorly\nfor locations and notably person names, whose spelling is challenging unless\nknown in advance. In this work, we explore how to leverage dictionaries of NEs\nknown to likely appear in a given context to improve S2T model outputs. Our\nexperiments show that we can reliably detect NEs likely present in an utterance\nstarting from S2T encoder outputs. Indeed, we demonstrate that the current\ndetection quality is sufficient to improve NE accuracy in the translation with\na 31% reduction in person name errors.\n","authors":["Marco Gaido","Yun Tang","Ilia Kulikov","Rongqing Huang","Hongyu Gong","Hirofumi Inaguma"],"pdf_url":"https://arxiv.org/pdf/2210.11981v2.pdf","comment":"\\c{opyright} 2022 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2303.06333v1","updated":"2023-03-11T07:30:52Z","published":"2023-03-11T07:30:52Z","title":"Parachute: Evaluating Interactive Human-LM Co-writing Systems","summary":"  A surge of advances in language models (LMs) has led to significant interest\nin using LMs to build co-writing systems, in which humans and LMs interactively\ncontribute to a shared writing artifact. However, there is a lack of studies\nassessing co-writing systems in interactive settings. We propose a\nhuman-centered evaluation framework, Parachute, for interactive co-writing\nsystems. Parachute showcases an integrative view of interaction evaluation,\nwhere each evaluation aspect consists of categorized practical metrics.\nFurthermore, we present Parachute with a use case to demonstrate how to\nevaluate and compare co-writing systems using Parachute.\n","authors":["Hua Shen","Tongshuang Wu"],"pdf_url":"https://arxiv.org/pdf/2303.06333v1.pdf","comment":"Accepted by CHI'23 In2Writing Workshop"},{"id":"http://arxiv.org/abs/2303.06296v1","updated":"2023-03-11T03:30:47Z","published":"2023-03-11T03:30:47Z","title":"Stabilizing Transformer Training by Preventing Attention Entropy\n  Collapse","summary":"  Training stability is of great importance to Transformers. In this work, we\ninvestigate the training dynamics of Transformers by examining the evolution of\nthe attention layers. In particular, we track the attention entropy for each\nattention head during the course of training, which is a proxy for model\nsharpness. We identify a common pattern across different architectures and\ntasks, where low attention entropy is accompanied by high training instability,\nwhich can take the form of oscillating loss or divergence. We denote the\npathologically low attention entropy, corresponding to highly concentrated\nattention scores, as $\\textit{entropy collapse}$. As a remedy, we propose\n$\\sigma$Reparam, a simple and efficient solution where we reparametrize all\nlinear layers with spectral normalization and an additional learned scalar. We\ndemonstrate that the proposed reparameterization successfully prevents entropy\ncollapse in the attention layers, promoting more stable training. Additionally,\nwe prove a tight lower bound of the attention entropy, which decreases\nexponentially fast with the spectral norm of the attention logits, providing\nadditional motivation for our approach. We conduct experiments with\n$\\sigma$Reparam on image classification, image self-supervised learning,\nmachine translation, automatic speech recognition, and language modeling tasks,\nacross Transformer architectures. We show that $\\sigma$Reparam provides\nstability and robustness with respect to the choice of hyperparameters, going\nso far as enabling training (a) a Vision Transformer to competitive performance\nwithout warmup, weight decay, layer normalization or adaptive optimizers; (b)\ndeep architectures in machine translation and (c) speech recognition to\ncompetitive performance without warmup and adaptive optimizers.\n","authors":["Shuangfei Zhai","Tatiana Likhomanenko","Etai Littwin","Dan Busbridge","Jason Ramapuram","Yizhe Zhang","Jiatao Gu","Josh Susskind"],"pdf_url":"https://arxiv.org/pdf/2303.06296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11768v2","updated":"2023-03-11T02:45:13Z","published":"2022-10-21T07:08:31Z","title":"Augmentation with Projection: Towards an Effective and Efficient Data\n  Augmentation Paradigm for Distillation","summary":"  Knowledge distillation is one of the primary methods of transferring\nknowledge from large to small models. However, it requires massive\ntask-specific data, which may not be plausible in many real-world applications.\nData augmentation methods such as representation interpolation, token\nreplacement, or augmentation with models are applied to tackle this problem.\nHowever, these data augmentation methods either potentially cause shifts in\ndecision boundaries (representation interpolation), are not expressive enough\n(token replacement), or introduce too much computational overhead (augmentation\nwith models). To this end, we propose AugPro (Augmentation with Projection), an\neffective and efficient data augmentation method for distillation. Our method\nbuilds on top of representation interpolation augmentation methods to maintain\nthe diversity of expressions and converts the augmented data to tokens to avoid\nshifting decision boundaries. It uses simple operations that come with little\ncomputational overhead. The results on multiple GLUE tasks show that our\nmethods can improve distillation performance by a large margin at a low time\ncost. Codes are available at\nhttps://github.com/google-research/google-research/tree/master/augpro.\n","authors":["Ziqi Wang","Yuexin Wu","Frederick Liu","Daogao Liu","Le Hou","Hongkun Yu","Jing Li","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2210.11768v2.pdf","comment":"20 pages, 5 figures. Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06273v1","updated":"2023-03-11T01:19:01Z","published":"2023-03-11T01:19:01Z","title":"Consistency Analysis of ChatGPT","summary":"  ChatGPT, a question-and-answer dialogue system based on a large language\nmodel, has gained huge popularity since its introduction. Its positive aspects\nhave been reported through many media platforms, and some analyses even showed\nthat ChatGPT achieved a decent grade in professional exams, including the law,\nmedical, and finance domains, adding extra support to the claim that AI now can\nassist and, even, replace humans in industrial fields. Others, however, doubt\nits reliability and trustworthiness. In this paper, we investigate ChatGPT's\ntrustworthiness regarding logically consistent behaviours. Our findings suggest\nthat, although ChatGPT seems to achieve an improved language understanding\nability, it still fails to generate logically correct predictions frequently.\nHence, while it is true that ChatGPT is an impressive and promising new\ntechnique, we conclude that its usage in real-world applications without\nthorough human inspection requires further consideration, especially for\nrisk-sensitive areas.\n","authors":["Myeongjun Jang","Thomas Lukasiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.06273v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.06264v1","updated":"2023-03-11T01:04:25Z","published":"2023-03-11T01:04:25Z","title":"An Interactive UI to Support Sensemaking over Collections of Parallel\n  Texts","summary":"  Scientists and science journalists, among others, often need to make sense of\na large number of papers and how they compare with each other in scope, focus,\nfindings, or any other important factors. However, with a large corpus of\npapers, it's cognitively demanding to pairwise compare and contrast them all\nwith each other. Fully automating this review process would be infeasible,\nbecause it often requires domain-specific knowledge, as well as understanding\nwhat the context and motivations for the review are. While there are existing\ntools to help with the process of organizing and annotating papers for\nliterature reviews, at the core they still rely on people to serially read\nthrough papers and manually make sense of relevant information.\n  We present AVTALER, which combines peoples' unique skills, contextual\nawareness, and knowledge, together with the strength of automation. Given a set\nof comparable text excerpts from a paper corpus, it supports users in\nsensemaking and contrasting paper attributes by interactively aligning text\nexcerpts in a table so that comparable details are presented in a shared\ncolumn. AVTALER is based on a core alignment algorithm that makes use of modern\nNLP tools. Furthermore, AVTALER is a mixed-initiative system: users can\ninteractively give the system constraints which are integrated into the\nalignment construction process.\n","authors":["Joyce Zhou","Elena Glassman","Daniel S. Weld"],"pdf_url":"https://arxiv.org/pdf/2303.06264v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2210.01343v3","updated":"2023-03-11T00:11:03Z","published":"2022-10-04T03:18:19Z","title":"The Surprising Computational Power of Nondeterministic Stack RNNs","summary":"  Traditional recurrent neural networks (RNNs) have a fixed, finite number of\nmemory cells. In theory (assuming bounded range and precision), this limits\ntheir formal language recognition power to regular languages, and in practice,\nRNNs have been shown to be unable to learn many context-free languages (CFLs).\nIn order to expand the class of languages RNNs recognize, prior work has\naugmented RNNs with a nondeterministic stack data structure, putting them on\npar with pushdown automata and increasing their language recognition power to\nCFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic\nCFLs), but in this paper, we show that nondeterminism and the neural controller\ninteract to produce two more unexpected abilities. First, the nondeterministic\nstack RNN can recognize not only CFLs, but also many non-context-free\nlanguages. Second, it can recognize languages with much larger alphabet sizes\nthan one might expect given the size of its stack alphabet. Finally, to\nincrease the information capacity in the stack and allow it to solve more\ncomplicated tasks with large alphabet sizes, we propose a new version of the\nnondeterministic stack that simulates stacks of vectors rather than discrete\nsymbols. We demonstrate perplexity improvements with this new model on the Penn\nTreebank language modeling benchmark.\n","authors":["Brian DuSell","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2210.01343v3.pdf","comment":"21 pages, 8 figures. Published at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.08080v1","updated":"2023-03-11T00:01:21Z","published":"2023-03-11T00:01:21Z","title":"Verbal behavior without syntactic structures: beyond Skinner and Chomsky","summary":"  What does it mean to know language? Since the Chomskian revolution, one\npopular answer to this question has been: to possess a generative grammar that\nexclusively licenses certain syntactic structures. Decades later, not even an\napproximation to such a grammar, for any language, has been formulated; the\nidea that grammar is universal and innately specified has proved barren; and\nattempts to show how it could be learned from experience invariably come up\nshort. To move on from this impasse, we must rediscover the extent to which\nlanguage is like any other human behavior: dynamic, social, multimodal,\npatterned, and purposive, its purpose being to promote desirable actions (or\nthoughts) in others and self. Recent psychological, computational,\nneurobiological, and evolutionary insights into the shaping and structure of\nbehavior may then point us toward a new, viable account of language.\n","authors":["Shimon Edelman"],"pdf_url":"https://arxiv.org/pdf/2303.08080v1.pdf","comment":"Ms completed on February 4, 2019"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.06356v1","updated":"2023-03-11T09:25:51Z","published":"2023-03-11T09:25:51Z","title":"PowerMat: context-aware recommender system without user item rating\n  values that solves the cold-start problem","summary":"  Recommender systems serves as an important technical asset in many modern\ncompanies. With the increasing demand for higher precision of the technology,\nmore and more research and investment has been allocated to the field. One\nimportant sub-field of recommender systems that has been stagnating is\ncontext-aware recommender systems. Due to the difficulty of collecting input\ndataset, the amount of research on context-aware recommender systems is much\nless than other sub-fields of recommender systems. In this paper, we propose a\nnew algorithm named PowerMat to tackle the context-aware recommendation\nproblem. We build our theory on matrix factorization and Zipf's law, and also\nmore recent research work such as DotMat. We prove by experiments that our\nmethod achieves superior results to the classic matrix factorization algorithm\nand other context-aware recommender systems such as MovieMat+. In addition, by\ntheoretical analysis, we show that our algorithm solves the cold-start problem\nfor context-aware recommendation.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06354v1","updated":"2023-03-11T09:11:27Z","published":"2023-03-11T09:11:27Z","title":"Betti Number for Point Sets","summary":"  Topology is the foundation for many industrial applications ranging from CAD\nto simulation analysis. Computational topology mostly focuses on structured\ndata such as mesh, however unstructured dataset such as point set remains a\nvirgin land for topology scientists. The significance of point-based topology\ncan never be overemphasized, especially in the area of reverse engineering,\ngeometric modeling and algorithmic analysis. In this paper, we propose a novel\napproach to compute the Betti number for point set data and illustrate its\nusefulness in real world examples. To the best of our knowledge, our work is\npioneering and first of its kind in the fields of computational topology.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.06354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06347v1","updated":"2023-03-11T08:44:14Z","published":"2023-03-11T08:44:14Z","title":"User Retention-oriented Recommendation with Decision Transformer","summary":"  Improving user retention with reinforcement learning~(RL) has attracted\nincreasing attention due to its significant importance in boosting user\nengagement. However, training the RL policy from scratch without hurting users'\nexperience is unavoidable due to the requirement of trial-and-error searches.\nFurthermore, the offline methods, which aim to optimize the policy without\nonline interactions, suffer from the notorious stability problem in value\nestimation or unbounded variance in counterfactual policy evaluation. To this\nend, we propose optimizing user retention with Decision Transformer~(DT), which\navoids the offline difficulty by translating the RL as an autoregressive\nproblem. However, deploying the DT in recommendation is a non-trivial problem\nbecause of the following challenges: (1) deficiency in modeling the numerical\nreward value; (2) data discrepancy between the policy learning and\nrecommendation generation; (3) unreliable offline performance evaluation. In\nthis work, we, therefore, contribute a series of strategies for tackling the\nexposed issues. We first articulate an efficient reward prompt by weighted\naggregation of meta embeddings for informative reward embedding. Then, we endow\na weighted contrastive learning method to solve the discrepancy between\ntraining and inference. Furthermore, we design two robust offline metrics to\nmeasure user retention. Finally, the significant improvement in the benchmark\ndatasets demonstrates the superiority of the proposed method.\n","authors":["Kesen Zhao","Lixin Zou","Xiangyu Zhao","Maolin Wang","Dawei yin"],"pdf_url":"https://arxiv.org/pdf/2303.06347v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.06337v1","updated":"2023-03-11T07:50:49Z","published":"2023-03-11T07:50:49Z","title":"AutoMLP: Automated MLP for Sequential Recommendations","summary":"  Sequential recommender systems aim to predict users' next interested item\ngiven their historical interactions. However, a long-standing issue is how to\ndistinguish between users' long/short-term interests, which may be\nheterogeneous and contribute differently to the next recommendation. Existing\napproaches usually set pre-defined short-term interest length by exhaustive\nsearch or empirical experience, which is either highly inefficient or yields\nsubpar results. The recent advanced transformer-based models can achieve\nstate-of-the-art performances despite the aforementioned issue, but they have a\nquadratic computational complexity to the length of the input sequence. To this\nend, this paper proposes a novel sequential recommender system, AutoMLP, aiming\nfor better modeling users' long/short-term interests from their historical\ninteractions. In addition, we design an automated and adaptive search algorithm\nfor preferable short-term interest length via end-to-end optimization. Through\nextensive experiments, we show that AutoMLP has competitive performance against\nstate-of-the-art methods, while maintaining linear computational complexity.\n","authors":["Muyang Li","Zijian Zhang","Xiangyu Zhao","Wanyu Wang","Minghao Zhao","Runze Wu","Ruocheng Guo"],"pdf_url":"https://arxiv.org/pdf/2303.06337v1.pdf","comment":"Accepted by WWW'23"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.02353v2","updated":"2023-03-11T19:53:30Z","published":"2023-03-04T08:33:46Z","title":"Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling","summary":"  High-resolution (HR) images are usually downscaled to low-resolution (LR)\nones for better display and afterward upscaled back to the original size to\nrecover details. Recent work in image rescaling formulates downscaling and\nupscaling as a unified task and learns a bijective mapping between HR and LR\nvia invertible networks. However, in real-world applications (e.g., social\nmedia), most images are compressed for transmission. Lossy compression will\nlead to irreversible information loss on LR images, hence damaging the inverse\nupscaling procedure and degrading the reconstruction accuracy. In this paper,\nwe propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware\nimage rescaling. To tackle the distribution shift, we first develop an\nend-to-end asymmetric framework with two separate bijective mappings for\nhigh-quality and compressed LR images, respectively. Then, based on empirical\nanalysis of this framework, we model the distribution of the lost information\n(including downscaling and compression) using isotropic Gaussian mixtures and\npropose the Enhanced Invertible Block to derive high-quality/compressed LR\nimages in one forward pass. Besides, we design a set of losses to regularize\nthe learned LR images and enhance the invertibility. Extensive experiments\ndemonstrate the consistent improvements of SAIN across various image rescaling\ndatasets in terms of both quantitative and qualitative evaluation under\nstandard image compression formats (i.e., JPEG and WebP).\n","authors":["Jinhai Yang","Mengxi Guo","Shijie Zhao","Junlin Li","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.02353v2.pdf","comment":"Accepted by AAAI 2023. Code is available at\n  https://github.com/yang-jin-hai/SAIN"},{"id":"http://arxiv.org/abs/2303.05338v2","updated":"2023-03-11T16:11:24Z","published":"2023-03-09T15:34:36Z","title":"MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual\n  Fine-Grained Learning","summary":"  Audio-visual learning helps to comprehensively understand the world by fusing\npractical information from multiple modalities. However, recent studies show\nthat the imbalanced optimization of uni-modal encoders in a joint-learning\nmodel is a bottleneck to enhancing the model's performance. We further find\nthat the up-to-date imbalance-mitigating methods fail on some audio-visual\nfine-grained tasks, which have a higher demand for distinguishable feature\ndistribution. Fueled by the success of cosine loss that builds hyperspherical\nfeature spaces and achieves lower intra-class angular variability, this paper\nproposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$\nnormalization to features and weights towards balanced and better multi-modal\nfine-grained learning. We demonstrate that our method can alleviate the\nimbalanced optimization from the perspective of weight norm and fully exploit\nthe discriminability of the cosine metric. Extensive experiments prove the\neffectiveness of our method and the versatility with advanced multi-modal\nfusion strategies and up-to-date imbalance-mitigating methods.\n","authors":["Ruize Xu","Ruoxuan Feng","Shi-Xiong Zhang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2303.05338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04216v4","updated":"2023-03-11T14:49:56Z","published":"2022-10-09T10:10:13Z","title":"AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human\n  Pose Estimation","summary":"  The graph convolutional networks (GCNs) have been applied to model the\nphysically connected and non-local relations among human joints for 3D human\npose estimation (HPE). In addition, the purely Transformer-based models\nrecently show promising results in video-based 3D HPE. However, the\nsingle-frame method still needs to model the physically connected relations\namong joints because the feature representations transformed only by global\nrelations via the Transformer neglect information on the human skeleton. To\ndeal with this problem, we propose a novel method in which the Transformer\nencoder and GCN blocks are alternately stacked, namely AMPose, to combine the\nglobal and physically connected relations among joints towards HPE. In the\nAMPose, the Transformer encoder is applied to connect each joint with all the\nother joints, while GCNs are applied to capture information on physically\nconnected relations. The effectiveness of our proposed method is evaluated on\nthe Human3.6M dataset. Our model also shows better generalization ability by\ntesting on the MPI-INF-3DHP dataset. Code can be retrieved at\nhttps://github.com/erikervalid/AMPose.\n","authors":["Hongxin Lin","Yunwei Chiu","Peiyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2210.04216v4.pdf","comment":"ICASSP 2023 Accepted Paper"},{"id":"http://arxiv.org/abs/2303.03857v2","updated":"2023-03-11T09:22:02Z","published":"2023-03-07T12:49:45Z","title":"Leveraging Pre-trained AudioLDM for Text to Sound Generation: A\n  Benchmark Study","summary":"  Deep neural networks have recently achieved breakthroughs in sound generation\nwith text prompts. Despite their promising performance, current text-to-sound\ngeneration models face issues on small-scale datasets (e.g., overfitting),\nsignificantly limiting their performance. In this paper, we investigate the use\nof pre-trained AudioLDM, the state-of-the-art model for text-to-audio\ngeneration, as the backbone for sound generation. Our study demonstrates the\nadvantages of using pre-trained models for text-to-sound generation, especially\nin data-scarcity scenarios. In addition, experiments show that different\ntraining strategies (e.g., training conditions) may affect the performance of\nAudioLDM on datasets of different scales. To facilitate future studies, we also\nevaluate various text-to-sound generation systems on several frequently used\ndatasets under the same evaluation protocols, which allow fair comparisons and\nbenchmarking of these methods on the common ground.\n","authors":["Yi Yuan","Haohe Liu","Jinhua Liang","Xubo Liu","Mark D. Plumbley","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03857v2.pdf","comment":"EUSIPCO 2023"},{"id":"http://arxiv.org/abs/2301.09799v2","updated":"2023-03-11T08:29:51Z","published":"2023-01-24T03:47:37Z","title":"LDMIC: Learning-based Distributed Multi-view Image Coding","summary":"  Multi-view image compression plays a critical role in 3D-related\napplications. Existing methods adopt a predictive coding architecture, which\nrequires joint encoding to compress the corresponding disparity as well as\nresidual information. This demands collaboration among cameras and enforces the\nepipolar geometric constraint between different views, which makes it\nchallenging to deploy these methods in distributed camera systems with randomly\noverlapping fields of view. Meanwhile, distributed source coding theory\nindicates that efficient data compression of correlated sources can be achieved\nby independent encoding and joint decoding, which motivates us to design a\nlearning-based distributed multi-view image coding (LDMIC) framework. With\nindependent encoders, LDMIC introduces a simple yet effective joint context\ntransfer module based on the cross-attention mechanism at the decoder to\neffectively capture the global inter-view correlations, which is insensitive to\nthe geometric relationships between images. Experimental results show that\nLDMIC significantly outperforms both traditional and learning-based MIC methods\nwhile enjoying fast encoding speed. Code will be released at\nhttps://github.com/Xinjie-Q/LDMIC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.09799v2.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2303.06326v1","updated":"2023-03-11T06:56:10Z","published":"2023-03-11T06:56:10Z","title":"The Multimodal Information based Speech Processing (MISP) 2022\n  Challenge: Audio-Visual Diarization and Recognition","summary":"  The Multi-modal Information based Speech Processing (MISP) challenge aims to\nextend the application of signal processing technology in specific scenarios by\npromoting the research into wake-up words, speaker diarization, speech\nrecognition, and other technologies. The MISP2022 challenge has two tracks: 1)\naudio-visual speaker diarization (AVSD), aiming to solve ``who spoken when''\nusing both audio and visual data; 2) a novel audio-visual diarization and\nrecognition (AVDR) task that focuses on addressing ``who spoken what when''\nwith audio-visual speaker diarization results. Both tracks focus on the Chinese\nlanguage, and use far-field audio and video in real home-tv scenarios: 2-6\npeople communicating each other with TV noise in the background. This paper\nintroduces the dataset, track settings, and baselines of the MISP2022\nchallenge. Our analyses of experiments and examples indicate the good\nperformance of AVDR baseline system, and the potential difficulties in this\nchallenge due to, e.g., the far-field video quality, the presence of TV noise\nin the background, and the indistinguishable speakers.\n","authors":["Zhe Wang","Shilong Wu","Hang Chen","Mao-Kui He","Jun Du","Chin-Hui Lee","Jingdong Chen","Shinji Watanabe","Sabato Siniscalchi","Odette Scharenborg","Diyuan Liu","Baocai Yin","Jia Pan","Jianqing Gao","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2303.06326v1.pdf","comment":"5 pages, 4 figures, to be published in ICASSP2023"}]},"2023-03-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.06245v1","updated":"2023-03-10T23:34:14Z","published":"2023-03-10T23:34:14Z","title":"AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model","summary":"  As large dialogue models become commonplace in practice, the problems\nsurrounding high compute requirements for training, inference and larger memory\nfootprint still persists. In this work, we present AUTODIAL, a multi-task\ndialogue model that addresses the challenges of deploying dialogue model.\nAUTODIAL utilizes parallel decoders to perform tasks such as dialogue act\nprediction, domain prediction, intent prediction, and dialogue state tracking.\nUsing classification decoders over generative decoders allows AUTODIAL to\nsignificantly reduce memory footprint and achieve faster inference times\ncompared to existing generative approach namely SimpleTOD. We demonstrate that\nAUTODIAL provides 3-6x speedups during inference while having 11x fewer\nparameters on three dialogue tasks compared to SimpleTOD. Our results show that\nextending current dialogue models to have parallel decoders can be a viable\nalternative for deploying them in resource-constrained environments.\n","authors":["Prajjwal Bhargava","Pooyan Amini","Shahin Shayandeh","Chinnadhurai Sankar"],"pdf_url":"https://arxiv.org/pdf/2303.06245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06230v1","updated":"2023-03-10T22:40:15Z","published":"2023-03-10T22:40:15Z","title":"Generating Query Focused Summaries without Fine-tuning the\n  Transformer-based Pre-trained Models","summary":"  Fine-tuning the Natural Language Processing (NLP) models for each new data\nset requires higher computational time associated with increased carbon\nfootprint and cost. However, fine-tuning helps the pre-trained models adapt to\nthe latest data sets; what if we avoid the fine-tuning steps and attempt to\ngenerate summaries using just the pre-trained models to reduce computational\ntime and cost. In this paper, we tried to omit the fine-tuning steps and\ninvestigate whether the Marginal Maximum Relevance (MMR)-based approach can\nhelp the pre-trained models to obtain query-focused summaries directly from a\nnew data set that was not used to pre-train the models. First, we used topic\nmodelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to\ngenerate queries for summarization tasks. Then, using MMR, we ranked the\nsentences of the documents according to the queries. Next, we passed the ranked\nsentences to seven transformer-based pre-trained models to perform the\nsummarization tasks. Finally, we used the MMR approach again to select the\nquery relevant sentences from the generated summaries of individual pre-trained\nmodels and constructed the final summary. As indicated by the experimental\nresults, our MMR-based approach successfully ranked and selected the most\nrelevant sentences as summaries and showed better performance than the\nindividual pre-trained models.\n","authors":["Deen Abdullah","Shamanth Nayak","Gandharv Suri","Yllias Chali"],"pdf_url":"https://arxiv.org/pdf/2303.06230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08351v2","updated":"2023-03-10T20:39:14Z","published":"2023-02-16T15:11:53Z","title":"A Survey on Event-based News Narrative Extraction","summary":"  Narratives are fundamental to our understanding of the world, providing us\nwith a natural structure for knowledge representation over time. Computational\nnarrative extraction is a subfield of artificial intelligence that makes heavy\nuse of information retrieval and natural language processing techniques.\nDespite the importance of computational narrative extraction, relatively little\nscholarly work exists on synthesizing previous research and strategizing future\nresearch in the area. In particular, this article focuses on extracting news\nnarratives from an event-centric perspective. Extracting narratives from news\ndata has multiple applications in understanding the evolving information\nlandscape. This survey presents an extensive study of research in the area of\nevent-based news narrative extraction. In particular, we screened over 900\narticles that yielded 54 relevant articles. These articles are synthesized and\norganized by representation model, extraction criteria, and evaluation\napproaches. Based on the reviewed studies, we identify recent trends, open\nchallenges, and potential research lines.\n","authors":["Brian Keith Norambuena","Tanushree Mitra","Chris North"],"pdf_url":"https://arxiv.org/pdf/2302.08351v2.pdf","comment":"37 pages, 3 figures, to be published in the journal ACM CSUR"},{"id":"http://arxiv.org/abs/2303.05077v2","updated":"2023-03-10T19:54:39Z","published":"2023-03-09T07:22:07Z","title":"Learning the Legibility of Visual Text Perturbations","summary":"  Many adversarial attacks in NLP perturb inputs to produce visually similar\nstrings ('ergo' $\\rightarrow$ '$\\epsilon$rgo') which are legible to humans but\ndegrade model performance. Although preserving legibility is a necessary\ncondition for text perturbation, little work has been done to systematically\ncharacterize it; instead, legibility is typically loosely enforced via\nintuitions around the nature and extent of perturbations. Particularly, it is\nunclear to what extent can inputs be perturbed while preserving legibility, or\nhow to quantify the legibility of a perturbed string. In this work, we address\nthis gap by learning models that predict the legibility of a perturbed string,\nand rank candidate perturbations based on their legibility. To do so, we\ncollect and release LEGIT, a human-annotated dataset comprising the legibility\nof visually perturbed text. Using this dataset, we build both text- and\nvision-based models which achieve up to $0.91$ F1 score in predicting whether\nan input is legible, and an accuracy of $0.86$ in predicting which of two given\nperturbations is more legible. Additionally, we discover that legible\nperturbations from the LEGIT dataset are more effective at lowering the\nperformance of NLP models than best-known attack strategies, suggesting that\ncurrent models may be vulnerable to a broad range of perturbations beyond what\nis captured by existing visual attacks. Data, code, and models are available at\nhttps://github.com/dvsth/learning-legibility-2023.\n","authors":["Dev Seth","Rickard Stureborg","Danish Pruthi","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2303.05077v2.pdf","comment":"14 pages, 7 figures. Accepted at EACL 2023 (main, long)"},{"id":"http://arxiv.org/abs/2303.06182v1","updated":"2023-03-10T19:30:15Z","published":"2023-03-10T19:30:15Z","title":"Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert\n  (MoE) Inference","summary":"  Mixture-of-Experts (MoE) models have recently gained steam in achieving the\nstate-of-the-art performance in a wide range of tasks in computer vision and\nnatural language processing. They effectively expand the model capacity while\nincurring a minimal increase in computation cost during training. However,\ndeploying such models for inference is difficult due to their large model size\nand complex communication pattern. In this work, we provide a characterization\nof two MoE workloads, namely Language Modeling (LM) and Machine Translation\n(MT) and identify their sources of inefficiencies at deployment.\n  We propose three optimization techniques to mitigate sources of\ninefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert\nload balancing. We show that dynamic gating improves execution time by\n1.25-4$\\times$ for LM, 2-5$\\times$ for MT Encoder and 1.09-1.5$\\times$ for MT\nDecoder. It also reduces memory usage by up to 1.36$\\times$ for LM and up to\n1.1$\\times$ for MT. We further propose Expert Buffering, a new caching\nmechanism that only keeps hot, active experts in GPU memory while buffering the\nrest in CPU memory. This reduces static memory allocation by 1.47$\\times$. We\nfinally propose a load balancing methodology that provides additional\nrobustness to the workload. The code will be open-sourced upon acceptance.\n","authors":["Haiyang Huang","Newsha Ardalani","Anna Sun","Liu Ke","Hsien-Hsin S. Lee","Anjali Sridhar","Shruti Bhosale","Carole-Jean Wu","Benjamin Lee"],"pdf_url":"https://arxiv.org/pdf/2303.06182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.13290v4","updated":"2023-03-10T18:59:02Z","published":"2021-07-28T11:34:00Z","title":"Arabic aspect sentiment polarity classification using BERT","summary":"  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic aspect sentiment polarity classification task. In\nparticular, we develop a simple but effective BERT-based neural baseline to\nhandle this task. Our BERT architecture with a simple linear classification\nlayer surpassed the state-of-the-art works, according to the experimental\nresults on three different Arabic datasets. Achieving an accuracy of 89.51% on\nthe Arabic hotel reviews dataset, 73% on the Human annotated book reviews\ndataset, and 85.73% on the Arabic news dataset.\n","authors":["Mohammed M. Abdelgwad","Taysir Hassan A Soliman","Ahmed I. Taloba"],"pdf_url":"https://arxiv.org/pdf/2107.13290v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06135v1","updated":"2023-03-10T18:53:52Z","published":"2023-03-10T18:53:52Z","title":"Rewarding Chatbots for Real-World Engagement with Millions of Users","summary":"  The emergence of pretrained large language models has led to the deployment\nof a range of social chatbots for chitchat. Although these chatbots demonstrate\nlanguage ability and fluency, they are not guaranteed to be engaging and can\nstruggle to retain users. This work investigates the development of social\nchatbots that prioritize user engagement to enhance retention, specifically\nexamining the use of human feedback to efficiently develop highly engaging\nchatbots. The proposed approach uses automatic pseudo-labels collected from\nuser interactions to train a reward model that can be used to reject\nlow-scoring sample responses generated by the chatbot model at inference time.\nIntuitive evaluation metrics, such as mean conversation length (MCL), are\nintroduced as proxies to measure the level of engagement of deployed chatbots.\nA/B testing on groups of 10,000 new daily chatbot users on the Chai Research\nplatform shows that this approach increases the MCL by up to 70%, which\ntranslates to a more than 30% increase in user retention for a GPT-J 6B model.\nFuture work aims to use the reward model to realise a data fly-wheel, where the\nlatest user conversations can be used to alternately fine-tune the language\nmodel and the reward model.\n","authors":["Robert Irvine","Douglas Boubert","Vyas Raina","Adian Liusie","Vineet Mudupalli","Aliaksei Korshuk","Zongyi Liu","Fritz Cremer","Valentin Assassi","Christie-Carol Beauchamp","Xiaoding Lu","Thomas Rialan","William Beauchamp"],"pdf_url":"https://arxiv.org/pdf/2303.06135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14228v4","updated":"2023-03-10T17:24:26Z","published":"2022-11-25T16:41:59Z","title":"GPT-3-driven pedagogical agents for training children's curious\n  question-asking skills","summary":"  In order to train children's ability to ask curiosity-driven questions,\nprevious research has explored designing specific exercises relying on\nproviding semantic and linguistic cues to help formulate such questions. But\ndespite showing pedagogical efficiency, this method is still limited as it\nrelies on generating the said cues by hand, which can be a very costly process.\nIn this context, we propose to leverage advances in the natural language\nprocessing field (NLP) and investigate the efficiency of using a large language\nmodel (LLM) for automating the production of the pedagogical content of a\ncurious question-asking (QA) training. We study generating the said content\nusing the \"prompt-based\" method that consists of explaining the task to the LLM\nin natural text. We evaluate the output using human experts annotations and\ncomparisons with hand-generated content. Results suggested indeed the relevance\nand usefulness of this content. We also conduct a field study in primary school\n(75 children aged 9-10), where we evaluate children's QA performance when\nhaving this training. We compare 3 types of content : 1) hand-generated content\nthat proposes \"closed\" cues leading to predefined questions; 2) GPT-3-generated\ncontent that proposes the same type of cues; 3) GPT-3-generated content that\nproposes \"open\" cues leading to several possible questions. We see a similar QA\nperformance between the two \"closed\" trainings (showing the scalability of the\napproach using GPT-3), and a better one for participants with the \"open\"\ntraining. These results suggest the efficiency of using LLMs to support\nchildren in generating more curious questions, using a natural language\nprompting approach that affords usability by teachers and other users not\nspecialists of AI techniques. Furthermore, results also show that open-ended\ncontent may be more suitable for training curious question-asking skills.\n","authors":["Rania Abdelghani","Yen-Hsiang Wang","Xingdi Yuan","Tong Wang","Pauline Lucas","Hélène Sauzéon","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2211.14228v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01910v2","updated":"2023-03-10T17:20:17Z","published":"2022-11-03T15:43:03Z","title":"Large Language Models Are Human-Level Prompt Engineers","summary":"  By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.\n","authors":["Yongchao Zhou","Andrei Ioan Muresanu","Ziwen Han","Keiran Paster","Silviu Pitis","Harris Chan","Jimmy Ba"],"pdf_url":"https://arxiv.org/pdf/2211.01910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06074v1","updated":"2023-03-10T16:53:30Z","published":"2023-03-10T16:53:30Z","title":"Susceptibility to Influence of Large Language Models","summary":"  Two studies tested the hypothesis that a Large Language Model (LLM) can be\nused to model psychological change following exposure to influential input. The\nfirst study tested a generic mode of influence - the Illusory Truth Effect\n(ITE) - where earlier exposure to a statement (through, for example, rating its\ninterest) boosts a later truthfulness test rating. Data was collected from 1000\nhuman participants using an online experiment, and 1000 simulated participants\nusing engineered prompts and LLM completion. 64 ratings per participant were\ncollected, using all exposure-test combinations of the attributes: truth,\ninterest, sentiment and importance. The results for human participants\nreconfirmed the ITE, and demonstrated an absence of effect for attributes other\nthan truth, and when the same attribute is used for exposure and test. The same\npattern of effects was found for LLM-simulated participants. The second study\nconcerns a specific mode of influence - populist framing of news to increase\nits persuasion and political mobilization. Data from LLM-simulated participants\nwas collected and compared to previously published data from a 15-country\nexperiment on 7286 human participants. Several effects previously demonstrated\nfrom the human study were replicated by the simulated study, including effects\nthat surprised the authors of the human study by contradicting their\ntheoretical expectations (anti-immigrant framing of news decreases its\npersuasion and mobilization); but some significant relationships found in human\ndata (modulation of the effectiveness of populist framing according to relative\ndeprivation of the participant) were not present in the LLM data. Together the\ntwo studies support the view that LLMs have potential to act as models of the\neffect of influence.\n","authors":["Lewis D Griffin","Bennett Kleinberg","Maximilian Mozes","Kimberly T Mai","Maria Vau","Matthew Caldwell","Augustine Marvor-Parker"],"pdf_url":"https://arxiv.org/pdf/2303.06074v1.pdf","comment":"24 pages, 6 figures, 7 tables, 53 references"},{"id":"http://arxiv.org/abs/2303.06002v1","updated":"2023-03-10T16:03:19Z","published":"2023-03-10T16:03:19Z","title":"Is In-hospital Meta-information Useful for Abstractive Discharge Summary\n  Generation?","summary":"  During the patient's hospitalization, the physician must record daily\nobservations of the patient and summarize them into a brief document called\n\"discharge summary\" when the patient is discharged. Automated generation of\ndischarge summary can greatly relieve the physicians' burden, and has been\naddressed recently in the research community. Most previous studies of\ndischarge summary generation using the sequence-to-sequence architecture focus\non only inpatient notes for input. However, electric health records (EHR) also\nhave rich structured metadata (e.g., hospital, physician, disease, length of\nstay, etc.) that might be useful. This paper investigates the effectiveness of\nmedical meta-information for summarization tasks. We obtain four types of\nmeta-information from the EHR systems and encode each meta-information into a\nsequence-to-sequence model. Using Japanese EHRs, meta-information encoded\nmodels increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over\nthe vanilla Longformer. Also, we found that the encoded meta-information\nimproves the precisions of its related terms in the outputs. Our results showed\nthe benefit of the use of medical meta-information.\n","authors":["Kenichiro Ando","Mamoru Komachi","Takashi Okumura","Hiromasa Horiguchi","Yuji Matsumoto"],"pdf_url":"https://arxiv.org/pdf/2303.06002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05958v1","updated":"2023-03-10T14:46:23Z","published":"2023-03-10T14:46:23Z","title":"Robust Knowledge Distillation from RNN-T Models With Noisy Training\n  Labels Using Full-Sum Loss","summary":"  This work studies knowledge distillation (KD) and addresses its constraints\nfor recurrent neural network transducer (RNN-T) models. In hard distillation, a\nteacher model transcribes large amounts of unlabelled speech to train a student\nmodel. Soft distillation is another popular KD method that distills the output\nlogits of the teacher model. Due to the nature of RNN-T alignments, applying\nsoft distillation between RNN-T architectures having different posterior\ndistributions is challenging. In addition, bad teachers having high\nword-error-rate (WER) reduce the efficacy of KD. We investigate how to\neffectively distill knowledge from variable quality ASR teachers, which has not\nbeen studied before to the best of our knowledge. We show that a sequence-level\nKD, full-sum distillation, outperforms other distillation methods for RNN-T\nmodels, especially for bad teachers. We also propose a variant of full-sum\ndistillation that distills the sequence discriminative knowledge of the teacher\nleading to further improvement in WER. We conduct experiments on public\ndatasets namely SpeechStew and LibriSpeech, and on in-house production data.\n","authors":["Mohammad Zeineldeen","Kartik Audhkhasi","Murali Karthick Baskar","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2303.05958v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.07304v1","updated":"2023-03-10T14:25:29Z","published":"2023-03-10T14:25:29Z","title":"Algorithmic Ghost in the Research Shell: Large Language Models and\n  Academic Knowledge Creation in Management Research","summary":"  The paper looks at the role of large language models in academic knowledge\ncreation based on a scoping review (2018 to January 2023) of how researchers\nhave previously used the language model GPT to assist in the performance of\nacademic knowledge creation tasks beyond data analysis. These tasks include\nwriting, editing, reviewing, dataset creation and curation, which have been\ndifficult to perform using earlier ML tools. Based on a synthesis of these\npapers, this study identifies pathways for a future academic research landscape\nthat incorporates wider usage of large language models based on the current\nmodes of adoption in published articles as a Co-Writer, Research Assistant and\nRespondent.\n","authors":["Nigel Williams","Stanislav Ivanov","Dimitrios Buhalis"],"pdf_url":"https://arxiv.org/pdf/2303.07304v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.05891v1","updated":"2023-03-10T12:58:34Z","published":"2023-03-10T12:58:34Z","title":"Creation and evaluation of timelines for longitudinal user posts","summary":"  There is increasing interest to work with user generated content in social\nmedia, especially textual posts over time. Currently there is no consistent way\nof segmenting user posts into timelines in a meaningful way that improves the\nquality and cost of manual annotation. Here we propose a set of methods for\nsegmenting longitudinal user posts into timelines likely to contain interesting\nmoments of change in a user's behaviour, based on their online posting\nactivity. We also propose a novel framework for evaluating timelines and show\nits applicability in the context of two different social media datasets.\nFinally, we present a discussion of the linguistic content of highly ranked\ntimelines.\n","authors":["Anthony Hills","Adam Tsakalidis","Federico Nanni","Ioannis Zachos","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2303.05891v1.pdf","comment":"Accepted at EACL 2023 (main, long); camera-ready version"},{"id":"http://arxiv.org/abs/2208.09012v2","updated":"2023-03-10T12:32:27Z","published":"2022-08-18T18:22:42Z","title":"A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of\n  Computational Approaches","summary":"  Aspectual meaning refers to how the internal temporal structure of situations\nis presented. This includes whether a situation is described as a state or as\nan event, whether the situation is finished or ongoing, and whether it is\nviewed as a whole or with a focus on a particular phase. This survey gives an\noverview of computational approaches to modeling lexical and grammatical aspect\nalong with intuitive explanations of the necessary linguistic concepts and\nterminology. In particular, we describe the concepts of stativity, telicity,\nhabituality, perfective and imperfective, as well as influential inventories of\neventuality and situation types. We argue that because aspect is a crucial\ncomponent of semantics, especially when it comes to reporting the temporal\nstructure of situations in a precise way, future NLP approaches need to be able\nto handle and evaluate it systematically in order to achieve human-level\nlanguage understanding.\n","authors":["Annemarie Friedrich","Nianwen Xue","Alexis Palmer"],"pdf_url":"https://arxiv.org/pdf/2208.09012v2.pdf","comment":"Accepted at EACL 2023, camera ready version"},{"id":"http://arxiv.org/abs/2210.07523v2","updated":"2023-03-10T12:32:16Z","published":"2022-10-14T05:10:53Z","title":"Self-Adaptive Named Entity Recognition by Retrieving Unstructured\n  Knowledge","summary":"  Although named entity recognition (NER) helps us to extract domain-specific\nentities from text (e.g., artists in the music domain), it is costly to create\na large amount of training data or a structured knowledge base to perform\naccurate NER in the target domain. Here, we propose self-adaptive NER, which\nretrieves external knowledge from unstructured text to learn the usages of\nentities that have not been learned well. To retrieve useful knowledge for NER,\nwe design an effective two-stage model that retrieves unstructured knowledge\nusing uncertain entities as queries. Our model predicts the entities in the\ninput and then finds those of which the prediction is not confident. Then, it\nretrieves knowledge by using these uncertain entities as queries and\nconcatenates the retrieved text to the original input to revise the prediction.\nExperiments on CrossNER datasets demonstrated that our model outperforms strong\nbaselines by 2.35 points in F1 metric.\n","authors":["Kosuke Nishida","Naoki Yoshinaga","Kyosuke Nishida"],"pdf_url":"https://arxiv.org/pdf/2210.07523v2.pdf","comment":"EACL2023 (long)"},{"id":"http://arxiv.org/abs/2301.03029v5","updated":"2023-03-10T11:38:05Z","published":"2023-01-08T12:33:58Z","title":"Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case\n  Study using Latent Dirichlet Allocation Method","summary":"  Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;\nBERT-topic\n","authors":["Bernadeta Griciūtė","Lifeng Han","Hao Li","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2301.03029v5.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2301.10761v3","updated":"2023-03-10T11:04:26Z","published":"2023-01-25T18:55:05Z","title":"Fillers in Spoken Language Understanding: Computational and\n  Psycholinguistic Perspectives","summary":"  Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to survey a breadth of perspectives in a\nholistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.\n","authors":["Tanvi Dinkar","Chloé Clavel","Ioana Vasilescu"],"pdf_url":"https://arxiv.org/pdf/2301.10761v3.pdf","comment":"To appear in TAL Journal"},{"id":"http://arxiv.org/abs/2301.04312v3","updated":"2023-03-10T11:03:10Z","published":"2023-01-11T05:21:00Z","title":"Word-Graph2vec: An efficient word embedding approach on word\n  co-occurrence graph using random walk sampling","summary":"  Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n","authors":["Wenting Li","Yuanzhe Cai","Jiahong Xue","Xi Zhang","Huacan Chen","Zeyu Chen"],"pdf_url":"https://arxiv.org/pdf/2301.04312v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08014v1","updated":"2023-03-10T10:47:59Z","published":"2023-03-10T10:47:59Z","title":"Does ChatGPT resemble humans in language use?","summary":"  Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have\nshown remarkable capacities in comprehending and producing language. However,\ntheir internal workings remain a black box in cognitive terms, and it is\nunclear whether LLMs and chatbots can develop humanlike characteristics in\nlanguage use. Cognitive scientists have devised many experiments that probe,\nand have made great progress in explaining, how people process language. We\nsubjected ChatGPT to 12 of these experiments, pre-registered and with 1,000\nruns per experiment. In 10 of them, ChatGPT replicated the human pattern of\nlanguage use. It associated unfamiliar words with different meanings depending\non their forms, continued to access recently encountered meanings of ambiguous\nwords, reused recent sentence structures, reinterpreted implausible sentences\nthat were likely to have been corrupted by noise, glossed over errors, drew\nreasonable inferences, associated causality with different discourse entities\naccording to verb semantics, and accessed different meanings and retrieved\ndifferent words depending on the identity of its interlocutor. However, unlike\nhumans, it did not prefer using shorter words to convey less informative\ncontent and it did not use context to disambiguate syntactic ambiguities. We\ndiscuss how these convergences and divergences may occur in the transformer\narchitecture. Overall, these experiments demonstrate that LLM-driven chatbots\nlike ChatGPT are capable of mimicking human language processing to a great\nextent, and that they have the potential to provide insights into how people\nlearn and use language.\n","authors":["Zhenguang G. Cai","David A. Haslett","Xufeng Duan","Shuqi Wang","Martin J. Pickering"],"pdf_url":"https://arxiv.org/pdf/2303.08014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05834v1","updated":"2023-03-10T10:21:20Z","published":"2023-03-10T10:21:20Z","title":"An algebraic approach to translating Japanese","summary":"  We use Lambek's pregroups and the framework of compositional distributional\nmodels of language (\"DisCoCat\") to study translations from Japanese to English\nas pairs of functors. Adding decorations to pregroups we show how to handle\nword order changes between languages.\n","authors":["Valentin Boboc"],"pdf_url":"https://arxiv.org/pdf/2303.05834v1.pdf","comment":"20 pages, multiple diagrams and glosses"},{"id":"http://arxiv.org/abs/2211.09733v2","updated":"2023-03-10T08:06:57Z","published":"2022-11-04T14:35:56Z","title":"BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19\n  Tweets","summary":"  The free flow of information has been accelerated by the rapid development of\nsocial media technology. There has been a significant social and psychological\nimpact on the population due to the outbreak of Coronavirus disease (COVID-19).\nThe COVID-19 pandemic is one of the current events being discussed on social\nmedia platforms. In order to safeguard societies from this pandemic, studying\npeople's emotions on social media is crucial. As a result of their particular\ncharacteristics, sentiment analysis of texts like tweets remains challenging.\nSentiment analysis is a powerful text analysis tool. It automatically detects\nand analyzes opinions and emotions from unstructured data. Texts from a wide\nrange of sources are examined by a sentiment analysis tool, which extracts\nmeaning from them, including emails, surveys, reviews, social media posts, and\nweb articles. To evaluate sentiments, natural language processing (NLP) and\nmachine learning techniques are used, which assign weights to entities, topics,\nthemes, and categories in sentences or phrases. Machine learning tools learn\nhow to detect sentiment without human intervention by examining examples of\nemotions in text. In a pandemic situation, analyzing social media texts to\nuncover sentimental trends can be very helpful in gaining a better\nunderstanding of society's needs and predicting future trends. We intend to\nstudy society's perception of the COVID-19 pandemic through social media using\nstate-of-the-art BERT and Deep CNN models. The superiority of BERT models over\nother deep models in sentiment analysis is evident and can be concluded from\nthe comparison of the various research studies mentioned in this article.\n","authors":["Javad Hassannataj Joloudari","Sadiq Hussain","Mohammad Ali Nematollahi","Rouhollah Bagheri","Fatemeh Fazl","Roohallah Alizadehsani","Reza Lashgari","Ashis Talukder"],"pdf_url":"https://arxiv.org/pdf/2211.09733v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.05759v1","updated":"2023-03-10T07:55:00Z","published":"2023-03-10T07:55:00Z","title":"An Overview on Language Models: Recent Developments and Outlook","summary":"  Language modeling studies the probability distributions over strings of\ntexts. It is one of the most fundamental tasks in natural language processing\n(NLP). It has been widely used in text generation, speech recognition, machine\ntranslation, etc. Conventional language models (CLMs) aim to predict the\nprobability of linguistic sequences in a causal manner. In contrast,\npre-trained language models (PLMs) cover broader concepts and can be used in\nboth causal sequential modeling and fine-tuning for downstream applications.\nPLMs have their own training paradigms (usually self-supervised) and serve as\nfoundation models in modern NLP systems. This overview paper provides an\nintroduction to both CLMs and PLMs from five aspects, i.e., linguistic units,\nstructures, training methods, evaluation methods, and applications.\nFurthermore, we discuss the relationship between CLMs and PLMs and shed light\non the future directions of language modeling in the pre-trained era.\n","authors":["Chengwei Wei","Yun-Cheng Wang","Bin Wang","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2303.05759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05758v1","updated":"2023-03-10T07:52:28Z","published":"2023-03-10T07:52:28Z","title":"MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems","summary":"  Automatic speech recognition (ASR) systems based on deep neural networks are\nweak against adversarial perturbations. We propose mixPGD adversarial training\nmethod to improve the robustness of the model for ASR systems. In standard\nadversarial training, adversarial samples are generated by leveraging\nsupervised or unsupervised methods. We merge the capabilities of both\nsupervised and unsupervised approaches in our method to generate new\nadversarial samples which aid in improving model robustness. Extensive\nexperiments and comparison across various state-of-the-art defense methods and\nadversarial attacks have been performed to show that mixPGD gains 4.1% WER of\nbetter performance than previous best performing models under white-box\nadversarial attack setting. We tested our proposed defense method against both\nwhite-box and transfer based black-box attack settings to ensure that our\ndefense strategy is robust against various types of attacks. Empirical results\non several adversarial attacks validate the effectiveness of our proposed\napproach.\n","authors":["Aminul Huq","Weiyi Zhang","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2303.05758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08016v1","updated":"2023-03-10T06:10:53Z","published":"2023-03-10T06:10:53Z","title":"Detection of Abuse in Financial Transaction Descriptions Using Machine\n  Learning","summary":"  Since introducing changes to the New Payments Platform (NPP) to include\nlonger messages as payment descriptions, it has been identified that people are\nnow using it for communication, and in some cases, the system was being used as\na targeted form of domestic and family violence. This type of tech-assisted\nabuse poses new challenges in terms of identification, actions and approaches\nto rectify this behaviour. Commonwealth Bank of Australia's Artificial\nIntelligence Labs team (CBA AI Labs) has developed a new system using advances\nin deep learning models for natural language processing (NLP) to create a\npowerful abuse detector that periodically scores all the transactions, and\nidentifies cases of high-risk abuse in millions of records. In this paper, we\ndescribe the problem of tech-assisted abuse in the context of banking services,\noutline the developed model and its performance, and the operating framework\nmore broadly.\n","authors":["Anna Leontjeva","Genevieve Richards","Kaavya Sriskandaraja","Jessica Perchman","Luiz Pizzato"],"pdf_url":"https://arxiv.org/pdf/2303.08016v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.05707v1","updated":"2023-03-10T05:22:39Z","published":"2023-03-10T05:22:39Z","title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler\n  and Multiple Choice Modeling","summary":"  Video-and-language understanding has a variety of applications in the\nindustry, such as video question answering, text-video retrieval and\nmulti-label classification. Existing video-and-language understanding methods\ngenerally adopt heavy multi-modal encoders and feature fusion modules, which\nconsume large amounts of GPU memory. Especially, they have difficulty dealing\nwith dense video frames or long text that are prevalent in industrial\napplications. In this paper, we propose MuLTI, a highly accurate and\nmemory-efficient video-and-language understanding model that achieves efficient\nand effective feature fusion through feature sampling and attention modules.\nTherefore, MuLTI can handle longer sequences with limited GPU memory. Then, we\nintroduce an attention-based adapter to the encoders, which finetunes the\nshallow features to improve the model's performance with low GPU memory\nconsumption. Finally, to further improve the model's performance, we introduce\na new pretraining task named Multiple Choice Modeling to bridge the task gap\nbetween pretraining and downstream tasks and enhance the model's ability to\nalign the video and the text. Benefiting from the efficient feature fusion\nmodule, the attention-based adapter and the new pretraining task, MuLTI\nachieves state-of-the-art performance on multiple datasets. Implementation and\npretrained models will be released.\n","authors":["Jiaqi Xu","Bo Liu","Yunkuo Chen","Mengli Cheng","Xing Shi"],"pdf_url":"https://arxiv.org/pdf/2303.05707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08233v2","updated":"2023-03-10T04:11:56Z","published":"2022-11-14T13:35:01Z","title":"Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach\n  for Speech Emotion Recognition","summary":"  Speech emotion recognition (SER) plays a vital role in improving the\ninteractions between humans and machines by inferring human emotion and\naffective states from speech signals. Whereas recent works primarily focus on\nmining spatiotemporal information from hand-crafted features, we explore how to\nmodel the temporal patterns of speech emotions from dynamic temporal scales.\nTowards that goal, we introduce a novel temporal emotional modeling approach\nfor SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),\nwhich learns multi-scale contextual affective representations from various time\nscales. Specifically, TIM-Net first employs temporal-aware blocks to learn\ntemporal affective representation, then integrates complementary information\nfrom the past and the future to enrich contextual representations, and finally,\nfuses multiple time scale features for better adaptation to the emotional\nvariation. Extensive experimental results on six benchmark SER datasets\ndemonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%\nimprovements of the average UAR and WAR over the second-best on each corpus.\nThe source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.\n","authors":["Jiaxin Ye","Xin-cheng Wen","Yujie Wei","Yong Xu","Kunhong Liu","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2211.08233v2.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.05670v1","updated":"2023-03-10T02:52:13Z","published":"2023-03-10T02:52:13Z","title":"Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence\n  Reasoning","summary":"  Due to their similarity-based learning objectives, pretrained sentence\nencoders often internalize stereotypical assumptions that reflect the social\nbiases that exist within their training corpora. In this paper, we describe\nseveral kinds of stereotypes concerning different communities that are present\nin popular sentence representation models, including pretrained next sentence\nprediction and contrastive sentence representation models. We compare such\nmodels to textual entailment models that learn language logic for a variety of\ndownstream language understanding tasks. By comparing strong pretrained models\nbased on text similarity with textual entailment learning, we conclude that the\nexplicit logic learning with textual entailment can significantly reduce bias\nand improve the recognition of social communities, without an explicit\nde-biasing process\n","authors":["Hongyin Luo","James Glass"],"pdf_url":"https://arxiv.org/pdf/2303.05670v1.pdf","comment":"Accepted by EACL 2023"},{"id":"http://arxiv.org/abs/2303.03387v2","updated":"2023-03-10T02:09:29Z","published":"2023-03-02T17:30:43Z","title":"CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a\n  Context Synergized Hyperbolic Network","summary":"  The tremendous growth of social media users interacting in online\nconversations has also led to significant growth in hate speech. Most of the\nprior works focus on detecting explicit hate speech, which is overt and\nleverages hateful phrases, with very little work focusing on detecting hate\nspeech that is implicit or denotes hatred through indirect or coded language.\nIn this paper, we present CoSyn, a user- and conversational-context synergized\nnetwork for detecting implicit hate speech in online conversation trees. CoSyn\nfirst models the user's personal historical and social context using a novel\nhyperbolic Fourier attention mechanism and hyperbolic graph convolution\nnetwork. Next, we jointly model the user's personal context and the\nconversational context using a novel context interaction mechanism in the\nhyperbolic space that clearly captures the interplay between the two and makes\nindependent assessments on the amounts of information to be retrieved from both\ncontexts. CoSyn performs all operations in the hyperbolic space to account for\nthe scale-free dynamics of social media. We demonstrate the effectiveness of\nCoSyn both qualitatively and quantitatively on an open-source hate speech\ndataset with Twitter conversations and show that CoSyn outperforms all our\nbaselines in detecting implicit hate speech with absolute improvements in the\nrange of 8.15% - 19.50%.\n","authors":["Sreyan Ghosh","Manan Suri","Purva Chiniya","Utkarsh Tyagi","Sonal Kumar","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2303.03387v2.pdf","comment":"Under review at IJCAI 2023"},{"id":"http://arxiv.org/abs/2210.03629v3","updated":"2023-03-10T01:00:17Z","published":"2022-10-06T01:00:32Z","title":"ReAct: Synergizing Reasoning and Acting in Language Models","summary":"  While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io\n","authors":["Shunyu Yao","Jeffrey Zhao","Dian Yu","Nan Du","Izhak Shafran","Karthik Narasimhan","Yuan Cao"],"pdf_url":"https://arxiv.org/pdf/2210.03629v3.pdf","comment":"v3 is the ICLR camera ready version with some typos fixed. Project\n  site with code: https://react-lm.github.io"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2302.08351v2","updated":"2023-03-10T20:39:14Z","published":"2023-02-16T15:11:53Z","title":"A Survey on Event-based News Narrative Extraction","summary":"  Narratives are fundamental to our understanding of the world, providing us\nwith a natural structure for knowledge representation over time. Computational\nnarrative extraction is a subfield of artificial intelligence that makes heavy\nuse of information retrieval and natural language processing techniques.\nDespite the importance of computational narrative extraction, relatively little\nscholarly work exists on synthesizing previous research and strategizing future\nresearch in the area. In particular, this article focuses on extracting news\nnarratives from an event-centric perspective. Extracting narratives from news\ndata has multiple applications in understanding the evolving information\nlandscape. This survey presents an extensive study of research in the area of\nevent-based news narrative extraction. In particular, we screened over 900\narticles that yielded 54 relevant articles. These articles are synthesized and\norganized by representation model, extraction criteria, and evaluation\napproaches. Based on the reviewed studies, we identify recent trends, open\nchallenges, and potential research lines.\n","authors":["Brian Keith Norambuena","Tanushree Mitra","Chris North"],"pdf_url":"https://arxiv.org/pdf/2302.08351v2.pdf","comment":"37 pages, 3 figures, to be published in the journal ACM CSUR"},{"id":"http://arxiv.org/abs/2303.05847v1","updated":"2023-03-10T10:42:21Z","published":"2023-03-10T10:42:21Z","title":"Gradient Coordination for Quantifying and Maximizing Knowledge\n  Transference in Multi-Task Learning","summary":"  Multi-task learning (MTL) has been widely applied in online advertising and\nrecommender systems. To address the negative transfer issue, recent studies\nhave proposed optimization methods that thoroughly focus on the gradient\nalignment of directions or magnitudes. However, since prior study has proven\nthat both general and specific knowledge exist in the limited shared capacity,\noveremphasizing on gradient alignment may crowd out task-specific knowledge,\nand vice versa. In this paper, we propose a transference-driven approach CoGrad\nthat adaptively maximizes knowledge transference via Coordinated Gradient\nmodification. We explicitly quantify the transference as loss reduction from\none task to another, and then derive an auxiliary gradient from optimizing it.\nWe perform the optimization by incorporating this gradient into original task\ngradients, making the model automatically maximize inter-task transfer and\nminimize individual losses. Thus, CoGrad can harmonize between general and\nspecific knowledge to boost overall performance. Besides, we introduce an\nefficient approximation of the Hessian matrix, making CoGrad computationally\nefficient and simple to implement. Both offline and online experiments verify\nthat CoGrad significantly outperforms previous methods.\n","authors":["Xuanhua Yang","Jianxin Zhao","Shaoguo Liu","Liang Wang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.05847v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.05812v1","updated":"2023-03-10T09:39:18Z","published":"2023-03-10T09:39:18Z","title":"Semi-supervised Adversarial Learning for Complementary Item\n  Recommendation","summary":"  Complementary item recommendations are a ubiquitous feature of modern\ne-commerce sites. Such recommendations are highly effective when they are based\non collaborative signals like co-purchase statistics. In certain online\nmarketplaces, however, e.g., on online auction sites, constantly new items are\nadded to the catalog. In such cases, complementary item recommendations are\noften based on item side-information due to a lack of interaction data. In this\nwork, we propose a novel approach that can leverage both item side-information\nand labeled complementary item pairs to generate effective complementary\nrecommendations for cold items, i.e., for items for which no co-purchase\nstatistics yet exist. Given that complementary items typically have to be of a\ndifferent category than the seed item, we technically maintain a latent space\nfor each item category. Simultaneously, we learn to project distributed item\nrepresentations into these category spaces to determine suitable\nrecommendations. The main learning process in our architecture utilizes labeled\npairs of complementary items. In addition, we adopt ideas from Cycle Generative\nAdversarial Networks (CycleGAN) to leverage available item information even in\ncase no labeled data exists for a given item and category. Experiments on three\ne-commerce datasets show that our method is highly effective.\n","authors":["Koby Bibas","Oren Sar Shalom","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2303.05812v1.pdf","comment":"ACM Web Conference 2023"},{"id":"http://arxiv.org/abs/2302.03328v2","updated":"2023-03-10T03:36:18Z","published":"2023-02-07T09:11:17Z","title":"Multi-Task Recommendations with Reinforcement Learning","summary":"  In recent years, Multi-task Learning (MTL) has yielded immense success in\nRecommender System (RS) applications. However, current MTL-based recommendation\nmodels tend to disregard the session-wise patterns of user-item interactions\nbecause they are predominantly constructed based on item-wise datasets.\nMoreover, balancing multiple objectives has always been a challenge in this\nfield, which is typically avoided via linear estimations in existing works. To\naddress these issues, in this paper, we propose a Reinforcement Learning (RL)\nenhanced MTL framework, namely RMTL, to combine the losses of different\nrecommendation tasks using dynamic weights. To be specific, the RMTL structure\ncan address the two aforementioned issues by (i) constructing an MTL\nenvironment from session-wise interactions and (ii) training multi-task\nactor-critic network structure, which is compatible with most existing\nMTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL\nloss function using the weights generated by critic networks. Experiments on\ntwo real-world public datasets demonstrate the effectiveness of RMTL with a\nhigher AUC against state-of-the-art MTL-based recommendation models.\nAdditionally, we evaluate and validate RMTL's compatibility and transferability\nacross various MTL models.\n","authors":["Ziru Liu","Jiejie Tian","Qingpeng Cai","Xiangyu Zhao","Jingtong Gao","Shuchang Liu","Dayou Chen","Tonghao He","Dong Zheng","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2302.03328v2.pdf","comment":"TheWebConf2023"},{"id":"http://arxiv.org/abs/2303.05648v1","updated":"2023-03-10T01:49:56Z","published":"2023-03-10T01:49:56Z","title":"Pacos: Modeling Users' Interpretable and Context-Dependent Choices in\n  Preference Reversals","summary":"  Choice problems refer to selecting the best choices from several items, and\nlearning users' preferences in choice problems is of great significance in\nunderstanding the decision making mechanisms and providing personalized\nservices. Existing works typically assume that people evaluate items\nindependently. In practice, however, users' preferences depend on the market in\nwhich items are placed, which is known as context effects; and the order of\nusers' preferences for two items may even be reversed, which is referred to\npreference reversals. In this work, we identify three factors contributing to\ncontext effects: users' adaptive weights, the inter-item comparison, and\ndisplay positions. We propose a context-dependent preference model named Pacos\nas a unified framework for addressing three factors simultaneously, and\nconsider two design methods including an additive method with high\ninterpretability and an ANN-based method with high accuracy. We study the\nconditions for preference reversals to occur and provide an theoretical proof\nof the effectiveness of Pacos in addressing preference reversals. Experimental\nresults show that the proposed method has better performance than prior works\nin predicting users' choices, and has great interpretability to help understand\nthe cause of preference reversals.\n","authors":["Qingming Li","H. Vicky Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.05648v1.pdf","comment":"29 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.04392v2","updated":"2023-03-10T01:31:11Z","published":"2023-03-08T05:53:33Z","title":"Achievable Rates and Low-Complexity Encoding of Posterior Matching for\n  the BSC","summary":"  Horstein, Burnashev, Shayevitz and Feder, Naghshvar et al. and others have\nstudied sequential transmission of a K-bit message over the binary symmetric\nchannel (BSC) with full, noiseless feedback using posterior matching. Yang et\nal. provide an improved lower bound on the achievable rate using martingale\nanalysis that relies on the small-enough difference (SED) partitioning\nintroduced by Naghshvar et al. SED requires a relatively complex encoder and\ndecoder. To reduce complexity, this paper replaces SED with relaxed constraints\nthat admit the small enough absolute difference (SEAD) partitioning rule. The\nmain analytical results show that achievable-rate bounds higher than those\nfound by Yang et al. are possible even under the new constraints, which are\nless restrictive than SED. The new analysis does not use martingale theory for\nthe confirmation phase and applies a surrogate channel technique to tighten the\nresults. An initial systematic transmission further increases the achievable\nrate bound. The simplified encoder associated with SEAD has a complexity below\norder O(K^2) and allows simulations for message sizes of at least 1000 bits.\nFor example, simulations achieve 99% of of the channel's 0.50-bit capacity with\nan average block size of 200 bits for a target codeword error rate of 10^(-3).\n","authors":["Amaael Antonini","Rita Gimelshein","Richard Wesel"],"pdf_url":"https://arxiv.org/pdf/2303.04392v2.pdf","comment":"This paper consists of 26 pages and contains 6 figures. An earlier\n  version of the algorithm included in this paper was published at the 2020\n  IEEE International Symposium on Information Theory (ISIT), (DOI:\n  10.1109/ISIT44484.2020.9174232)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2210.07839v3","updated":"2023-03-10T07:37:34Z","published":"2022-10-02T07:29:57Z","title":"Contrastive Audio-Visual Masked Autoencoder","summary":"  In this paper, we first extend the recent Masked Auto-Encoder (MAE) model\nfrom a single modality to audio-visual multi-modalities. Subsequently, we\npropose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining\ncontrastive learning and masked data modeling, two major self-supervised\nlearning frameworks, to learn a joint and coordinated audio-visual\nrepresentation. Our experiments show that the contrastive audio-visual\ncorrespondence learning objective not only enables the model to perform\naudio-visual retrieval tasks, but also helps the model learn a better joint\nrepresentation. As a result, our fully self-supervised pretrained CAV-MAE\nachieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the\nprevious best supervised pretrained model on AudioSet in the audio-visual event\nclassification task. Code and pretrained models are at\nhttps://github.com/yuangongnd/cav-mae.\n","authors":["Yuan Gong","Andrew Rouditchenko","Alexander H. Liu","David Harwath","Leonid Karlinsky","Hilde Kuehne","James Glass"],"pdf_url":"https://arxiv.org/pdf/2210.07839v3.pdf","comment":"Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained\n  models are at https://github.com/yuangongnd/cav-mae"},{"id":"http://arxiv.org/abs/2303.05744v1","updated":"2023-03-10T07:08:24Z","published":"2023-03-10T07:08:24Z","title":"QVRF: A Quantization-error-aware Variable Rate Framework for Learned\n  Image Compression","summary":"  Learned image compression has exhibited promising compression performance,\nbut variable bitrates over a wide range remain a challenge. State-of-the-art\nvariable rate methods compromise the loss of model performance and require\nnumerous additional parameters. In this paper, we present a\nQuantization-error-aware Variable Rate Framework (QVRF) that utilizes a\nunivariate quantization regulator a to achieve wide-range variable rates within\na single model. Specifically, QVRF defines a quantization regulator vector\ncoupled with predefined Lagrange multipliers to control quantization error of\nall latent representation for discrete variable rates. Additionally, the\nreparameterization method makes QVRF compatible with a round quantizer.\nExhaustive experiments demonstrate that existing fixed-rate VAE-based methods\nequipped with QVRF can achieve wide-range continuous variable rates within a\nsingle model without significant performance degradation. Furthermore, QVRF\noutperforms contemporary variable-rate methods in rate-distortion performance\nwith minimal additional parameters.\n","authors":["Kedeng Tong","Yaojun Wu","Yue Li","Kai Zhang","Li Zhang","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2303.05744v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.05707v1","updated":"2023-03-10T05:22:39Z","published":"2023-03-10T05:22:39Z","title":"MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler\n  and Multiple Choice Modeling","summary":"  Video-and-language understanding has a variety of applications in the\nindustry, such as video question answering, text-video retrieval and\nmulti-label classification. Existing video-and-language understanding methods\ngenerally adopt heavy multi-modal encoders and feature fusion modules, which\nconsume large amounts of GPU memory. Especially, they have difficulty dealing\nwith dense video frames or long text that are prevalent in industrial\napplications. In this paper, we propose MuLTI, a highly accurate and\nmemory-efficient video-and-language understanding model that achieves efficient\nand effective feature fusion through feature sampling and attention modules.\nTherefore, MuLTI can handle longer sequences with limited GPU memory. Then, we\nintroduce an attention-based adapter to the encoders, which finetunes the\nshallow features to improve the model's performance with low GPU memory\nconsumption. Finally, to further improve the model's performance, we introduce\na new pretraining task named Multiple Choice Modeling to bridge the task gap\nbetween pretraining and downstream tasks and enhance the model's ability to\nalign the video and the text. Benefiting from the efficient feature fusion\nmodule, the attention-based adapter and the new pretraining task, MuLTI\nachieves state-of-the-art performance on multiple datasets. Implementation and\npretrained models will be released.\n","authors":["Jiaqi Xu","Bo Liu","Yunkuo Chen","Mengli Cheng","Xing Shi"],"pdf_url":"https://arxiv.org/pdf/2303.05707v1.pdf","comment":null}]},"2023-03-09T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.05582v1","updated":"2023-03-09T21:13:32Z","published":"2023-03-09T21:13:32Z","title":"Generalization analysis of an unfolding network for analysis-based\n  Compressed Sensing","summary":"  Unfolding networks have shown promising results in the Compressed Sensing\n(CS) field. Yet, the investigation of their generalization ability is still in\nits infancy. In this paper, we perform generalization analysis of a\nstate-of-the-art ADMM-based unfolding network, which jointly learns a decoder\nfor CS and a sparsifying redundant analysis operator. To this end, we first\nimpose a structural constraint on the learnable sparsifier, which parametrizes\nthe network's hypothesis class. For the latter, we estimate its Rademacher\ncomplexity. With this estimate in hand, we deliver generalization error bounds\nfor the examined network. Finally, the validity of our theory is assessed and\nnumerical comparisons to a state-of-the-art unfolding network are made, on\nsynthetic and real-world datasets. Our experimental results demonstrate that\nour proposed framework complies with our theoretical findings and outperforms\nthe baseline, consistently for all datasets.\n","authors":["Vicky Kouni","Yannis Panagakis"],"pdf_url":"https://arxiv.org/pdf/2303.05582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05575v1","updated":"2023-03-09T20:51:18Z","published":"2023-03-09T20:51:18Z","title":"Evaluating the Robustness of Conversational Recommender Systems by\n  Adversarial Examples","summary":"  Conversational recommender systems (CRSs) are improving rapidly, according to\nthe standard recommendation accuracy metrics. However, it is essential to make\nsure that these systems are robust in interacting with users including regular\nand malicious users who want to attack the system by feeding the system\nmodified input data. In this paper, we propose an adversarial evaluation scheme\nincluding four scenarios in two categories and automatically generate\nadversarial examples to evaluate the robustness of these systems in the face of\ndifferent input data. By executing these adversarial examples we can compare\nthe ability of different conversational recommender systems to satisfy the\nuser's preferences. We evaluate three CRSs by the proposed adversarial examples\non two datasets. Our results show that none of these systems are robust and\nreliable to the adversarial examples.\n","authors":["Ali Montazeralghaem","James Allan"],"pdf_url":"https://arxiv.org/pdf/2303.05575v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2302.09051v3","updated":"2023-03-09T17:25:10Z","published":"2023-02-17T18:31:31Z","title":"Complex QA and language models hybrid architectures, Survey","summary":"  This paper reviews the state-of-the-art of hybrid language models\narchitectures and strategies for \"complex\" question-answering (QA, CQA, CPS).\nLarge Language Models (LLM) are good at leveraging public data on standard\nproblems but once you want to tackle more specific complex questions or\nproblems you may need specific architecture, knowledge, skills, methods,\nsensitive data protection, explainability, human approval and versatile\nfeedback... We identify key elements augmenting LLM to solve complex questions\nor problems. We extend findings from the robust community edited research\npapers BIG, BLOOM and HELM which open source, benchmark and analyze limits and\nchallenges of LLM in terms of tasks complexity and strict evaluation on\naccuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like\nChatGPT and GALACTICA have allowed non-specialists to grasp the great potential\nas well as the equally strong limitations of language models in complex QA.\nHybridizing these models with different components could allow to overcome\nthese different limits and go much further. We discuss some challenges\nassociated with complex QA, including domain adaptation, decomposition and\nefficient multi-step QA, long form and non-factoid QA, safety and\nmulti-sensitivity data protection, multimodal search, hallucinations,\nexplainability and truthfulness, temproal reasoning. Therefore, we analyze\ncurrent solutions and promising research trends, using elements such as: hybrid\nLLM architectures, active human reinforcement learning supervised with AI,\nprompting adaptation, neuro-symbolic and structured knowledge grounding,\nprogram synthesis, iterated decomposition and others.\n","authors":["Xavier Daull","Patrice Bellot","Emmanuel Bruno","Vincent Martin","Elisabeth Murisasco"],"pdf_url":"https://arxiv.org/pdf/2302.09051v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06016v1","updated":"2023-03-09T12:13:46Z","published":"2023-03-09T12:13:46Z","title":"Modelling Projection Bias in Intertemporal Choices: A Prospect Theory\n  Based Approach","summary":"  Users often face bundle promotions when purchasing, where they have to select\nbetween two options: buy the single item at full price, or buy the bundle at a\ndiscount. In this scenario, users' preferences are usually influenced by the\nprojection bias, that is, users often believe that their future preferences are\nsimilar to their current preferences, causing them to make irrational and\nshort-sighted decisions. It is of great significance to analyze the effect of\nthe projection bias on users' preferences, and this study may help understand\nusers' decision-making process and provide bundling and pricing strategies for\nsellers. Prior works typically use a linear bias model for qualitative\nanalysis, and they cannot quantitatively calculate users' nonlinear and\npersonalized bias. In this work, we propose Pobe, a projection bias-embedded\npreference model to accurately predict users' choices. The proposed Pobe\nintroduces the prospect theory to analyze users' irrational decisions, and\nutilizes the weight function to handle users' nonlinear and personalized bias.\nBased on the proposed Pobe, we also study the impact of items' correlations or\ndiscount prices on users' choices, and provide four bundling strategies.\nExperimental results show that the proposed method can achieve better\nperformance than prior works, especially when only small data is available.\n","authors":["Qingming Li","H. Vicky Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.06016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14233v2","updated":"2023-03-09T10:56:28Z","published":"2021-09-29T07:14:22Z","title":"A Next Basket Recommendation Reality Check","summary":"  The goal of a next basket recommendation (NBR) system is to recommend items\nfor the next basket for a user, based on the sequence of their prior baskets.\nRecently, a number of methods with complex modules have been proposed that\nclaim state-of-the-art performance. They rarely look into the predicted basket\nand just provide intuitive reasons for the observed improvements, e.g., better\nrepresentation, capturing intentions or relations, etc. We provide a novel\nangle on the evaluation of next basket recommendation methods, centered on the\ndistinction between repetition and exploration: the next basket is typically\ncomposed of previously consumed items (i.e., repeat items) and new items (i.e,\nexplore items). We propose a set of metrics that measure the repeat/explore\nratio and performance of NBR models. Using these new metrics, we analyze\nstate-of-the-art NBR models. The results of our analysis help to clarify the\nextent of the actual progress achieved by existing NBR methods as well as the\nunderlying reasons for the improvements. Overall, our work sheds light on the\nevaluation problem of NBR and provides useful insights into the model design\nfor this task.\n","authors":["Ming Li","Sami Jullien","Mozhdeh Ariannezhad","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2109.14233v2.pdf","comment":"This paper has been accepted to ACM TOIS"},{"id":"http://arxiv.org/abs/2303.05103v1","updated":"2023-03-09T08:23:56Z","published":"2023-03-09T08:23:56Z","title":"Algorithmic neutrality","summary":"  Bias infects the algorithms that wield increasing control over our lives.\nPredictive policing systems overestimate crime in communities of color; hiring\nalgorithms dock qualified female candidates; and facial recognition software\nstruggles to recognize dark-skinned faces. Algorithmic bias has received\nsignificant attention. Algorithmic neutrality, in contrast, has been largely\nneglected. Algorithmic neutrality is my topic. I take up three questions. What\nis algorithmic neutrality? Is algorithmic neutrality possible? When we have an\neye to algorithmic neutrality, what can we learn about algorithmic bias? To\nanswer these questions in concrete terms, I work with a case study: search\nengines. Drawing on work about neutrality in science, I say that a search\nengine is neutral only if certain values, like political ideologies or the\nfinancial interests of the search engine operator, play no role in how the\nsearch engine ranks pages. Search neutrality, I argue, is impossible. Its\nimpossibility seems to threaten the significance of search bias: if no search\nengine is neutral, then every search engine is biased. To defuse this threat, I\ndistinguish two forms of bias, failing-on-its-own-terms bias and other-values\nbias. This distinction allows us to make sense of search bias, and capture its\nnormative complexion, despite the impossibility of neutrality.\n","authors":["Milo Phillips-Brown"],"pdf_url":"https://arxiv.org/pdf/2303.05103v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2303.05039v1","updated":"2023-03-09T05:20:17Z","published":"2023-03-09T05:20:17Z","title":"Improving Recommendation Systems with User Personality Inferred from\n  Product Reviews","summary":"  Personality is a psychological factor that reflects people's preferences,\nwhich in turn influences their decision-making. We hypothesize that accurate\nmodeling of users' personalities improves recommendation systems' performance.\nHowever, acquiring such personality profiles is both sensitive and expensive.\nWe address this problem by introducing a novel method to automatically extract\npersonality profiles from public product review text. We then design and assess\nthree context-aware recommendation architectures that leverage the profiles to\ntest our hypothesis.\n  Experiments on our two newly contributed personality datasets --\nAmazon-beauty and Amazon-music -- validate our hypothesis, showing performance\nboosts of 3--28%.Our analysis uncovers that varying personality types\ncontribute differently to recommendation performance: open and extroverted\npersonalities are most helpful in music recommendation, while a conscientious\npersonality is most helpful in beauty product recommendation. The dataset is\navailable at https://github.com/XinyuanLu00/IRS-WSDM2023-personality-dataset.\n","authors":["Xinyuan Lu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2303.05039v1.pdf","comment":"Accepted by IRS@WSDM'23"},{"id":"http://arxiv.org/abs/2301.00767v2","updated":"2023-03-09T04:40:23Z","published":"2022-12-27T08:09:45Z","title":"A Survey on Federated Recommendation Systems","summary":"  Federated learning has recently been applied to recommendation systems to\nprotect user privacy. In federated learning settings, recommendation systems\ncan train recommendation models only collecting the intermediate parameters\ninstead of the real user data, which greatly enhances the user privacy. Beside,\nfederated recommendation systems enable to collaborate with other data\nplatforms to improve recommended model performance while meeting the regulation\nand privacy constraints. However, federated recommendation systems faces many\nnew challenges such as privacy, security, heterogeneity and communication\ncosts. While significant research has been conducted in these areas, gaps in\nthe surveying literature still exist. In this survey, we-(1) summarize some\ncommon privacy mechanisms used in federated recommendation systems and discuss\nthe advantages and limitations of each mechanism; (2) review some robust\naggregation strategies and several novel attacks against security; (3)\nsummarize some approaches to address heterogeneity and communication costs\nproblems; (4)introduce some open source platforms that can be used to build\nfederated recommendation systems; (5) present some prospective research\ndirections in the future. This survey can guide researchers and practitioners\nunderstand the research progress in these areas.\n","authors":["Zehua Sun","Yonghui Xu","Yong Liu","Wei He","Lanju Kong","Fangzhao Wu","Yali Jiang","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2301.00767v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.05322v1","updated":"2023-03-09T15:13:46Z","published":"2023-03-09T15:13:46Z","title":"Improving Few-Shot Learning for Talking Face System with TTS Data\n  Augmentation","summary":"  Audio-driven talking face has attracted broad interest from academia and\nindustry recently. However, data acquisition and labeling in audio-driven\ntalking face are labor-intensive and costly. The lack of data resource results\nin poor synthesis effect. To alleviate this issue, we propose to use TTS\n(Text-To-Speech) for data augmentation to improve few-shot ability of the\ntalking face system. The misalignment problem brought by the TTS audio is\nsolved with the introduction of soft-DTW, which is first adopted in the talking\nface task. Moreover, features extracted by HuBERT are explored to utilize\nunderlying information of audio, and found to be superior over other features.\nThe proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW score\nand user study preference repectively over the baseline model, which shows the\neffectiveness of improving few-shot learning for talking face system with TTS\naugmentation.\n","authors":["Qi Chen","Ziyang Ma","Tao Liu","Xu Tan","Qu Lu","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2303.05322v1.pdf","comment":"4 pages. Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.05007v1","updated":"2023-03-09T03:16:04Z","published":"2023-03-09T03:16:04Z","title":"Towards Robust Image-in-Audio Deep Steganography","summary":"  The field of steganography has experienced a surge of interest due to the\nrecent advancements in AI-powered techniques, particularly in the context of\nmultimodal setups that enable the concealment of signals within signals of a\ndifferent nature. The primary objectives of all steganographic methods are to\nachieve perceptual transparency, robustness, and large embedding capacity -\nwhich often present conflicting goals that classical methods have struggled to\nreconcile. This paper extends and enhances an existing image-in-audio deep\nsteganography method by focusing on improving its robustness. The proposed\nenhancements include modifications to the loss function, utilization of the\nShort-Time Fourier Transform (STFT), introduction of redundancy in the encoding\nprocess for error correction, and buffering of additional information in the\npixel subconvolution operation. The results demonstrate that our approach\noutperforms the existing method in terms of robustness and perceptual\ntransparency.\n","authors":["Jaume Ros Alonso","Margarita Geleta","Jordi Pons","Xavier Giro-i-Nieto"],"pdf_url":"https://arxiv.org/pdf/2303.05007v1.pdf","comment":"8 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.04027v2","updated":"2023-03-09T02:58:24Z","published":"2023-03-07T16:39:09Z","title":"BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud\n  Compression","summary":"  The large amount of data collected by LiDAR sensors brings the issue of LiDAR\npoint cloud compression (PCC). Previous works on LiDAR PCC have used range\nimage representations and followed the predictive coding paradigm to create a\nbasic prototype of a coding framework. However, their prediction methods give\nan inaccurate result due to the negligence of invalid pixels in range images\nand the omission of future frames in the time step. Moreover, their handcrafted\ndesign of residual coding methods could not fully exploit spatial redundancy.\nTo remedy this, we propose a coding framework BIRD-PCC. Our prediction module\nis aware of the coordinates of invalid pixels in range images and takes a\nbidirectional scheme. Also, we introduce a deep-learned residual coding module\nthat can further exploit spatial redundancy within a residual frame.\nExperiments conducted on SemanticKITTI and KITTI-360 datasets show that\nBIRD-PCC outperforms other methods in most bitrate conditions and generalizes\nwell to unseen environments.\n","authors":["Chia-Sheng Liu","Jia-Fong Yeh","Hao Hsu","Hung-Ting Su","Ming-Sui Lee","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.04027v2.pdf","comment":"Accepted to ICASSP 2023"}]},"2023-03-08T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.04766v1","updated":"2023-03-08T18:03:51Z","published":"2023-03-08T18:03:51Z","title":"FastFill: Efficient Compatible Model Update","summary":"  In many retrieval systems the original high dimensional data (e.g., images)\nis mapped to a lower dimensional feature through a learned embedding model. The\ntask of retrieving the most similar data from a gallery set to a given query\ndata is performed through a similarity comparison on features. When the\nembedding model is updated, it might produce features that are not\ncomparable/compatible with features already in the gallery computed with the\nold model. Subsequently, all features in the gallery need to be re-computed\nusing the new embedding model -- a computationally expensive process called\nbackfilling. Recently, compatible representation learning methods have been\nproposed to avoid backfilling. Despite their relative success, there is an\ninherent trade-off between the new model performance and its compatibility with\nthe old model. In this work, we introduce FastFill: a compatible model update\nprocess using feature alignment and policy based partial backfilling to\npromptly elevate retrieval performance. We show that previous backfilling\nstrategies suffer from decreased performance and demonstrate the importance of\nboth the training objective and the ordering in online partial backfilling. We\npropose a new training method for feature alignment between old and new\nembedding models using uncertainty estimation. Compared to previous works, we\nobtain significantly improved backfilling results on a variety of datasets: mAP\non ImageNet (+4.4\\%), Places-365 (+2.7\\%), and VGG-Face2 (+1.3\\%). Further, we\ndemonstrate that when updating a biased model with FastFill, the minority\nsubgroup accuracy gap promptly vanishes with a small fraction of partial\nbackfilling.\n","authors":["Florian Jaeckle","Fartash Faghri","Ali Farhadi","Oncel Tuzel","Hadi Pouransari"],"pdf_url":"https://arxiv.org/pdf/2303.04766v1.pdf","comment":"To appear in The Eleventh International Conference on Learning\n  Representations"},{"id":"http://arxiv.org/abs/2207.08087v4","updated":"2023-03-08T15:06:11Z","published":"2022-07-17T06:50:35Z","title":"Automatic Context Pattern Generation for Entity Set Expansion","summary":"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.\n","authors":["Yinghui Li","Shulin Huang","Xinwei Zhang","Qingyu Zhou","Yangning Li","Ruiyang Liu","Yunbo Cao","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2207.08087v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2303.04587v1","updated":"2023-03-08T13:59:41Z","published":"2023-03-08T13:59:41Z","title":"A Prompt Log Analysis of Text-to-Image Generation Systems","summary":"  Recent developments in diffusion models have unleashed the astonishing\ncapabilities of text-to-image generation systems to synthesize high-quality\nimages that are faithful to a given reference text, known as a \"prompt.\" These\nsystems, once released to the public, have immediately received tons of\nattention from researchers, creators, and common users. Despite the plenty of\nefforts to improve the underneath generative models, there is limited work on\nunderstanding the information needs of the real users of these systems, e.g.,\nby investigating the prompts the users input at scale. In this paper, we take\nthe initiative to conduct a comprehensive analysis of large-scale prompt logs\ncollected from multiple text-to-image generation systems. Our work is analogous\nto analyzing the query log of Web search engines, a line of work that has made\ncritical contributions to the glory of the Web search industry and research. We\nanalyze over two million user-input prompts submitted to three popular\ntext-to-image systems at scale. Compared to Web search queries, text-to-image\nprompts are significantly longer, often organized into unique structures, and\npresent different categories of information needs. Users tend to make more\nedits within creation sessions, showing remarkable exploratory patterns. Our\nfindings provide concrete implications on how to improve text-to-image\ngeneration systems for creation purposes.\n","authors":["Yutong Xie","Zhaoying Pan","Jinge Ma","Jie Luo","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2303.04587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11879v4","updated":"2023-03-08T13:45:55Z","published":"2022-10-21T11:08:10Z","title":"GLCC: A General Framework for Graph-Level Clustering","summary":"  This paper studies the problem of graph-level clustering, which is a novel\nyet challenging task. This problem is critical in a variety of real-world\napplications such as protein clustering and genome analysis in bioinformatics.\nRecent years have witnessed the success of deep clustering coupled with graph\nneural networks (GNNs). However, existing methods focus on clustering among\nnodes given a single graph, while exploring clustering on multiple graphs is\nstill under-explored. In this paper, we propose a general graph-level\nclustering framework named Graph-Level Contrastive Clustering (GLCC) given\nmultiple graphs. Specifically, GLCC first constructs an adaptive affinity graph\nto explore instance- and cluster-level contrastive learning (CL).\nInstance-level CL leverages graph Laplacian based contrastive loss to learn\nclustering-friendly representations while cluster-level CL captures\ndiscriminative cluster representations incorporating neighbor information of\neach sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the\noptimization of representation learning. The two steps can be alternatively\ntrained to collaborate and benefit each other. Experiments on a range of\nwell-known datasets demonstrate the superiority of our proposed GLCC over\ncompetitive baselines.\n","authors":["Wei Ju","Yiyang Gu","Binqi Chen","Gongbo Sun","Yifang Qin","Xingyuming Liu","Xiao Luo","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.11879v4.pdf","comment":"Accepted by Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI 2023)"},{"id":"http://arxiv.org/abs/2303.04561v1","updated":"2023-03-08T13:20:53Z","published":"2023-03-08T13:20:53Z","title":"Kernel-CF: Collaborative filtering done right with social network\n  analysis and kernel smoothing","summary":"  Collaborative filtering is the simplest but oldest machine learning algorithm\nin the field of recommender systems. In spite of its long history, it remains a\ndiscussion topic in research venues. Usually people use users/items whose\nsimilarity scores with the target customer greater than 0 to compute the\nalgorithms. However, this might not be the optimal solution after careful\nscrutiny. In this paper, we transform the recommender system input data into a\n2-D social network, and apply kernel smoothing to compute preferences for\nunknown values in the user item rating matrix. We unifies the theoretical\nframework of recommender system and non-parametric statistics and provides an\nalgorithmic procedure with optimal parameter selection method to achieve the\ngoal.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.04561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04532v1","updated":"2023-03-08T12:04:16Z","published":"2023-03-08T12:04:16Z","title":"Class Cardinality Comparison as a Fermi Problem","summary":"  Questions on class cardinality comparisons are quite tricky to answer and\ncome with its own challenges. They require some kind of reasoning since web\ndocuments and knowledge bases, indispensable sources of information, rarely\nstore direct answers to questions, such as, ``Are there more astronauts or\nPhysics Nobel Laureates?'' We tackle questions on class cardinality comparison\nby tapping into three sources for absolute cardinalities as well as the\ncardinalities of orthogonal subgroups of the classes. We propose novel\ntechniques for aggregating signals with partial coverage for more reliable\nestimates and evaluate them on a dataset of 4005 class pairs, achieving an\naccuracy of 83.7%.\n","authors":["Shrestha Ghosh","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2303.04532v1.pdf","comment":"Accepted to the Web Conference 2023"},{"id":"http://arxiv.org/abs/2302.13053v2","updated":"2023-03-08T04:13:48Z","published":"2023-02-25T10:42:34Z","title":"RETEXO: Scalable Neural Network Training over Distributed Graphs","summary":"  Graph neural networks offer a promising approach to supervised learning over\ngraph data. Graph data, especially when it is privacy-sensitive or too large to\ntrain on centrally, is often stored partitioned across disparate processing\nunits (clients) which want to minimize the communication costs during\ncollaborative training. The fully-distributed setup takes such partitioning to\nits extreme, wherein features of only a single node and its adjacent edges are\nkept locally with one client processor. Existing GNNs are not architected for\ntraining in such setups and incur prohibitive costs therein. We propose RETEXO,\na novel transformation of existing GNNs that improves the communication\nefficiency during training in the fully-distributed setup. We experimentally\nconfirm that RETEXO offers up to 6 orders of magnitude better communication\nefficiency even when training shallow GNNs, with a minimal trade-off in\naccuracy for supervised node classification tasks.\n","authors":["Aashish Kolluri","Sarthak Choudhary","Bryan Hooi","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2302.13053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13795v2","updated":"2023-03-08T02:51:32Z","published":"2022-10-25T06:57:00Z","title":"Line Graph Contrastive Learning for Link Prediction","summary":"  Link prediction tasks focus on predicting possible future connections. Most\nexisting researches measure the likelihood of links by different similarity\nscores on node pairs and predict links between nodes. However, the\nsimilarity-based approaches have some challenges in information loss on nodes\nand generalization ability on similarity indexes. To address the above issues,\nwe propose a Line Graph Contrastive Learning(LGCL) method to obtain rich\ninformation with multiple perspectives. LGCL obtains a subgraph view by h-hop\nsubgraph sampling with target node pairs. After transforming the sampled\nsubgraph into a line graph, the link prediction task is converted into a node\nclassification task, which graph convolution progress can learn edge embeddings\nfrom graphs more effectively. Then we design a novel cross-scale contrastive\nlearning framework on the line graph and the subgraph to maximize the mutual\ninformation of them, so that fuses the structure and feature information. The\nexperimental results demonstrate that the proposed LGCL outperforms the\nstate-of-the-art methods and has better performance on generalization and\nrobustness.\n","authors":["Zehua Zhang","Shilin Sun","Guixiang Ma","Caiming Zhong"],"pdf_url":"https://arxiv.org/pdf/2210.13795v2.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2303.04335v1","updated":"2023-03-08T02:14:08Z","published":"2023-03-08T02:14:08Z","title":"Unbiased Learning to Rank with Biased Continuous Feedback","summary":"  It is a well-known challenge to learn an unbiased ranker with biased\nfeedback. Unbiased learning-to-rank(LTR) algorithms, which are verified to\nmodel the relative relevance accurately based on noisy feedback, are appealing\ncandidates and have already been applied in many applications with single\ncategorical labels, such as user click signals. Nevertheless, the existing\nunbiased LTR methods cannot properly handle continuous feedback, which are\nessential for many industrial applications, such as content recommender\nsystems.\n  To provide personalized high-quality recommendation results, recommender\nsystems need model both categorical and continuous biased feedback, such as\nclick and dwell time. Accordingly, we design a novel unbiased LTR algorithm to\ntackle the challenges, which innovatively models position bias in the pairwise\nfashion and introduces the pairwise trust bias to separate the position bias,\ntrust bias, and user relevance explicitly and can work for both continuous and\ncategorical feedback. Experiment results on public benchmark datasets and\ninternal live traffic of a large-scale recommender system at Tencent News show\nsuperior results for continuous labels and also competitive performance for\ncategorical labels of the proposed method.\n","authors":["Yi Ren","Hongyan Tang","Siwen Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.04335v1.pdf","comment":"10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2111.12929"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.04626v1","updated":"2023-03-08T14:45:51Z","published":"2023-03-08T14:45:51Z","title":"Multi-MEC Cooperation Based VR Video Transmission and Cache using\n  K-Shortest Paths Optimization","summary":"  In recent network architectures, multi-MEC cooperative caching has been\nintroduced to reduce the transmission latency of VR videos, in which MEC\nservers' computing and caching capability are utilized to optimize the\ntransmission process. However, many solutions that use the computing capability\nof MEC servers ignore the additional arithmetic power consumed by the codec\nprocess, thus making them infeasible. Besides, the minimum cache unit is\nusually the entire VR video, which makes caching inefficient.\n  To address these challenges, we split VR videos into tile files for caching\nbased on the current popular network architecture and provide a reliable\ntransmission mechanism and an effective caching strategy. Since the number of\ndifferent tile files N is too large, the current cooperative caching algorithms\ndo not cope with such large-scale input data. We further analyze the problem\nand propose an optimized k-shortest paths (OKSP) algorithm with an upper bound\ntime complexity of O((K * M + N) * M * logN)), and suitable for shortest paths\nwith restricted number of edges, where K is the total number of tiles that all\nM MEC servers can cache in the collaboration domain. And we prove the OKSP\nalgorithm can compute the caching scheme with the lowest average latency in any\ncase, which means the solution given is the exact solution. The simulation\nresults show that the OKSP algorithm has excellent speed for solving\nlarge-scale data and consistently outperforms other caching algorithms in the\nexperiments.\n","authors":["Jingwen Xia","Luyao Chen","Yong Tang","Ting Yang","Wenyong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.04626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08428v2","updated":"2023-03-08T14:25:10Z","published":"2022-11-15T05:14:48Z","title":"CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming","summary":"  Recent years have witnessed the dramatic growth of Internet video traffic,\nwhere the video bitstreams are often compressed and delivered in low quality to\nfit the streamer's uplink bandwidth. To alleviate the quality degradation, it\ncomes the rise of Neural-enhanced Video Streaming (NVS), which shows great\nprospects for recovering low-quality videos by mostly deploying neural\nsuper-resolution (SR) on the media server. Despite its benefit, we reveal that\ncurrent mainstream works with SR enhancement have not achieved the desired\nrate-distortion trade-off between bitrate saving and quality restoration, due\nto: (1) overemphasizing the enhancement on the decoder side while omitting the\nco-design of encoder, (2) limited generative capacity to recover high-fidelity\nperceptual details, and (3) optimizing the compression-and-restoration pipeline\nfrom the resolution perspective solely, without considering color bit-depth.\nAiming at overcoming these limitations, we are the first to conduct an\nencoder-decoder (i.e., codec) synergy by leveraging the inherent\nvisual-generative property of diffusion models. Specifically, we present the\nCodec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly\nreduce streaming delivery bitrates while holding pretty higher restoration\ncapacity over existing methods. First, CaDM improves the encoder's compression\nefficiency by simultaneously reducing resolution and color bit-depth of video\nframes. Second, CaDM empowers the decoder with high-quality enhancement by\nmaking the denoising diffusion restoration aware of encoder's resolution-color\nconditions. Evaluation on public cloud services with OpenMMLab benchmarks shows\nthat CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common\nvideo standards and achieves much better recovery quality (e.g., FID of 0.61)\nover state-of-the-art neural-enhancing methods.\n","authors":["Qihua Zhou","Ruibin Li","Song Guo","Peiran Dong","Yi Liu","Jingcai Guo","Zhenda Xu"],"pdf_url":"https://arxiv.org/pdf/2211.08428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03131v2","updated":"2023-03-08T11:35:51Z","published":"2023-03-06T13:49:15Z","title":"Video Question Answering Using CLIP-Guided Visual-Text Attention","summary":"  Cross-modal learning of video and text plays a key role in Video Question\nAnswering (VideoQA). In this paper, we propose a visual-text attention\nmechanism to utilize the Contrastive Language-Image Pre-training (CLIP) trained\non lots of general domain language-image pairs to guide the cross-modal\nlearning for VideoQA. Specifically, we first extract video features using a\nTimeSformer and text features using a BERT from the target application domain,\nand utilize CLIP to extract a pair of visual-text features from the\ngeneral-knowledge domain through the domain-specific learning. We then propose\na Cross-domain Learning to extract the attention information between visual and\nlinguistic features across the target domain and general domain. The set of\nCLIP-guided visual-text features are integrated to predict the answer. The\nproposed method is evaluated on MSVD-QA and MSRVTT-QA datasets, and outperforms\nstate-of-the-art methods.\n","authors":["Shuhong Ye","Weikai Kong","Chenglin Yao","Jianfeng Ren","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2303.03131v2.pdf","comment":"Submitted to the 2023 IEEE International Conference on Image\n  Processing (ICIP 2023)"}]},"2023-03-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.08841v2","updated":"2023-03-07T20:51:26Z","published":"2022-12-17T10:43:25Z","title":"AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation","summary":"  Dense retrievers have made significant strides in text retrieval and\nopen-domain question answering, even though most achievements were made\npossible only with large amounts of human supervision. In this work, we aim to\ndevelop unsupervised methods by proposing two methods that create pseudo\nquery-document pairs and train dense retrieval models in an annotation-free and\nscalable manner: query extraction and transferred query generation. The former\nmethod produces pseudo queries by selecting salient spans from the original\ndocument. The latter utilizes generation models trained for other NLP tasks\n(e.g., summarization) to produce pseudo queries. Extensive experiments show\nthat models trained with the proposed augmentation methods can perform\ncomparably well (or better) to multiple strong baselines. Combining those\nstrategies leads to further improvements, achieving the state-of-the-art\nperformance of unsupervised dense retrieval on both BEIR and ODQA datasets.\n","authors":["Rui Meng","Ye Liu","Semih Yavuz","Divyansh Agarwal","Lifu Tu","Ning Yu","Jianguo Zhang","Meghana Bhat","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.08841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04188v1","updated":"2023-03-07T19:23:33Z","published":"2023-03-07T19:23:33Z","title":"Clustering large 3D volumes: A sampling-based approach","summary":"  In many applications of X-ray computed tomography, an unsupervised\nsegmentation of the reconstructed 3D volumes forms an important step in the\nimage processing chain for further investigation of the digitized object.\nTherefore, the goal is to train a clustering algorithm on the volume, which\nproduces a voxelwise classification by assigning a cluster index to each voxel.\nHowever, clustering methods, e.g., K-Means, typically have an asymptotic\npolynomial runtime with respect to the dataset size, and thus, these techniques\nare rarely applicable to large volumes. In this work, we introduce a novel\nclustering technique based on random sampling, which allows for the voxelwise\nclassification of arbitrarily large volumes. The presented method conducts\nefficient linear passes over the data to extract a representative random sample\nof a fixed size on which the classifier can be trained. Then, a final linear\npass performs the segmentation and assigns a cluster index to each individual\nvoxel. Quantitative and qualitative evaluations show that excellent results can\nbe achieved even with a very small sample size. Consequently, the unsupervised\nsegmentation by means of clustering becomes feasible for arbitrarily large\nvolumes.\n","authors":["Thomas Lang"],"pdf_url":"https://arxiv.org/pdf/2303.04188v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.05392v1","updated":"2023-03-07T17:30:48Z","published":"2023-03-07T17:30:48Z","title":"Automatically Summarizing Evidence from Clinical Trials: A Prototype\n  Highlighting Current Challenges","summary":"  We present TrialsSummarizer, a system that aims to automatically summarize\nevidence presented in the set of randomized controlled trials most relevant to\na given query. Building on prior work, the system retrieves trial publications\nmatching a query specifying a combination of condition, intervention(s), and\noutcome(s), and ranks these according to sample size and estimated study\nquality. The top-k such studies are passed through a neural multi-document\nsummarization system, yielding a synopsis of these trials. We consider two\narchitectures: A standard sequence-to-sequence model based on BART and a\nmulti-headed architecture intended to provide greater transparency to\nend-users. Both models produce fluent and relevant summaries of evidence\nretrieved for queries, but their tendency to introduce unsupported statements\nrender them inappropriate for use in this domain at present. The proposed\narchitecture may help users verify outputs allowing users to trace generated\ntokens back to inputs.\n","authors":["Sanjana Ramprasad","Denis Jered McInerney","Iain J. Marshal","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2303.05392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04689v1","updated":"2023-03-07T17:22:38Z","published":"2023-03-07T17:22:38Z","title":"A Privacy Preserving System for Movie Recommendations using Federated\n  Learning","summary":"  Recommender systems have become ubiquitous in the past years. They solve the\ntyranny of choice problem faced by many users, and are employed by many online\nbusinesses to drive engagement and sales. Besides other criticisms, like\ncreating filter bubbles within social networks, recommender systems are often\nreproved for collecting considerable amounts of personal data. However, to\npersonalize recommendations, personal information is fundamentally required. A\nrecent distributed learning scheme called federated learning has made it\npossible to learn from personal user data without its central collection.\nAccordingly, we present a complete recommender system for movie\nrecommendations, which provides privacy and thus trustworthiness on two levels:\nFirst, it is trained using federated learning and thus is, by its very nature,\nprivacy-preserving, while still enabling individual users to benefit from\nglobal insights. And second, a novel federated learning scheme, FedQ, is\nemployed, which not only addresses the problem of non-i.i.d. and small local\ndatasets, but also prevents input data reconstruction attacks by aggregating\nclient models early. To reduce the communication overhead, compression is\napplied, which significantly reduces the exchanged neural network updates to a\nfraction of their original data. We conjecture that it may also improve data\nprivacy through its lossy quantization stage.\n","authors":["David Neumann","Andreas Lutz","Karsten Müller","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2303.04689v1.pdf","comment":"Submitted to the ACM TORS Special Issue on Trustworthy Recommender\n  Systems"},{"id":"http://arxiv.org/abs/2102.13392v3","updated":"2023-03-07T14:56:18Z","published":"2021-02-26T11:01:30Z","title":"Unifying Remote Sensing Image Retrieval and Classification with Robust\n  Fine-tuning","summary":"  Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.\n","authors":["Dimitri Gominski","Valérie Gouet-Brunet","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2102.13392v3.pdf","comment":"Performance margin with the proposed method is not statistically\n  significant. Please refer to http://alegoria.ign.fr/en/SF300_dataset if you\n  are interested in the dataset"},{"id":"http://arxiv.org/abs/2210.00305v2","updated":"2023-03-07T14:35:33Z","published":"2022-10-01T16:01:53Z","title":"LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph\n  Embeddings","summary":"  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph\nstructure and text-rich entity/relation information. Text-based KG embeddings\ncan represent entities by encoding descriptions with pre-trained language\nmodels, but no open-sourced library is specifically designed for KGs with PLMs\nat present. In this paper, we present LambdaKG, a library for KGE that equips\nwith many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and\nsupports various tasks (e.g., knowledge graph completion, question answering,\nrecommendation, and knowledge probing). LambdaKG is publicly open-sourced at\nhttps://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at\nhttp://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.\n","authors":["Xin Xie","Zhoubo Li","Xiaohan Wang","Yuqi Zhu","Ningyu Zhang","Jintian Zhang","Siyuan Cheng","Bozhong Tian","Shumin Deng","Feiyu Xiong","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2210.00305v2.pdf","comment":"Work in progress and the project website is\n  https://zjunlp.github.io/project/promptkg/"},{"id":"http://arxiv.org/abs/2202.12524v4","updated":"2023-03-07T03:21:01Z","published":"2022-02-25T06:58:28Z","title":"MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation","summary":"  Large-scale e-commercial platforms in the real-world usually contain various\nrecommendation scenarios (domains) to meet demands of diverse customer groups.\nMulti-Domain Recommendation (MDR), which aims to jointly improve\nrecommendations on all domains and easily scales to thousands of domains, has\nattracted increasing attention from practitioners and researchers. Existing MDR\nmethods usually employ a shared structure and several specific components to\nrespectively leverage reusable features and domain-specific information.\nHowever, data distribution differs across domains, making it challenging to\ndevelop a general model that can be applied to all circumstances. Additionally,\nduring training, shared parameters often suffer from the domain conflict while\nspecific parameters are inclined to overfitting on data sparsity domains. we\nfirst present a scalable MDR platform served in Taobao that enables to provide\nservices for thousands of domains without specialists involved. To address the\nproblems of MDR methods, we propose a novel model agnostic learning framework,\nnamely MAMDR, for the multi-domain recommendation. Specifically, we first\npropose a Domain Negotiation (DN) strategy to alleviate the conflict between\ndomains. Then, we develop a Domain Regularization (DR) to improve the\ngeneralizability of specific parameters by learning from other domains. We\nintegrate these components into a unified framework and present MAMDR, which\ncan be applied to any model structure to perform multi-domain recommendation.\nFinally, we present a large-scale implementation of MAMDR in the Taobao\napplication and construct various public MDR benchmark datasets which can be\nused for following studies. Extensive experiments on both benchmark datasets\nand industry datasets demonstrate the effectiveness and generalizability of\nMAMDR.\n","authors":["Linhao Luo","Yumeng Li","Buyu Gao","Shuai Tang","Sinan Wang","Jiancheng Li","Tanchao Zhu","Jiancai Liu","Zhao Li","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2202.12524v4.pdf","comment":"This paper has been accepted by ICDE 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2209.08212v4","updated":"2023-03-07T14:19:17Z","published":"2022-09-17T01:20:59Z","title":"Compose & Embellish: Well-Structured Piano Performance Generation via A\n  Two-Stage Approach","summary":"  Even with strong sequence models like Transformers, generating expressive\npiano performances with long-range musical structures remains challenging.\nMeanwhile, methods to compose well-structured melodies or lead sheets (melody +\nchords), i.e., simpler forms of music, gained more success. Observing the\nabove, we devise a two-stage Transformer-based framework that Composes a lead\nsheet first, and then Embellishes it with accompaniment and expressive touches.\nSuch a factorization also enables pretraining on non-piano data. Our objective\nand subjective experiments show that Compose & Embellish shrinks the gap in\nstructureness between a current state of the art and real performances by half,\nand improves other musical aspects such as richness and coherence as well.\n","authors":["Shih-Lun Wu","Yi-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2209.08212v4.pdf","comment":"Accepted to International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2023"},{"id":"http://arxiv.org/abs/2106.06924v3","updated":"2023-03-07T14:05:05Z","published":"2021-06-13T05:32:17Z","title":"Deep Learning for Predictive Analytics in Reversible Steganography","summary":"  Deep learning is regarded as a promising solution for reversible\nsteganography. There is an accelerating trend of representing a reversible\nsteo-system by monolithic neural networks, which bypass intermediate operations\nin traditional pipelines of reversible steganography. This end-to-end paradigm,\nhowever, suffers from imperfect reversibility. By contrast, the modular\nparadigm that incorporates neural networks into modules of traditional\npipelines can stably guarantee reversibility with mathematical explainability.\nPrediction-error modulation is a well-established reversible steganography\npipeline for digital images. It consists of a predictive analytics module and a\nreversible coding module. Given that reversibility is governed independently by\nthe coding module, we narrow our focus to the incorporation of neural networks\ninto the analytics module, which serves the purpose of predicting pixel\nintensities and a pivotal role in determining capacity and imperceptibility.\nThe objective of this study is to evaluate the impacts of different training\nconfigurations upon predictive accuracy of neural networks and provide\npractical insights. In particular, we investigate how different initialisation\nstrategies for input images may affect the learning process and how different\ntraining strategies for dual-layer prediction respond to the problem of\ndistributional shift. Furthermore, we compare steganographic performance of\nvarious model architectures with different loss functions.\n","authors":["Ching-Chun Chang","Xu Wang","Sisheng Chen","Isao Echizen","Victor Sanchez","Chang-Tsun Li"],"pdf_url":"https://arxiv.org/pdf/2106.06924v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.02478v2","updated":"2023-03-07T13:11:04Z","published":"2022-01-07T14:56:33Z","title":"Bayesian Neural Networks for Reversible Steganography","summary":"  Recent advances in deep learning have led to a paradigm shift in the field of\nreversible steganography. A fundamental pillar of reversible steganography is\npredictive modelling which can be realised via deep neural networks. However,\nnon-trivial errors exist in inferences about some out-of-distribution and noisy\ndata. In view of this issue, we propose to consider uncertainty in predictive\nmodels based upon a theoretical framework of Bayesian deep learning, thereby\ncreating an adaptive steganographic system. Most modern deep-learning models\nare regarded as deterministic because they only offer predictions while failing\nto provide uncertainty measurement. Bayesian neural networks bring a\nprobabilistic perspective to deep learning and can be regarded as self-aware\nintelligent machinery; that is, a machine that knows its own limitations. To\nquantify uncertainty, we apply Bayesian statistics to model the predictive\ndistribution and approximate it through Monte Carlo sampling with stochastic\nforward passes. We further show that predictive uncertainty can be disentangled\ninto aleatoric and epistemic uncertainties and these quantities can be learnt\nunsupervised. Experimental results demonstrate an improvement delivered by\nBayesian uncertainty analysis upon steganographic rate-distortion performance.\n","authors":["Ching-Chun Chang"],"pdf_url":"https://arxiv.org/pdf/2201.02478v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02518v2","updated":"2023-03-07T12:55:55Z","published":"2022-02-05T09:04:50Z","title":"On the predictability in reversible steganography","summary":"  Artificial neural networks have advanced the frontiers of reversible\nsteganography. The core strength of neural networks is the ability to render\naccurate predictions for a bewildering variety of data. Residual modulation is\nrecognised as the most advanced reversible steganographic algorithm for digital\nimages. The pivot of this algorithm is predictive analytics in which pixel\nintensities are predicted given some pixel-wise contextual information. This\ntask can be perceived as a low-level vision problem and hence neural networks\nfor addressing a similar class of problems can be deployed. On top of the prior\nart, this paper investigates predictability of pixel intensities based on\nsupervised and unsupervised learning frameworks. Predictability analysis\nenables adaptive data embedding, which in turn leads to a better trade-off\nbetween capacity and imperceptibility. While conventional methods estimate\npredictability by the statistics of local image patterns, learning-based\nframeworks consider further the degree to which correct predictions can be made\nby a designated predictor. Not only should the image patterns be taken into\naccount but also the predictor in use. Experimental results show that\nsteganographic performance can be significantly improved by incorporating the\nlearning-based predictability analysers into a reversible steganographic\nsystem.\n","authors":["Ching-Chun Chang","Xu Wang","Sisheng Chen","Hitoshi Kiya","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2202.02518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04894v3","updated":"2023-03-07T08:25:24Z","published":"2022-11-09T13:55:50Z","title":"Exploring Video Quality Assessment on User Generated Contents from\n  Aesthetic and Technical Perspectives","summary":"  The rapid increase in user-generated-content (UGC) videos calls for the\ndevelopment of effective video quality assessment (VQA) algorithms. However,\nthe objective of the UGC-VQA problem is still ambiguous and can be viewed from\ntwo perspectives: the technical perspective, measuring the perception of\ndistortions; and the aesthetic perspective, which relates to preference and\nrecommendation on contents. To understand how these two perspectives affect\noverall subjective opinions in UGC-VQA, we conduct a large-scale subjective\nstudy to collect human quality opinions on overall quality of videos as well as\nperceptions from aesthetic and technical perspectives. The collected\nDisentangled Video Quality Database (DIVIDE-3k) confirms that human quality\nopinions on UGC videos are universally and inevitably affected by both\naesthetic and technical perspectives. In light of this, we propose the\nDisentangled Objective Video Quality Evaluator (DOVER) to learn the quality of\nUGC videos based on the two perspectives. The DOVER proves state-of-the-art\nperformance in UGC-VQA under very high efficiency. With perspective opinions in\nDIVIDE-3k, we further propose DOVER++, the first approach to provide reliable\nclear-cut quality evaluations from a single aesthetic or technical perspective.\nCode at https://github.com/VQAssessment/DOVER.\n","authors":["Haoning Wu","Erli Zhang","Liang Liao","Chaofeng Chen","Jingwen Hou","Annan Wang","Wenxiu Sun","Qiong Yan","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2211.04894v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15603v3","updated":"2023-03-07T06:14:56Z","published":"2022-11-28T17:57:48Z","title":"Action-GPT: Leveraging Large-scale Language Models for Improved and\n  Generalized Action Generation","summary":"  We introduce Action-GPT, a plug-and-play framework for incorporating Large\nLanguage Models (LLMs) into text-based action generation models. Action phrases\nin current motion capture datasets contain minimal and to-the-point\ninformation. By carefully crafting prompts for LLMs, we generate richer and\nfine-grained descriptions of the action. We show that utilizing these detailed\ndescriptions instead of the original action phrases leads to better alignment\nof text and motion spaces. We introduce a generic approach compatible with\nstochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion\nmodels. In addition, the approach enables multiple text descriptions to be\nutilized. Our experiments show (i) noticeable qualitative and quantitative\nimprovement in the quality of synthesized motions, (ii) benefits of utilizing\nmultiple LLM-generated descriptions, (iii) suitability of the prompt function,\nand (iv) zero-shot generation capabilities of the proposed approach. Project\npage: https://actiongpt.github.io\n","authors":["Sai Shashank Kalakonda","Shubh Maheshwari","Ravi Kiran Sarvadevabhatla"],"pdf_url":"https://arxiv.org/pdf/2211.15603v3.pdf","comment":"Code, pretrained models and sample videos will be made available at\n  \\url{https://actiongpt.github.io}"},{"id":"http://arxiv.org/abs/2303.03105v2","updated":"2023-03-07T05:39:39Z","published":"2023-03-06T13:16:17Z","title":"Confidence-based Event-centric Online Video Question Answering on a\n  Newly Constructed ATBS Dataset","summary":"  Deep neural networks facilitate video question answering (VideoQA), but the\nreal-world applications on video streams such as CCTV and live cast place\nhigher demands on the solver. To address the challenges of VideoQA on long\nvideos of unknown length, we define a new set of problems called Online\nOpen-ended Video Question Answering (O^2VQA). It requires an online\nstate-updating mechanism for the solver to decide if the collected information\nis sufficient to conclude an answer. We then propose a Confidence-based\nEvent-centric Online Video Question Answering (CEO-VQA) model to solve this\nproblem. Furthermore, a dataset called Answer Target in Background Stream\n(ATBS) is constructed to evaluate this newly developed online VideoQA\napplication. Compared to the baseline VideoQA method that watches the whole\nvideo, the experimental results show that the proposed method achieves a\nsignificant performance gain.\n","authors":["Weikai Kong","Shuhong Ye","Chenglin Yao","Jianfeng Ren"],"pdf_url":"https://arxiv.org/pdf/2303.03105v2.pdf","comment":"Accepted for publication at the 2023 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2023)"},{"id":"http://arxiv.org/abs/2303.02673v2","updated":"2023-03-07T02:38:16Z","published":"2023-03-05T13:48:47Z","title":"Time-frequency Network for Robust Speaker Recognition","summary":"  The wide deployment of speech-based biometric systems usually demands\nhigh-performance speaker recognition algorithms. However, most of the prior\nworks for speaker recognition either process the speech in the frequency domain\nor time domain, which may produce suboptimal results because both time and\nfrequency domains are important for speaker recognition. In this paper, we\nattempt to analyze the speech signal in both time and frequency domains and\npropose the time-frequency network~(TFN) for speaker recognition by extracting\nand fusing the features in the two domains. Based on the recent advance of deep\nneural networks, we propose a convolution neural network to encode the raw\nspeech waveform and the frequency spectrum into domain-specific features, which\nare then fused and transformed into a classification feature space for speaker\nrecognition. Experimental results on the publicly available datasets TIMIT and\nLibriSpeech show that our framework is effective to combine the information in\nthe two domains and performs better than the state-of-the-art methods for\nspeaker recognition.\n","authors":["Jiguo Li","Tianzi Zhang","Xiaobin Liu","Lirong Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.02673v2.pdf","comment":"5pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.03599v1","updated":"2023-03-07T02:31:08Z","published":"2023-03-07T02:31:08Z","title":"FSVVD: A Dataset of Full Scene Volumetric Video","summary":"  Recent years have witnessed a rapid development of immersive multimedia which\nbridges the gap between the real world and virtual space. Volumetric videos, as\nan emerging representative 3D video paradigm that empowers extended reality,\nstand out to provide unprecedented immersive and interactive video watching\nexperience. Despite the tremendous potential, the research towards 3D\nvolumetric video is still in its infancy, relying on sufficient and complete\ndatasets for further exploration. However, existing related volumetric video\ndatasets mostly only include a single object, lacking details about the scene\nand the interaction between them. In this paper, we focus on the current most\nwidely used data format, point cloud, and for the first time release a\nfull-scene volumetric video dataset that includes multiple people and their\ndaily activities interacting with the external environments. Comprehensive\ndataset description and analysis are conducted, with potential usage of this\ndataset. The dataset and additional tools can be accessed via the following\nwebsite: https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/.\n","authors":["Kaiyuan Hu","Yili Jin","Haowen Yang","Junhua Liu","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.03599v1.pdf","comment":"Accepted by MMSys'23 Open Dataset and Software Track, A preliminary\n  version. The dataset and additional tools can be accessed via\n  https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/"},{"id":"http://arxiv.org/abs/2303.03591v1","updated":"2023-03-07T01:54:24Z","published":"2023-03-07T01:54:24Z","title":"Approach to Learning Generalized Audio Representation Through Batch\n  Embedding Covariance Regularization and Constant-Q Transforms","summary":"  General-purpose embedding is highly desirable for few-shot even zero-shot\nlearning in many application scenarios, including audio tasks. In order to\nunderstand representations better, we conducted a thorough error analysis and\nvisualization of HEAR 2021 submission results. Inspired by the analysis, this\nwork experiments with different front-end audio preprocessing methods,\nincluding Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),\nand proposes a Batch Embedding Covariance Regularization (BECR) term to uncover\na more holistic simulation of the frequency information received by the human\nauditory system. We tested the models on the suite of HEAR 2021 tasks, which\nencompass a broad category of tasks. Preliminary results show (1) the proposed\nBECR can incur a more dispersed embedding on the test set, (2) BECR improves\nthe PaSST model without extra computation complexity, and (3) STFT\npreprocessing outperforms CQT in all tasks we tested.\nGithub:https://github.com/ankitshah009/general_audio_embedding_hear_2021\n","authors":["Ankit Shah","Shuyi Chen","Kejun Zhou","Yue Chen","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2303.03591v1.pdf","comment":"Technical report, 10 pages"}]}}