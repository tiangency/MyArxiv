<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-14T00:00:00Z">2023-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CB2: Collaborative Natural Language Interaction Research Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Sharf, Mustafa Omer Gul, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CB2 is a multi-agent platform to study collaborative natural language
interaction in a grounded task-oriented scenario. It includes a 3D game
environment, a backend server designed to serve trained models to human agents,
and various tools and processes to enable scalable studies. We deploy CB2 at
https://cb2.ai as a system demonstration with a learned instruction following
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do <span class="highlight-title">Transformer</span>s Parse while Predicting the Masked Word? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhao, Abhishek Panigrahi, Rong Ge, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have been shown to encode linguistic structures,
e.g. dependency and constituency parse trees, in their embeddings while being
trained on unsupervised loss functions like masked language modeling. Some
doubts have been raised whether the models actually are doing parsing or only
some computation weakly correlated with it. We study questions: (a) Is it
possible to explicitly describe transformers with realistic embedding
dimension, number of heads, etc. that are capable of doing parsing -- or even
approximate parsing? (b) Why do pre-trained models capture parsing structure?
This paper takes a step toward answering these questions in the context of
generative modeling with PCFGs. We show that masked language models like BERT
or RoBERTa of moderate sizes can approximately execute the Inside-Outside
algorithm for the English PCFG [Marcus et al, 1993]. We also show that the
Inside-Outside algorithm is optimal for masked language modeling loss on the
PCFG-generated data. We also give a construction of transformers with $50$
layers, $15$ attention heads, and $1275$ dimensional embeddings in average such
that using its embeddings it is possible to do constituency parsing with
$>70\%$ F1 score on PTB dataset. We conduct probing experiments on models
pre-trained on PCFG-generated data to show that this not only allows recovery
of approximate parse tree, but also recovers marginal span probabilities
computed by the Inside-Outside algorithm, which suggests an implicit bias of
masked language modeling towards this algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simfluence: Modeling the Influence of Individual Training Examples by
  Simulating Training Runs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, Tolga Bolukbasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data attribution (TDA) methods offer to trace a model's prediction
on any given example back to specific influential training examples. Existing
approaches do so by assigning a scalar influence score to each training
example, under a simplifying assumption that influence is additive. But in
reality, we observe that training examples interact in highly non-additive ways
due to factors such as inter-example redundancy, training order, and curriculum
learning effects.
  To study such interactions, we propose Simfluence, a new paradigm for TDA
where the goal is not to produce a single influence score per example, but
instead a training run simulator: the user asks, ``If my model had trained on
example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on
$z_{test}$?''; the simulator should then output a simulated training run, which
is a time series predicting the loss on $z_{test}$ at every step of the
simulated run. This enables users to answer counterfactual questions about what
their model would have learned under different training curricula, and to
directly see where in training that learning would occur.
  We present a simulator, Simfluence-Linear, that captures non-additive
interactions and is often able to predict the spiky trajectory of individual
example losses with surprising fidelity. Furthermore, we show that existing TDA
methods such as TracIn and influence functions can be viewed as special cases
of Simfluence-Linear. This enables us to directly compare methods in terms of
their simulation accuracy, subsuming several prior TDA approaches to
evaluation. In experiments on large language model (LLM) fine-tuning, we show
that our method predicts loss trajectories with much higher accuracy than
existing TDA methods (doubling Spearman's correlation and reducing mean-squared
error by 75%) across several tasks, models, and training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Happy-GLL: modular, reusable and complete top-down parsers for
  parameterized nonterminals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        L. Thomas van Binsbergen, Damian Frolich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parser generators and parser combinator libraries are the most popular tools
for producing parsers. Parser combinators use the host language to provide
reusable components in the form of higher-order functions with parsers as
parameters. Very few parser generators support this kind of reuse through
abstraction and even fewer generate parsers that are as modular and reusable as
the parts of the grammar for which they are produced. This paper presents a
strategy for generating modular, reusable and complete top-down parsers from
syntax descriptions with parameterized nonterminals, based on the FUN-GLL
variant of the GLL algorithm.
  The strategy is discussed and demonstrated as a novel back-end for the Happy
parser generator. Happy grammars can contain `parameterized nonterminals' in
which parameters abstract over grammar symbols, granting an abstraction
mechanism to define reusable grammar operators. However, the existing Happy
back-ends do not deliver on the full potential of parameterized nonterminals as
parameterized nonterminals cannot be reused across grammars. Moreover, the
parser generation process may fail to terminate or may result in exponentially
large parsers generated in an exponential amount of time.
  The GLL back-end presented in this paper implements parameterized
nonterminals successfully by generating higher-order functions that resemble
parser combinators, inheriting all the advantages of top-down parsing. The
back-end is capable of generating parsers for the full class of context-free
grammars, generates parsers in linear time and generates parsers that find all
derivations of the input string. To our knowledge, the presented GLL back-end
makes Happy the first parser generator that combines all these features.
  This paper describes the translation procedure of the GLL back-end and
compares it to the LALR and GLR back-ends of Happy in several experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progress Note Understanding -- Assessment and Plan Reasoning: <span class="highlight-title">Overview</span>
  of the 2022 N2C2 Track 3 Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Gao, Dmitriy Dligach, Timothy Miller, Matthew M Churpek, Ozlem Uzuner, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Daily progress notes are common types in the electronic health record (EHR)
where healthcare providers document the patient's daily progress and treatment
plans. The EHR is designed to document all the care provided to patients, but
it also enables note bloat with extraneous information that distracts from the
diagnoses and treatment plans. Applications of natural language processing
(NLP) in the EHR is a growing field with the majority of methods in information
extraction. Few tasks use NLP methods for downstream diagnostic decision
support. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3:
Progress Note Understanding - Assessment and Plan Reasoning as one step towards
a new suite of tasks. The Assessment and Plan Reasoning task focuses on the
most critical components of progress notes, Assessment and Plan subsections
where health problems and diagnoses are contained. The goal of the task was to
develop and evaluate NLP systems that automatically predict causal relations
between the overall status of the patient contained in the Assessment section
and its relation to each component of the Plan section which contains the
diagnoses and treatment plans. The goal of the task was to identify and
prioritize diagnoses as the first steps in diagnostic decision support to find
the most relevant information in long documents like daily progress notes. We
present the results of 2022 n2c2 Track 3 and provide a description of the data,
evaluation, participation and system performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Journal of Biomedical Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BODEGA: Benchmark for Adversarial Example Generation in Credibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Przybyła, Alexander Shvets, Horacio Saggion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we introduce BODEGA: a benchmark for testing both victim models
and attack methods on four misinformation detection tasks in an evaluation
framework designed to simulate real use-cases of content moderation. We also
systematically test the robustness of popular text classifiers against
available attacking techniques and discover that, indeed, in some cases barely
significant changes in input text can mislead the models. We openly share the
BODEGA code and data in hope of enhancing the comparability and replicability
of further research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Deep Learning Model Parameters with the Bees Algorithm for
  Improved Medical Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mai A. Shaaban, Mariam Kashkash, Maryam Alghfeli, Adham Ibrahim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel mechanism to obtain the optimal parameters of a
deep learning model using the Bees Algorithm, which is a recent promising swarm
intelligence algorithm. The optimization problem is to maximize the accuracy of
classifying ailments based on medical text given the initial hyper-parameters
to be adjusted throughout a definite number of iterations. Experiments included
two different datasets: English and Arabic. The highest accuracy achieved is
99.63% on the English dataset using Long Short-Term Memory (LSTM) along with
the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of ChatGPT as a Question Answering System for Answering
  Complex Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is a powerful large language model (LLM) that has made remarkable
progress in natural language understanding. Nevertheless, the performance and
limitations of the model still need to be extensively evaluated. As ChatGPT
covers resources such as Wikipedia and supports natural language question
answering, it has garnered attention as a potential replacement for traditional
knowledge based question answering (KBQA) models. Complex question answering is
a challenge task of KBQA, which comprehensively tests the ability of models in
semantic parsing and reasoning. To assess the performance of ChatGPT as a
question answering system (QAS) using its own knowledge, we present a framework
that evaluates its ability to answer complex questions. Our approach involves
categorizing the potential features of complex questions and describing each
test question with multiple labels to identify combinatorial reasoning.
Following the black-box testing specifications of CheckList proposed by Ribeiro
et.al, we develop an evaluation method to measure the functionality and
reliability of ChatGPT in reasoning for answering complex questions. We use the
proposed framework to evaluate the performance of ChatGPT in question answering
on 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual
datasets, with a total of approximately 190,000 test cases. We compare the
evaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common
long-term problems in LLMs. The dataset and code are available at
https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the Needle in a Haystack: Unsupervised Rationale Extraction from
  Long Text Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Bujel, Andrew Caines, Helen Yannakoudakis, Marek Rei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-sequence transformers are designed to improve the representation of
longer texts by language models and their performance on downstream
document-level tasks. However, not much is understood about the quality of
token-level predictions in long-form models. We investigate the performance of
such architectures in the context of document classification with unsupervised
rationale extraction. We find standard soft attention methods to perform
significantly worse when combined with the Longformer language model. We
propose a compositional soft attention architecture that applies RoBERTa
sentence-wise to extract plausible rationales at the token-level. We find this
method to significantly outperform Longformer-driven baselines on sentiment
classification datasets, while also exhibiting significantly lower runtimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theory of Emergent In-Context Learning as Implicit Structure Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hahn, Navin Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling large language models (LLMs) leads to an emergent capacity to learn
in-context from example demonstrations. Despite progress, theoretical
understanding of this phenomenon remains limited. We argue that in-context
learning relies on recombination of compositional operations found in natural
language data. We derive an information-theoretic bound showing how in-context
learning abilities arise from generic next-token prediction when the
pretraining distribution has sufficient amounts of compositional structure,
under linguistically motivated assumptions. A second bound provides a
theoretical justification for the empirical success of prompting LLMs to output
intermediate steps towards an answer. To validate theoretical predictions, we
introduce a controlled setup for inducing in-context learning; unlike previous
approaches, it accounts for the compositional nature of language. Trained
transformers can perform in-context learning for a range of tasks, in a manner
consistent with the theoretical results. Mirroring real-world LLMs in a
miniature setup, in-context learning emerges when scaling parameters and data,
and models perform better when prompted to output intermediate steps. Probing
shows that in-context learning is supported by a representation of the input's
compositional structure. Taken together, these results provide a step towards
theoretical understanding of emergent behavior in large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Accented Speech Recognition with Multi-Domain Training <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Maison, Yannick Estève
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the rise of self-supervised learning, automatic speech recognition
(ASR) systems now achieve near-human performance on a wide variety of datasets.
However, they still lack generalization capability and are not robust to domain
shifts like accent variations. In this work, we use speech audio representing
four different French accents to create fine-tuning datasets that improve the
robustness of pre-trained ASR models. By incorporating various accents in the
training set, we obtain both in-domain and out-of-domain improvements. Our
numerical experiments show that we can reduce error rates by up to 25%
(relative) on African and Belgian accents compared to single-domain training
while keeping a good performance on standard French.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures. Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Offline Speech Translation Models for Streaming with
  Future-Aware Distillation and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Fu, Kai Fan, Minpeng Liao, Zhongqiang Huang, Boxing Chen, Yidong Chen, Xiaodong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A popular approach to streaming speech translation is to employ a single
offline model with a \textit{wait-$k$} policy to support different latency
requirements, which is simpler than training multiple online models with
different latency constraints. However, there is a mismatch problem in using a
model trained with complete utterances for streaming inference with partial
input. We demonstrate that speech representations extracted at the end of a
streaming input are significantly different from those extracted from a
complete utterance. To address this issue, we propose a new approach called
Future-Aware Streaming Translation (FAST) that adapts an offline ST model for
streaming input. FAST includes a Future-Aware Inference (FAI) strategy that
incorporates future context through a trainable masked embedding, and a
Future-Aware Distillation (FAD) framework that transfers future context from an
approximation of full speech to streaming input. Our experiments on the MuST-C
EnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs
between translation quality and latency than strong baselines. Extensive
analyses suggest that our methods effectively alleviate the aforementioned
mismatch problem between offline training and online inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Learnability of In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Wies, Yoav Levine, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning is a surprising and important phenomenon that emerged
when modern language models were scaled to billions of learned parameters.
Without modifying a large language model's weights, it can be tuned to perform
various downstream natural language tasks simply by including concatenated
training examples of these tasks in its input. Though disruptive for many
practical applications of large language models, this emergent learning
paradigm is not well understood from a theoretical perspective. In this paper,
we propose a first-of-its-kind PAC based framework for in-context learnability,
and use it to provide the first finite sample complexity results for the
in-context learning setup. Our framework includes an initial pretraining phase,
which fits a function to the pretraining distribution, and then a second
in-context learning phase, which keeps this function constant and concatenates
training examples of the downstream task in its input. We use our framework in
order to prove that, under mild assumptions, when the pretraining distribution
is a mixture of latent tasks (a model often considered for natural language
pretraining), these tasks can be efficiently learned via in-context learning,
even though the model's weights are unchanged and the input significantly
diverges from the pretraining distribution. Our theoretical analysis reveals
that in this setting, in-context learning is more about identifying the task
than about learning it, a result which is in line with a series of recent
empirical findings. We hope that the in-context learnability framework
presented in this paper will facilitate future progress towards a deeper
understanding of this important new learning paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geolocation Predicting of Tweets Using BERT-Based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kateryna Lutsai, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research is aimed to solve the tweet/user geolocation prediction task
and provide a flexible methodology for the geotagging of textual big data. The
suggested approach implements neural networks for natural language processing
(NLP) to estimate the location as coordinate pairs (longitude, latitude) and
two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models
has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder
Representations from Transformers (BERT) as base models. Performance metrics
show a median error of fewer than 30 km on a worldwide-level, and fewer than 15
km on the US-level datasets for the models trained and evaluated on text
features of tweets' content and metadata context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-turn dialogue generation, responses are not only related to the
topic and background of the context but also related to words and phrases in
the sentences of the context. However, currently widely used hierarchical
dialog models solely rely on context representations from the utterance-level
encoder, ignoring the sentence representations output by the word-level
encoder. This inevitably results in a loss of information while decoding and
generating. In this paper, we propose a new dialog model X-ReCoSa to tackle
this problem which aggregates multi-scale context information for hierarchical
dialog models. Specifically, we divide the generation decoder into upper and
lower parts, namely the intention part and the generation part. Firstly, the
intention part takes context representations as input to generate the intention
of the response. Then the generation part generates words depending on sentence
representations. Therefore, the hierarchical information has been fused into
response generation. we conduct experiments on the English dataset DailyDialog.
Experimental results exhibit that our method outperforms baseline models on
both automatic metric-based and human-based evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Cao, Yang Bai, Jingyao Wang, Ziqiang Cao, Liqiang Nie, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under the flourishing development in performance, current image-text
retrieval methods suffer from $N$-related time complexity, which hinders their
application in practice. Targeting at efficiency improvement, this paper
presents a simple and effective keyword-guided pre-screening framework for the
image-text retrieval. Specifically, we convert the image and text data into the
keywords and perform the keyword matching across modalities to exclude a large
number of irrelevant gallery samples prior to the retrieval network. For the
keyword prediction, we transfer it into a multi-label classification problem
and propose a multi-task learning scheme by appending the multi-label
classifiers to the image-text retrieval network to achieve a lightweight and
high-performance keyword prediction. For the keyword matching, we introduce the
inverted index in the search engine and create a win-win situation on both time
and space complexities for the pre-screening. Extensive experiments on two
widely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of
the proposed framework. The proposed framework equipped with only two embedding
layers achieves $O(1)$ querying time complexity, while improving the retrieval
efficiency and keeping its performance, when applied prior to the common
image-text retrieval methods. Our code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme
  Conversion <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungjun Kim, Changjin Han, Gyuhyeon Nam, Gyeongsu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework
that first transforms input sequences into character embeddings, obtains
linguistic information using language models, and then predicts the phonemes
based on global context about the entire input sequence. However, linguistic
knowledge alone is often inadequate. Language models frequently encode overly
general structures of a sentence and fail to cover specific cases needed to use
phonetic knowledge. Also, a handcrafted post-processing system is needed to
address the problems relevant to the tone of the characters. However, the
system exhibits inconsistency in the segmentation of word boundaries which
consequently degrades the performance of the G2P system. To address these
issues, we propose the Reinforcer that provides strong inductive bias for
language models by emphasizing the phonological information between neighboring
characters to help disambiguate pronunciations. Experimental results show that
the Reinforcer boosts the cutting-edge architectures by a large margin. We also
combine the Reinforcer with a large-scale pre-trained model and demonstrate the
validity of using neighboring context in knowledge transfer scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised
  Style Extractor and Hierarchical Modeling in Speech Synthesis <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Qiang, Peng Yang, Hao Che, Ying Zhang, Xiaorui Wang, Zhongyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesized speech of a target speaker's timbre. In most
previous methods, the synthesized fine-grained prosody features often represent
the source speaker's average style, similar to the one-to-many problem(i.e.,
multiple prosody variations correspond to the same text). In response to this
problem, a strength-controlled semi-supervised style extractor is proposed to
disentangle the style from content and timbre, improving the representation and
interpretability of the global style embedding, which can alleviate the
one-to-many mapping and data imbalance problems in prosody prediction. A
hierarchical prosody predictor is proposed to improve prosody modeling. We find
that better style transfer can be achieved by using the source speaker's
prosody features that are easily predicted. Additionally, a
speaker-transfer-wise cycle consistency loss is proposed to assist the model in
learning unseen style-timbre combinations during the training phase.
Experimental results show that the method outperforms the baseline. We provide
a website with audio samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Attention Model for Aspect-Level Sentiment Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfei Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I propose a novel dual-attention model(DAM) for aspect-level sentiment
classification. Many methods have been proposed, such as support vector
machines for artificial design features, long short-term memory networks based
on attention mechanisms, and graph neural networks based on dependency parsing.
While these methods all have decent performance, I think they all miss one
important piece of syntactic information: dependency labels. Based on this
idea, this paper proposes a model using dependency labels for the attention
mechanism to do this task. We evaluate the proposed approach on three datasets:
laptop and restaurant are from SemEval 2014, and the last one is a twitter
dataset. Experimental results show that the dual attention model has good
performance on all three datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xulong Zhang, Haobin Tang, Jianzong Wang, Ning Cheng, Jian Luo, Jing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Because of predicting all the target tokens in parallel, the
non-autoregressive models greatly improve the decoding efficiency of speech
recognition compared with traditional autoregressive models. In this work, we
present dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross
Entropy (AXE), finding the monotonic alignment that minimizes the cross-entropy
loss through dynamic programming, (2) Dynamic Rectification, creating new
training samples by replacing some masks with model predicted tokens. The AXE
ignores the absolute position alignment between prediction and ground truth
sentence and focuses on tokens matching in relative order. The dynamic
rectification method makes the model capable of simulating the non-mask but
possible wrong tokens, even if they have high confidence. Our experiments on
WSJ dataset demonstrated that not only AXE loss but also the rectification
method could improve the WER performance of Mask CTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobin Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent expressive text to speech (TTS) models focus on synthesizing emotional
speech, but some fine-grained styles such as intonation are neglected. In this
paper, we propose QI-TTS which aims to better transfer and control intonation
to further deliver the speaker's questioning intention while transferring
emotion from reference speech. We propose a multi-style extractor to extract
style embedding from two different levels. While the sentence level represents
emotion, the final syllable level represents intonation. For fine-grained
intonation control, we use relative attributes to represent intonation
intensity at the syllable level.Experiments have validated the effectiveness of
QI-TTS for improving intonation expressiveness in emotional speech synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query2doc: Query Expansion with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a simple yet effective query expansion approach,
denoted as query2doc, to improve both sparse and dense retrieval systems. The
proposed method first generates pseudo-documents by few-shot prompting large
language models (LLMs), and then expands the query with generated
pseudo-documents. LLMs are trained on web-scale text corpora and are adept at
knowledge memorization. The pseudo-documents from LLMs often contain highly
relevant information that can aid in query disambiguation and guide the
retrievers. Experimental results demonstrate that query2doc boosts the
performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and
TREC DL, without any model fine-tuning. Furthermore, our method also benefits
state-of-the-art dense retrievers in terms of both in-domain and out-of-domain
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RenewNAT: Renewing Potential Translation for Non-Autoregressive
  <span class="highlight-title">Transformer</span> <span class="chip">AAAI23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Guo, Yisheng Xiao, Juntao Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive neural machine translation (NAT) models are proposed to
accelerate the inference process while maintaining relatively high performance.
However, existing NAT models are difficult to achieve the desired
efficiency-quality trade-off. For one thing, fully NAT models with efficient
inference perform inferior to their autoregressive counterparts. For another,
iterative NAT models can, though, achieve comparable performance while
diminishing the advantage of speed. In this paper, we propose RenewNAT, a
flexible framework with high efficiency and effectiveness, to incorporate the
merits of fully and iterative NAT models. RenewNAT first generates the
potential translation results and then renews them in a single pass. It can
achieve significant performance improvements at the same expense as traditional
NAT models (without introducing additional model parameters and decoding
latency). Experimental results on various translation benchmarks (e.g.,
\textbf{4} WMT) show that our framework consistently improves the performance
of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed
overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-lingual Alzheimer's Disease detection based on paralinguistic and
  <span class="highlight-title">pre-train</span>ed features <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchu Chen, Yu Pu, Jinpeng Li, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task,
which aims to investigate which acoustic features can be generalized and
transferred across languages for Alzheimer's Disease (AD) prediction. The
challenge consists of two tasks: one is to classify the speech of AD patients
and healthy individuals, and the other is to infer Mini Mental State
Examination (MMSE) score based on speech only. The difficulty is mainly
embodied in the mismatch of the dataset, in which the training set is in
English while the test set is in Greek. We extract paralinguistic features
using openSmile toolkit and acoustic features using XLSR-53. In addition, we
extract linguistic features after transcribing the speech into text. These
features are used as indicators for AD detection in our method. Our method
achieves an accuracy of 69.6% on the classification task and a root mean
squared error (RMSE) of 4.788 on the regression task. The results show that our
proposed method is expected to achieve automatic multilingual Alzheimer's
Disease detection through spontaneous speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I3D: <span class="highlight-title">Transformer</span> architectures with input-dependent dynamic depth for
  speech recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Jaesong Lee, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based end-to-end speech recognition has achieved great success.
However, the large footprint and computational overhead make it difficult to
deploy these models in some real-world applications. Model compression
techniques can reduce the model size and speed up inference, but the compressed
model has a fixed architecture which might be suboptimal. We propose a novel
Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong
performance-efficiency trade-offs. With a similar number of layers at inference
time, I3D-based models outperform the vanilla Transformer and the static pruned
model via iterative layer pruning. We also present interesting analysis on the
gate probabilities and the input-dependency, which helps us better understand
deep encoders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Life Cycle of Knowledge in Big Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge plays a critical role in artificial intelligence. Recently, the
extensive success of pre-trained language models (PLMs) has raised significant
attention about how knowledge can be acquired, maintained, updated and used by
language models. Despite the enormous amount of related studies, there still
lacks a unified view of how knowledge circulates within language models
throughout the learning, tuning, and application processes, which may prevent
us from further understanding the connections between current progress or
realizing existing limitations. In this survey, we revisit PLMs as
knowledge-based systems by dividing the life circle of knowledge in PLMs into
five critical periods, and investigating how knowledge circulates when it is
built, maintained and used. To this end, we systematically review existing
studies of each period of the knowledge life cycle, summarize the main
challenges and current limitations, and discuss future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paperlist: https://github.com/c-box/KnowledgeLifecycle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on
  Consistency with Human Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, Xiangang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a natural language assistant, ChatGPT is capable of performing various
tasks, including but not limited to article generation, code completion, and
data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable
level of accuracy and reliability in terms of content evaluation, exhibiting
the capability of mimicking human preferences. To further explore ChatGPT's
potential in this regard, a study is conducted to assess its ability to rank
content. In order to do so, a test set consisting of prompts is created,
covering a wide range of use cases, and five models are utilized to generate
corresponding responses. ChatGPT is then instructed to rank the responses
generated by these models. The results on the test set show that ChatGPT's
ranking preferences are consistent with human to a certain extent. This
preliminary experimental finding implies that ChatGPT's zero-shot ranking
capability could be used to reduce annotation pressure in a number of ranking
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Input-length-shortening and text generation via attention values <span class="chip">AAAI23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neşet Özkan Tan, Alex Yuxuan Peng, Joshua Bensemann, Qiming Bao, Tim Hartill, Mark Gahegan, Michael Witbrock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying words that impact a task's performance more than others is a
challenge in natural language processing. Transformers models have recently
addressed this issue by incorporating an attention mechanism that assigns
greater attention (i.e., relevance) scores to some words than others. Because
of the attention mechanism's high computational cost, transformer models
usually have an input-length limitation caused by hardware constraints. This
limitation applies to many transformers, including the well-known bidirectional
encoder representations of the transformer (BERT) model. In this paper, we
examined BERT's attention assignment mechanism, focusing on two questions: (1)
How can attention be employed to reduce input length? (2) How can attention be
used as a control mechanism for conditional text generation? We investigated
these questions in the context of a text classification task. We discovered
that BERT's early layers assign more critical attention scores for text
classification tasks compared to later layers. We demonstrated that the first
layer's attention sums could be used to filter tokens in a given sequence,
considerably decreasing the input length while maintaining good test accuracy.
We also applied filtering, which uses a compute-efficient semantic similarities
algorithm, and discovered that retaining approximately 6\% of the original
sequence is sufficient to obtain 86.5\% accuracy. Finally, we showed that we
could generate data in a stable manner and indistinguishable from the original
one by only using a small percentage (10\%) of the tokens with high attention
scores according to BERT's first layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures. AAAI23-EMC2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Models in NLP: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuansong Zhu, Yu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have become a powerful family of deep generative models,
with record-breaking performance in many applications. This paper first gives
an overview and derivation of the basic theory of diffusion models, then
reviews the research results of diffusion models in the field of natural
language processing, from text generation, text-driven image generation and
other four aspects, and analyzes and summarizes the relevant literature
materials sorted out, and finally records the experience and feelings of this
topic literature review research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Models Trained on Indian Legal Data Fair? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Girhepuje, Anmol Goel, Gokul S Krishnan, Shreya Goyal, Satyendra Pandey, Ponnurangam Kumaraguru, Balaraman Ravindran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances and applications of language technology and artificial
intelligence have enabled much success across multiple domains like law,
medical and mental health. AI-based Language Models, like Judgement Prediction,
have recently been proposed for the legal sector. However, these models are
strife with encoded social biases picked up from the training data. While bias
and fairness have been studied across NLP, most studies primarily locate
themselves within a Western context. In this work, we present an initial
investigation of fairness from the Indian perspective in the legal domain. We
highlight the propagation of learnt algorithmic biases in the bail prediction
task for models trained on Hindi legal documents. We evaluate the fairness gap
using demographic parity and show that a decision tree model trained for the
bail prediction task has an overall fairness disparity of 0.237 between input
features associated with Hindus and Muslims. Additionally, we highlight the
need for further research and studies in the avenues of fairness/bias in
applying AI in the legal sector with a specific focus on the Indian context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Symposium on AI and Law (SAIL) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ordinal analysis of lexical patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Sanchez, Luciano Zunino, Juan De Gregorio, Raul Toral, Claudio Mirasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Words are fundamental linguistic units that connect thoughts and things
through meaning. However, words do not appear independently in a text sequence.
The existence of syntactic rules induces correlations among neighboring words.
Using an ordinal pattern approach, we present an analysis of lexical
statistical connections for 11 major languages. We find that the diverse
manners that languages utilize to express word relations give rise to unique
pattern structural distributions. Furthermore, fluctuations of these pattern
distributions for a given language can allow us to determine both the
historical period when the text was written and its author. Taken together, our
results emphasize the relevance of ordinal time series analysis in linguistic
typology, historical linguistics and stylometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 12 figures, 2 tables; v2: the section on universality has
  been removed because previous results were affected by spurious correlations.
  Published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models in the Workplace: A Case Study on <span class="highlight-title">Prompt</span>
  Engineering for Job Type Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clavié, Alexandru Ciceu, Frederick Naylor, Guillaume Soulié, Thomas Brightwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This case study investigates the task of job classification in a real-world
setting, where the goal is to determine whether an English-language job posting
is appropriate for a graduate or entry-level position. We explore multiple
approaches to text classification, including supervised approaches such as
traditional models like Support Vector Machines (SVMs) and state-of-the-art
deep learning methods such as DeBERTa. We compare them with Large Language
Models (LLMs) used in both few-shot and zero-shot classification settings. To
accomplish this task, we employ prompt engineering, a technique that involves
designing prompts to guide the LLMs towards the desired output. Specifically,
we evaluate the performance of two commercially available state-of-the-art
GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also
conduct a detailed analysis of the impact of different aspects of prompt
engineering on the model's performance. Our results show that, with a
well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all
other models, achieving a 6% increase in Precision@95% Recall compared to the
best supervised approach. Furthermore, we observe that the wording of the
prompt is a critical factor in eliciting the appropriate "reasoning" in the
model, and that seemingly minor aspects of the prompt significantly affect the
model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuS-X: Training-Free Name-Only Transfer of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishaal Udandarao, Ankush Gupta, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free "name-only transfer" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural inhibition during speech planning contributes to contrastive
  hyperarticulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael C. Stern, Jason A. Shaw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work has demonstrated that words are hyperarticulated on dimensions
of speech that differentiate them from a minimal pair competitor. This
phenomenon has been termed contrastive hyperarticulation (CH). We present a
dynamic neural field (DNF) model of voice onset time (VOT) planning that
derives CH from an inhibitory influence of the minimal pair competitor during
planning. We test some predictions of the model with a novel experiment
investigating CH of voiceless stop consonant VOT in pseudowords. The results
demonstrate a CH effect in pseudowords, consistent with a basis for the effect
in the real-time planning and production of speech. The scope and magnitude of
CH in pseudowords was reduced compared to CH in real words, consistent with a
role for interactive activation between lexical and phonological levels of
planning. We discuss the potential of our model to unify an apparently
disparate set of phenomena, from CH to phonological neighborhood effects to
phonetic trace effects in speech errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human heuristics for AI-generated language are flawed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07271v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07271v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maurice Jakesch, Jeffrey Hancock, Mor Naaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems suggest words, complete
sentences, or produce entire conversations. AI-generated language is often not
identified as such but presented as language written by humans, raising
concerns about novel forms of deception and manipulation. Here, we study how
humans discern whether verbal self-presentations, one of the most personal and
consequential forms of language, were generated by AI. In six experiments,
participants (N = 4,600) were unable to detect self-presentations generated by
state-of-the-art AI language models in professional, hospitality, and dating
contexts. A computational analysis of language features shows that human
judgments of AI-generated language are hindered by intuitive but flawed
heuristics such as associating first-person pronouns, use of contractions, or
family topics with human-written language. We experimentally demonstrate that
these heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce text perceived as "more human than
human." We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Suffix Retrieval-Augmented Language Modeling <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zecheng Wang, Yik-Cheung Tam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal language modeling (LM) uses word history to predict the next word.
BERT, on the other hand, makes use of bi-directional word information in a
sentence to predict words at masked positions. While BERT is effective in
sequence encoding, it is non-causal by nature and is not designed for sequence
generation. In this paper, we propose a novel language model, SUffix
REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual
effect in an autoregressive manner. SUREALM employs an embedding retriever to
search for training sentences in a data store that share similar word history
during sequence generation. In particular, the suffix portions of the retrieved
sentences mimick the "future" context. We evaluated our proposed model on the
DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on
the validation and test set compared to competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure. Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DECAR: Deep Clustering for learning general-purpose Audio
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08895v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08895v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Sandesh V Katta, Ashish Seth, S. Umesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DECAR, a self-supervised pre-training approach for learning
general-purpose audio representations. Our system is based on clustering: it
utilizes an offline clustering step to provide target labels that act as
pseudo-labels for solving a prediction task. We develop on top of recent
advances in self-supervised learning for computer vision and design a
lightweight, easy-to-use self-supervised pre-training scheme. We pre-train
DECAR embeddings on a balanced subset of the large-scale Audioset dataset and
transfer those representations to 9 downstream classification tasks, including
speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct
ablation studies identifying key design choices and also make all our code and
pre-trained models publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Fail on Trivial Alterations to Theory-of-Mind
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08399v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08399v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Ullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intuitive psychology is a pillar of common-sense reasoning. The replication
of this reasoning in machine intelligence is an important stepping-stone on the
way to human-like artificial intelligence. Several recent tasks and benchmarks
for examining this reasoning in Large-Large Models have focused in particular
on belief attribution in Theory-of-Mind tasks. These tasks have shown both
successes and failures. We consider in particular a recent purported success
case, and show that small variations that maintain the principles of ToM turn
the results on their head. We argue that in general, the zero-hypothesis for
model evaluation in intuitive psychology should be skeptical, and that outlying
failure cases should outweigh average success rates. We also consider what
possible future successes on Theory-of-Mind tasks by more powerful LLMs would
mean for ToM tasks with people.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TriNet: stabilizing <span class="highlight-title">self-supervised</span> learning from complete or slow
  collapse on ASR <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixin Cao, Jun Wang, Ben Yang, Dan Su, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) models confront challenges of abrupt
informational collapse or slow dimensional collapse. We propose TriNet, which
introduces a novel triple-branch architecture for preventing collapse and
stabilizing the pre-training. TriNet learns the SSL latent embedding space and
incorporates it to a higher level space for predicting pseudo target vectors
generated by a frozen teacher. Our experimental results show that the proposed
method notably stabilizes and accelerates pre-training and achieves a relative
word error rate reduction (WERR) of 6.06% compared to the state-of-the-art
(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code
at https://github.com/tencent-ailab/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph
  Question Answering <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Liu, Di Liang, Fang Fang, Sirui Wang, Wei Wu, Rui Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) have received increasing attention due to its wide
applications on natural language processing. However, its use case on temporal
question answering (QA) has not been well-explored. Most of existing methods
are developed based on pre-trained language models, which might not be capable
to learn \emph{temporal-specific} presentations of entities in terms of
temporal KGQA task. To alleviate this problem, we propose a novel
\textbf{T}ime-aware \textbf{M}ultiway \textbf{A}daptive (\textbf{TMA}) fusion
network. Inspired by the step-by-step reasoning behavior of humans. For each
given question, TMA first extracts the relevant concepts from the KG, and then
feeds them into a multiway adaptive module to produce a
\emph{temporal-specific} representation of the question. This representation
can be incorporated with the pre-trained KG embedding to generate the final
prediction. Empirical results verify that the proposed model achieves better
performance than the state-of-the-art models in the benchmark dataset. Notably,
the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex
questions are absolutely improved by 24\% and 10\% compared to the
best-performing baseline. Furthermore, we also show that TMA employing an
adaptive fusion mechanism can provide interpretability by analyzing the
proportion of information in question representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Speech Translation with Dynamic Latent Perceivers <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Tsiamas, Gerard I. Gállego, José A. R. Fonollosa, Marta R. Costa-jussà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been the dominant architecture for Speech Translation in
recent years, achieving significant improvements in translation quality. Since
speech signals are longer than their textual counterparts, and due to the
quadratic complexity of the Transformer, a down-sampling step is essential for
its adoption in Speech Translation. Instead, in this research, we propose to
ease the complexity by using a Perceiver encoder to map the speech inputs to a
fixed-length latent representation. Furthermore, we introduce a novel way of
training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent
spaces without any additional computational overhead. Speech-to-Text Perceivers
with DLA can match the performance of Transformer baselines across three
language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to
DLA at inference, and can be flexibly deployed with various computational
budgets, without significant drops in translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Concept Knowledge Graph for User Next Intent Prediction at Alipay <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00503v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00503v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacheng He, Qianghuai Jia, Lin Yuan, Ruopeng Li, Yixin Ou, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper illustrates the technologies of user next intent prediction with a
concept knowledge graph. The system has been deployed on the Web at Alipay,
serving more than 100 million daily active users. To explicitly characterize
user intent, we propose AlipayKG, which is an offline concept knowledge graph
in the Life-Service domain modeling the historical behaviors of users, the rich
content interacted by users and the relations between them. We further
introduce a Transformer-based model which integrates expert rules from the
knowledge graph to infer the online user's next intent. Experimental results
demonstrate that the proposed system can effectively enhance the performance of
the downstream tasks while retaining explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Xue, Di Liang, Sirui Wang, Wei Wu, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based pre-trained models have achieved great improvements in
semantic matching. However, existing models still suffer from insufficient
ability to capture subtle differences. The modification, addition and deletion
of words in sentence pairs may make it difficult for the model to predict their
relationship. To alleviate this problem, we propose a novel Dual Path Modeling
Framework to enhance the model's ability to perceive subtle differences in
sentence pairs by separately modeling affinity and difference semantics. Based
on dual-path modeling framework we design the Dual Path Modeling Network
(DPM-Net) to recognize semantic relations. And we conduct extensive experiments
on 10 well-studied semantic matching and robustness test datasets, and the
experimental results show that our proposed method achieves consistent
improvements over baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Discrimination to Generation: Knowledge Graph Completion with
  Generative <span class="highlight-title">Transformer</span> <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02113v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02113v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui Chen, Feiyu Xiong, Mosha Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2022 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ed Language Models are Symbolic Mathematics Solvers too! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimia Noorbakhsh, Modar Sulaiman, Mahdi Sharifi, Kallol Roy, Pooyan Jamshidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving symbolic mathematics has always been of in the arena of human
ingenuity that needs compositional reasoning and recurrence. However, recent
studies have shown that large-scale language models such as transformers are
universal and surprisingly can be trained as a sequence-to-sequence task to
solve complex mathematical equations. These large transformer models need
humongous amounts of training data to generalize to unseen symbolic mathematics
problems. In this paper, we present a sample efficient way of solving the
symbolic tasks by first pretraining the transformer model with language
translation and then fine-tuning the pretrained transformer model to solve the
downstream task of symbolic mathematics. We achieve comparable accuracy on the
integration task with our pretrained model while using around $1.5$ orders of
magnitude less number of training samples with respect to the state-of-the-art
deep learning for symbolic mathematics. The test accuracy on differential
equation tasks is considerably lower comparing with integration as they need
higher order recursions that are not present in language translations. We
propose the generalizability of our pretrained language model from Anna
Karenina Principle (AKP). We pretrain our model with different pairs of
language translations. Our results show language bias in solving symbolic
mathematics tasks. Finally, we study the robustness of the fine-tuned model on
symbolic math tasks against distribution shift, and our approach generalizes
better in distribution shift scenarios for the function integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relphormer: Relational Graph <span class="highlight-title">Transformer</span> for Knowledge Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10852v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10852v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Ningyu Zhang, Qiang Chen, Feiyu Xiong, Wei Guo, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thought Flow Nets: From Single Predictions to Trains of Model Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.12220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.12220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Schuff, Heike Adel, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When humans solve complex problems, they typically create a sequence of ideas
(involving an intuitive decision, reflection, error correction, etc.) in order
to reach a conclusive decision. Contrary to this, today's models are mostly
trained to map an input to one single and fixed output. In this paper, we
investigate how we can give models the opportunity of a second, third and
$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the
concept of a thought flow which creates a sequence of predictions. We present a
self-correction mechanism that is trained to estimate the model's correctness
and performs iterative prediction updates based on the correctness prediction's
gradient. We introduce our method at the example of question answering and
conduct extensive experiments that demonstrate (i) our method's ability to
correct its own predictions and (ii) its potential to notably improve model
performances. In addition, we conduct a qualitative analysis of thought flow
correction patterns and explore how thought flow predictions affect human users
within a crowdsourcing study. We find that (iii) thought flows enable improved
user performance and are perceived as more natural, correct, and intelligent as
single and/or top-3 predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied
  Agents under Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated embodied agent learning protects the data privacy of individual
visual environments by keeping data locally at each client (the individual
environment) during training. However, since the local data is inaccessible to
the server under federated learning, attackers may easily poison the training
data of the local client to build a backdoor in the agent without notice.
Deploying such an agent raises the risk of potential harm to humans, as the
attackers may easily navigate and control the agent as they wish via the
backdoor. Towards Byzantine-robust federated embodied agent learning, in this
paper, we study the attack and defense for the task of vision-and-language
navigation (VLN), where the agent is required to follow natural language
instructions to navigate indoor environments. First, we introduce a simple but
effective attack strategy, Navigation as Wish (NAW), in which the malicious
client manipulates local trajectory data to implant a backdoor into the global
model. Results on two VLN datasets (R2R and RxR) show that NAW can easily
navigate the deployed VLN agent regardless of the language instruction, without
affecting its performance on normal test sets. Then, we propose a new
Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated
VLN, which provides the server with a ''prompt'' of the vision-and-language
alignment variance between the benign and malicious clients so that they can be
distinguished during training. We validate the effectiveness of the PBA method
on protecting the global model from the NAW attack, which outperforms other
state-of-the-art defense methods by a large margin in the defense metrics on
R2R and RxR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving CTC-based ASR Models with Gated Interlayer Collaboration <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Yang, Yuke Li, Binbin Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CTC-based automatic speech recognition (ASR) models without the external
language model usually lack the capacity to model conditional dependencies and
textual interactions. In this paper, we present a Gated Interlayer
Collaboration (GIC) mechanism to improve the performance of CTC-based models,
which introduces textual information into the model and thus relaxes the
conditional independence assumption of CTC-based models. Specifically, we
consider the weighted sum of token embeddings as the textual representation for
each position, where the position-specific weights are the softmax probability
distribution constructed via inter-layer auxiliary CTC losses. The textual
representations are then fused with acoustic features by developing a gate
unit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the
proposed method outperforms several strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Relation Discovery: Towards General and Label-aware Open Relation
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Yinghui Li, Xi Chen, Hai-Tao Zheng, Ying Shen, Hong-Gee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Relation Extraction (OpenRE) aims to discover novel relations from open
domains. Previous OpenRE methods mainly suffer from two problems: (1)
Insufficient capacity to discriminate between known and novel relations. When
extending conventional test settings to a more general setting where test data
might also come from seen classes, existing approaches have a significant
performance decline. (2) Secondary labeling must be performed before practical
application. Existing methods cannot label human-readable and meaningful types
for novel relations, which is urgently required by the downstream tasks. To
address these issues, we propose the Active Relation Discovery (ARD) framework,
which utilizes relational outlier detection for discriminating known and novel
relations and involves active learning for labeling novel relations. Extensive
experiments on three real-world datasets show that ARD significantly
outperforms previous state-of-the-art methods on both conventional and our
proposed general OpenRE settings. The source code and datasets will be
available for reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Understand Representation of Layer-aware Sequence Encoders as
  Multi-order-graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.06397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.06397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sufeng Duan, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an explanation of representation for self-attention
network (SAN) based neural sequence encoders, which regards the information
captured by the model and the encoding of the model as graph structure and the
generation of these graph structures respectively. The proposed explanation
applies to existing works on SAN-based models and can explain the relationship
among the ability to capture the structural or linguistic information, depth of
model, and length of sentence, and can also be extended to other models such as
recurrent neural network based models. We also propose a revisited multigraph
called Multi-order-Graph (MoG) based on our explanation to model the graph
structures in the SAN-based model as subgraphs in MoG and convert the encoding
of SAN-based model to the generation of MoG. Based on our explanation, we
further introduce a Graph-Transformer by enhancing the ability to capture
multiple subgraphs of different orders and focusing on subgraphs of high
orders. Experimental results on multiple neural machine translation tasks show
that the Graph-Transformer can yield effective performance improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2009.07489</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Augmented Network Towards Multiview Representation
  Learning for Aspect-based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Hua Jin, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment
analysis. To better comprehend long complicated sentences and obtain accurate
aspect-specific information, linguistic and commonsense knowledge are generally
required in this task. However, most current methods employ complicated and
inefficient approaches to incorporate external knowledge, e.g., directly
searching the graph nodes. Additionally, the complementarity between external
knowledge and linguistic information has not been thoroughly studied. To this
end, we propose a knowledge graph augmented network KGAN, which aims to
effectively incorporate external knowledge with explicitly syntactic and
contextual information. In particular, KGAN captures the sentiment feature
representations from multiple different perspectives, i.e., context-, syntax-
and knowledge-based. First, KGAN learns the contextual and syntactic
representations in parallel to fully extract the semantic features. Then, KGAN
integrates the knowledge graphs into the embedding space, based on which the
aspect-specific knowledge representations are further obtained via an attention
mechanism. Last, we propose a hierarchical fusion module to complement these
multi-view representations in a local-to-global manner. Extensive experiments
on five popular ABSA benchmarks demonstrate the effectiveness and robustness of
our KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN
achieves a new record of state-of-the-art performance among all datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-Aware Meta Visual <span class="highlight-title">Prompt</span>ing <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Diversity-Aware Meta Visual Prompting~(DAM-VP), an efficient and
effective prompting method for transferring pre-trained models to downstream
tasks with frozen backbone. A challenging issue in visual prompting is that
image datasets sometimes have a large data diversity whereas a per-dataset
generic prompt can hardly handle the complex distribution shift toward the
original pretraining data distribution properly. To address this issue, we
propose a dataset Diversity-Aware prompting strategy whose initialization is
realized by a Meta-prompt. Specifically, we cluster the downstream dataset into
small homogeneity subsets in a diversity-adaptive way, with each subset has its
own prompt optimized separately. Such a divide-and-conquer design reduces the
optimization difficulty greatly and significantly boosts the prompting
performance. Furthermore, all the prompts are initialized with a meta-prompt,
which is learned across several datasets. It is a bootstrapped paradigm, with
the key observation that the prompting knowledge learned from previous datasets
could help the prompt to converge faster and perform better on a new dataset.
During inference, we dynamically select a proper prompt for each input, based
on the feature distance between the input and each subset. Through extensive
experiments, our DAM-VP demonstrates superior efficiency and effectiveness,
clearly surpassing previous prompting methods in a series of downstream
datasets for different pretraining models. Our code is available at:
\url{https://github.com/shikiw/DAM-VP}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023, code is available at https://github.com/shikiw/DAM-VP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayoutDM: Discrete <span class="highlight-title">Diffusion</span> Model for Controllable Layout Generation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable layout generation aims at synthesizing plausible arrangement of
element bounding boxes with optional constraints, such as type or position of a
specific element. In this work, we try to solve a broad range of layout
generation tasks in a single model that is based on discrete state-space
diffusion models. Our model, named LayoutDM, naturally handles the structured
layout data in the discrete representation and learns to progressively infer a
noiseless layout from the initial input, where we model the layout corruption
process by modality-wise discrete diffusion. For conditional generation, we
propose to inject layout constraints in the form of masking or logit adjustment
during inference. We show in the experiments that our LayoutDM successfully
generates high-quality layouts and outperforms both task-specific and
task-agnostic baselines on several layout tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in CVPR2023, project page:
  https://cyberagentailab.github.io/layout-dm/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter is Not All You Need: Starting from Non-Parametric Networks for
  3D Point Cloud Analysis <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Liuhui Wang, Yali Wang, Peng Gao, Hongsheng Li, Jianbo Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Non-parametric Network for 3D point cloud analysis, Point-NN,
which consists of purely non-learnable components: farthest point sampling
(FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric
functions. Surprisingly, it performs well on various 3D tasks, requiring no
parameters or training, and even surpasses existing fully trained models.
Starting from this basic non-parametric model, we propose two extensions.
First, Point-NN can serve as a base architectural framework to construct
Parametric Networks by simply inserting linear layers on top. Given the
superior non-parametric foundation, the derived Point-PN exhibits a high
performance-efficiency trade-off with only a few learnable parameters. Second,
Point-NN can be regarded as a plug-and-play module for the already trained 3D
models during inference. Point-NN captures the complementary geometric
knowledge and enhances existing methods for different 3D benchmarks without
re-training. We hope our work may cast a light on the community for
understanding 3D point clouds with non-parametric methods. Code is available at
https://github.com/ZrrSkywalker/Point-NN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Code is available at
  https://github.com/ZrrSkywalker/Point-NN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesh<span class="highlight-title">Diffusion</span>: Score-based Generative 3D Mesh Modeling <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of generating realistic 3D shapes, which is useful for a
variety of applications such as automatic scene generation and physical
simulation. Compared to other 3D representations like voxels and point clouds,
meshes are more desirable in practice, because (1) they enable easy and
arbitrary manipulation of shapes for relighting and simulation, and (2) they
can fully leverage the power of modern graphics pipelines which are mostly
optimized for meshes. Previous scalable methods for generating meshes typically
rely on sub-optimal post-processing, and they tend to produce overly-smooth or
noisy surfaces without fine-grained geometric details. To overcome these
shortcomings, we take advantage of the graph structure of meshes and use a
simple yet very effective generative modeling method to generate 3D meshes.
Specifically, we represent meshes with deformable tetrahedral grids, and then
train a diffusion model on this direct parametrization. We demonstrate the
effectiveness of our model on multiple generative tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2023 (Spotlight, Notable-top-25%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstMove: Instance Motion for Object-centric Video Segmentation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Liu, Junfeng Wu, Yi Jiang, Xiang Bai, Alan Yuille, Song Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant efforts, cutting-edge video segmentation methods still
remain sensitive to occlusion and rapid movement, due to their reliance on the
appearance of objects in the form of object embeddings, which are vulnerable to
these disturbances. A common solution is to use optical flow to provide motion
information, but essentially it only considers pixel-level motion, which still
relies on appearance similarity and hence is often inaccurate under occlusion
and fast movement. In this work, we study the instance-level motion and present
InstMove, which stands for Instance Motion for Object-centric Video
Segmentation. In comparison to pixel-wise motion, InstMove mainly relies on
instance-level motion information that is free from image feature embeddings,
and features physical interpretations, making it more accurate and robust
toward occlusion and fast-moving objects. To better fit in with the video
segmentation tasks, InstMove uses instance masks to model the physical presence
of an object and learns the dynamic model through a memory network to predict
its position and shape in the next frame. With only a few lines of code,
InstMove can be integrated into current SOTA methods for three different video
segmentation tasks and boost their performance. Specifically, we improve the
previous arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and
4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast-moving objects.
These results suggest that instance-level motion is robust and accurate, and
hence serving as a powerful solution in complex scenarios for object-centric
video segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Framework for Open-Vocabulary Segmentation and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present \ourmodel{}, a simple Open-vocabulary Segmentation and Detection
framework that jointly learns from different segmentation and detection
datasets. To bridge the gap of vocabulary and annotation granularity, we first
introduce a pre-trained text encoder to encode all the visual concepts in two
tasks and learn a common semantic space for them. This gives us reasonably good
results compared with the counterparts trained on segmentation task only. To
further reconcile them, we locate two discrepancies: $i$) task discrepancy --
segmentation requires extracting masks for both foreground objects and
background stuff, while detection merely cares about the former; $ii$) data
discrepancy -- box and mask annotations are with different spatial granularity,
and thus not directly interchangeable. To address these issues, we propose a
decoupled decoding to reduce the interference between foreground/background and
a conditioned mask decoding to assist in generating masks for given boxes. To
this end, we develop a simple encoder-decoder model encompassing all three
techniques and train it jointly on COCO and Objects365. After pre-training, our
model exhibits competitive or stronger zero-shot transferability for both
segmentation and detection. Specifically, \ourmodel{} beats the
state-of-the-art method for open-vocabulary instance and panoptic segmentation
across 5 datasets, and outperforms previous work for open-vocabulary detection
on LVIS and ODinW under similar settings. When transferred to specific tasks,
our model achieves new SoTA for panoptic segmentation on COCO and ADE20K, and
instance segmentation on ADE20K and Cityscapes.
  Finally, we note that \ourmodel{} is the first to explore the potential of
joint training on segmentation and detection, and hope it can be received as a
strong baseline for developing a single model for both tasks in open world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A Simple Framework for Open-Vocabulary Segmentation and Detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D
  Object Detection <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang, Yuheng Lu, Yandong Guo, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders learn strong visual representations and achieve
state-of-the-art results in several independent modalities, yet very few works
have addressed their capabilities in multi-modality settings. In this work, we
focus on point cloud and RGB image data, two modalities that are often
presented together in the real world, and explore their meaningful
interactions. To improve upon the cross-modal synergy in existing works, we
propose PiMAE, a self-supervised pre-training framework that promotes 3D and 2D
interaction through three aspects. Specifically, we first notice the importance
of masking strategies between the two sources and utilize a projection module
to complementarily align the mask and visible tokens of the two modalities.
Then, we utilize a well-crafted two-branch MAE pipeline with a novel shared
decoder to promote cross-modality interaction in the mask tokens. Finally, we
design a unique cross-modal reconstruction module to enhance representation
learning for both modalities. Through extensive experiments performed on
large-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2), we
discover it is nontrivial to interactively learn point-image features, where we
greatly improve multiple 3D detectors, 2D detectors, and few-shot classifiers
by 2.9%, 6.7%, and 2.4%, respectively. Code is available at
https://github.com/BLVLab/PiMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023. Code is available at
  https://github.com/BLVLab/PiMAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViperGPT: Visual Inference via Python Execution for Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dídac Surís, Sachit Menon, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering visual queries is a complex task that requires both visual
processing and reasoning. End-to-end models, the dominant approach for this
task, do not explicitly differentiate between the two, limiting
interpretability and generalization. Learning modular programs presents a
promising alternative, but has proven challenging due to the difficulty of
learning both the programs and modules simultaneously. We introduce ViperGPT, a
framework that leverages code-generation models to compose vision-and-language
models into subroutines to produce a result for any query. ViperGPT utilizes a
provided API to access the available modules, and composes them by generating
Python code that is later executed. This simple approach requires no further
training, and achieves state-of-the-art results across various complex visual
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://viper.cs.columbia.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind Video Deflickering by Neural Filtering with a Flawed Atlas <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Lei, Xuanchi Ren, Zhaoxiang Zhang, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many videos contain flickering artifacts. Common causes of flicker include
video processing algorithms, video generation algorithms, and capturing videos
under specific situations. Prior work usually requires specific guidance such
as the flickering frequency, manual annotations, or extra consistent videos to
remove the flicker. In this work, we propose a general flicker removal
framework that only receives a single flickering video as input without
additional guidance. Since it is blind to a specific flickering type or
guidance, we name this "blind deflickering." The core of our approach is
utilizing the neural atlas in cooperation with a neural filtering strategy. The
neural atlas is a unified representation for all frames in a video that
provides temporal consistency guidance but is flawed in many cases. To this
end, a neural network is trained to mimic a filter to learn the consistent
features (e.g., color, brightness) and avoid introducing the artifacts in the
atlas. To validate our method, we construct a dataset that contains diverse
real-world flickering videos. Extensive experiments show that our method
achieves satisfying deflickering performance and even outperforms baselines
that use extra guidance on a public benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2023. Code:
  github.com/ChenyangLEI/All-In-One-Deflicker Website:
  chenyanglei.github.io/deflicker</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homeomorphic Image Registration via Conformal-Invariant Hyperelastic
  Regularisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zou, Noémie Debroux, Lihao Liu, Jing Qin, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration is a fundamental task in medical image analysis
and plays a crucial role in a wide range of clinical applications. Recently,
deep learning-based approaches have been widely studied for deformable medical
image registration and achieved promising results. However, existing deep
learning image registration techniques do not theoretically guarantee
topology-preserving transformations. This is a key property to preserve
anatomical structures and achieve plausible transformations that can be used in
real clinical settings. We propose a novel framework for deformable image
registration. Firstly, we introduce a novel regulariser based on
conformal-invariant properties in a nonlinear elasticity setting. Our
regulariser enforces the deformation field to be smooth, invertible and
orientation-preserving. More importantly, we strictly guarantee topology
preservation yielding to a clinical meaningful registration. Secondly, we boost
the performance of our regulariser through coordinate MLPs, where one can view
the to-be-registered images as continuously differentiable entities. We
demonstrate, through numerical and visual experiments, that our framework is
able to outperform current techniques for image registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MELON: NeRF with Unposed Images Using Equivalence Class Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, Dmitry Lagun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields enable novel-view synthesis and scene reconstruction
with photorealistic quality from a few images, but require known and accurate
camera poses. Conventional pose estimation algorithms fail on smooth or
self-similar scenes, while methods performing inverse rendering from unposed
views require a rough initialization of the camera orientations. The main
difficulty of pose estimation lies in real-life objects being almost invariant
under certain transformations, making the photometric distance between rendered
views non-convex with respect to the camera parameters. Using an equivalence
relation that matches the distribution of local minima in camera space, we
reduce this space to its quotient set, in which pose estimation becomes a more
convex problem. Using a neural-network to regularize pose estimation, we
demonstrate that our method - MELON - can reconstruct a neural radiance field
from unposed images with state-of-the-art accuracy while requiring ten times
fewer views than adversarial approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alias-Free Convnets: Fractional Shift Invariance via Polynomial
  Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hagay Michaeli, Tomer Michaeli, Daniel Soudry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although CNNs are believed to be invariant to translations, recent works have
shown this is not the case, due to aliasing effects that stem from downsampling
layers. The existing architectural solutions to prevent aliasing are partial
since they do not solve these effects, that originate in non-linearities. We
propose an extended anti-aliasing method that tackles both downsampling and
non-linear layers, thus creating truly alias-free, shift-invariant CNNs. We
show that the presented model is invariant to integer as well as fractional
(i.e., sub-pixel) translations, thus outperforming other shift-invariant
methods in terms of robustness to adversarial translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/hmichaeli/alias_free_convnets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Editing Implicit Assumptions in Text-to-Image <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Bahjat Kawar, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models often make implicit assumptions about the
world when generating images. While some assumptions are useful (e.g., the sky
is blue), they can also be outdated, incorrect, or reflective of social biases
present in the training data. Thus, there is a need to control these
assumptions without requiring explicit user input or costly re-training. In
this work, we aim to edit a given implicit assumption in a pre-trained
diffusion model. Our Text-to-Image Model Editing method, TIME for short,
receives a pair of inputs: a "source" under-specified prompt for which the
model makes an implicit assumption (e.g., "a pack of roses"), and a
"destination" prompt that describes the same setting, but with a specified
desired attribute (e.g., "a pack of blue roses"). TIME then updates the model's
cross-attention layers, as these layers assign visual meaning to textual
tokens. We edit the projection matrices in these layers such that the source
prompt is projected close to the destination prompt. Our method is highly
efficient, as it modifies a mere 2.2% of the model's parameters in under one
second. To evaluate model editing approaches, we introduce TIMED (TIME
Dataset), containing 147 source and destination prompt pairs from various
domains. Our experiments (using Stable Diffusion) show that TIME is successful
in model editing, generalizes well for related prompts unseen during editing,
and imposes minimal effect on unrelated generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://time-diffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable ODE-style Generative <span class="highlight-title">Diffusion</span> Model via Force Field
  Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyang Jin, Yongpei Zhu, Yuxi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a considerable time, researchers have focused on developing a method that
establishes a deep connection between the generative diffusion model and
mathematical physics. Despite previous efforts, progress has been limited to
the pursuit of a single specialized method. In order to advance the
interpretability of diffusion models and explore new research directions, it is
essential to establish a unified ODE-style generative diffusion model. Such a
model should draw inspiration from physical models and possess a clear
geometric meaning. This paper aims to identify various physical models that are
suitable for constructing ODE-style generative diffusion models accurately from
a mathematical perspective. We then summarize these models into a unified
method. Additionally, we perform a case study where we use the theoretical
model identified by our method to develop a range of new diffusion model
methods, and conduct experiments. Our experiments on CIFAR-10 demonstrate the
effectiveness of our approach. We have constructed a computational framework
that attains highly proficient results with regards to image generation speed,
alongside an additional model that demonstrates exceptional performance in both
Inception score and FID score. These results underscore the significance of our
method in advancing the field of diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16pages, 13figures, 2tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud <span class="highlight-title">Diffusion</span> Models for Automatic Implant Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Friedrich, Julia Wolleb, Florentin Bieder, Florian M. Thieringer, Philippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in 3D printing of biocompatible materials make patient-specific
implants increasingly popular. The design of these implants is, however, still
a tedious and largely manual process. Existing approaches to automate implant
generation are mainly based on 3D U-Net architectures on downsampled or
patch-wise data, which can result in a loss of detail or contextual
information. Following the recent success of Diffusion Probabilistic Models, we
propose a novel approach for implant generation based on a combination of 3D
point cloud diffusion models and voxelization networks. Due to the stochastic
sampling process in our diffusion model, we can propose an ensemble of
different implants per defect, from which the physicians can choose the most
suitable one. We evaluate our method on the SkullBreak and SkullFix datasets,
generating high-quality implants and achieving competitive evaluation scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective and Objective Quality Assessment for in-the-Wild Computer
  Graphics Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Wei Sun, Tao Wang, Wei Lu, Quan Zhou, Jun he, Qiyuan Wang, Xiongkuo Min, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer graphics images (CGIs) are artificially generated by means of
computer programs and are widely perceived under various scenarios, such as
games, streaming media, etc. In practical, the quality of CGIs consistently
suffers from poor rendering during the production and inevitable compression
artifacts during the transmission of multimedia applications. However, few
works have been dedicated to dealing with the challenge of computer graphics
images quality assessment (CGIQA). Most image quality assessment (IQA) metrics
are developed for natural scene images (NSIs) and validated on the databases
consisting of NSIs with synthetic distortions, which are not suitable for
in-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and
CGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000
CGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled
laboratory environment to obtain the accurate perceptual ratings of the CGIs.
Then, we propose an effective deep learning-based no-reference (NR) IQA model
by utilizing multi-stage feature fusion strategy and multi-stage channel
attention mechanism. The major motivation of the proposed model is to make full
use of inter-channel information from low-level to high-level since CGIs have
apparent patterns as well as rich interactive semantic content. Experimental
results show that the proposed method outperforms all other state-of-the-art NR
IQA methods on the constructed CGIQA-6k database and other CGIQA-related
databases. The database along with the code will be released to facilitate
further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISimDL: Importance Sampling-Driven Acceleration of Fault Injection
  Simulations for Evaluating the Robustness of Deep Learning <span class="chip">IJCNN2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Colucci, Andreas Steininger, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) systems have proliferated in many applications, requiring
specialized hardware accelerators and chips. In the nano-era, devices have
become increasingly more susceptible to permanent and transient faults.
Therefore, we need an efficient methodology for analyzing the resilience of
advanced DL systems against such faults, and understand how the faults in
neural accelerator chips manifest as errors at the DL application level, where
faults can lead to undetectable and unrecoverable errors. Using fault
injection, we can perform resilience investigations of the DL system by
modifying neuron weights and outputs at the software-level, as if the hardware
had been affected by a transient fault. Existing fault models reduce the search
space, allowing faster analysis, but requiring a-priori knowledge on the model,
and not allowing further analysis of the filtered-out search space. Therefore,
we propose ISimDL, a novel methodology that employs neuron sensitivity to
generate importance sampling-based fault-scenarios. Without any a-priori
knowledge of the model-under-test, ISimDL provides an equivalent reduction of
the search space as existing works, while allowing long simulations to cover
all the possible faults, improving on existing model requirements. Our
experiments show that the importance sampling provides up to 15x higher
precision in selecting critical faults than the random uniform sampling,
reaching such precision in less than 100 faults. Additionally, we showcase
another practical use-case for importance sampling for reliable DNN design,
namely Fault Aware Training (FAT). By using ISimDL to select the faults leading
to errors, we can insert the faults during the DNN training process to harden
the DNN against such faults. Using importance sampling in FAT reduces the
overhead required for finding faults that lead to a predetermined drop in
accuracy by more than 12x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IJCNN2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-level Multiple Distributions Representation are Necessary for
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianjian Yin, Zhichao Zheng, Yanhui Gu, Junsheng Zhou, Yi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches focus on using class-level features to improve semantic
segmentation performance. How to characterize the relationships of intra-class
pixels and inter-class pixels is the key to extract the discriminative
representative class-level features. In this paper, we introduce for the first
time to describe intra-class variations by multiple distributions. Then,
multiple distributions representation learning(\textbf{MDRL}) is proposed to
augment the pixel representations for semantic segmentation. Meanwhile, we
design a class multiple distributions consistency strategy to construct
discriminative multiple distribution representations of embedded pixels.
Moreover, we put forward a multiple distribution semantic aggregation module to
aggregate multiple distributions of the corresponding class to enhance pixel
semantic information. Our approach can be seamlessly integrated into popular
segmentation frameworks FCN/PSPNet/CCNet and achieve 5.61\%/1.75\%/0.75\% mIoU
improvements on ADE20K. Extensive experiments on the Cityscapes, ADE20K
datasets have proved that our method can bring significant performance
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep
  Ensembles are More Efficient than Single Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Ensembles are a simple, reliable, and effective method of improving both
the predictive performance and uncertainty estimates of deep learning
approaches. However, they are widely criticised as being computationally
expensive, due to the need to deploy multiple independent models. Recent work
has challenged this view, showing that for predictive accuracy, ensembles can
be more computationally efficient (at inference) than scaling single models
within an architecture family. This is achieved by cascading ensemble members
via an early-exit approach. In this work, we investigate extending these
efficiency gains to tasks related to uncertainty estimation. As many such
tasks, e.g. selective classification, are binary classification, our key novel
insight is to only pass samples within a window close to the binary decision
boundary to later cascade stages. Experiments on ImageNet-scale data across a
number of network architectures and uncertainty tasks show that the proposed
window-based early-exit approach is able to achieve a superior
uncertainty-computation trade-off compared to scaling single models. For
example, a cascaded EfficientNet-B2 ensemble is able to achieve similar
coverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.
We also find that cascades/ensembles give more reliable improvements on OOD
data vs scaling models up. Code for this work is available at:
https://github.com/Guoxoug/window-early-exit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FingerSLAM: Closed-loop Unknown Object Localization and Reconstruction
  from Visuo-tactile Feedback <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Zhao, Maria Bauza, Edward H. Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the problem of using visuo-tactile feedback for
6-DoF localization and 3D reconstruction of unknown in-hand objects. We propose
FingerSLAM, a closed-loop factor graph-based pose estimator that combines local
tactile sensing at finger-tip and global vision sensing from a wrist-mount
camera. FingerSLAM is constructed with two constituent pose estimators: a
multi-pass refined tactile-based pose estimator that captures movements from
detailed local textures, and a single-pass vision-based pose estimator that
predicts from a global view of the object. We also design a loop closure
mechanism that actively matches current vision and tactile images to previously
stored key-frames to reduce accumulated error. FingerSLAM incorporates the two
sensing modalities of tactile and vision, as well as the loop closure mechanism
with a factor graph-based optimization framework. Such a framework produces an
optimized pose estimation solution that is more accurate than the standalone
estimators. The estimated poses are then used to reconstruct the shape of the
unknown object incrementally by stitching the local point clouds recovered from
tactile images. We train our system on real-world data collected with 20
objects. We demonstrate reliable visuo-tactile pose estimation and shape
reconstruction through quantitative and qualitative real-world evaluations on 6
objects that are unseen during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted and accepted to 2023 IEEE International Conference on
  Robotics and Automation (ICRA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Subhankar Ghosh, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air-writing refers to virtually writing linguistic characters through hand
gestures in three-dimensional space with six degrees of freedom. This paper
proposes a generic video camera-aided convolutional neural network (CNN) based
air-writing framework. Gestures are performed using a marker of fixed color in
front of a generic video camera, followed by color-based segmentation to
identify the marker and track the trajectory of the marker tip. A pre-trained
CNN is then used to classify the gesture. The recognition accuracy is further
improved using transfer learning with the newly acquired data. The performance
of the system varies significantly on the illumination condition due to
color-based segmentation. In a less fluctuating illumination condition, the
system is able to recognize isolated unistroke numerals of multiple languages.
The proposed framework has achieved 97.7%, 95.4% and 93.7% recognition rates in
person independent evaluations on English, Bengali and Devanagari numerals,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The International Conference on Frontiers of Handwriting
  Recognition (ICFHR) 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Slimani, Brahim Tamadazte, Catherine Achard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new method for 3D point cloud registration based on
deep learning. The architecture is composed of three distinct blocs: (i) an
encoder composed of a convolutional graph-based descriptor that encodes the
immediate neighbourhood of each point and an attention mechanism that encodes
the variations of the surface normals. Such descriptors are refined by
highlighting attention between the points of the same set and then between the
points of the two sets. (ii) a matching process that estimates a matrix of
correspondences using the Sinkhorn algorithm. (iii) Finally, the rigid
transformation between the two point clouds is calculated by RANSAC using the
Kc best scores from the correspondence matrix. We conduct experiments on the
ModelNet40 dataset, and our proposed architecture shows very promising results,
outperforming state-of-the-art methods in most of the simulated configurations,
including partial overlap and data augmentation with Gaussian noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaMixer: A Regularization Strategy for Online Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maorong Wang, Ling Xiao, Toshihiko Yamasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online knowledge distillation (KD) has received increasing attention in
recent years. However, while most existing online KD methods focus on
developing complicated model structures and training strategies to improve the
distillation of high-level knowledge like probability distribution, the effects
of the multi-level knowledge in the online KD are greatly overlooked,
especially the low-level knowledge. Thus, to provide a novel viewpoint to
online KD, we propose MetaMixer, a regularization strategy that can strengthen
the distillation by combining the low-level knowledge that impacts the
localization capability of the networks, and high-level knowledge that focuses
on the whole image. Experiments under different conditions show that MetaMixer
can achieve significant performance gains over state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edit-A-Video: Single Video Editing with Object-Aware Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the fact that text-to-video (TTV) model has recently achieved
remarkable success, there have been few approaches on TTV for its extension to
video editing. Motivated by approaches on TTV models adapting from
diffusion-based text-to-image (TTI) models, we suggest the video editing
framework given only a pretrained TTI model and a single <text, video> pair,
which we term Edit-A-Video. The framework consists of two stages: (1) inflating
the 2D model into the 3D model by appending temporal modules and tuning on the
source video (2) inverting the source video into the noise and editing with
target text prompt and attention map injection. Each stage enables the temporal
modeling and preservation of semantic attributes of the source video. One of
the key challenges for video editing include a background inconsistency
problem, where the regions not included for the edit suffer from undesirable
and inconsistent temporal alterations. To mitigate this issue, we also
introduce a novel mask blending method, termed as sparse-causal blending (SC
Blending). We improve previous mask blending methods to reflect the temporal
consistency so that the area where the editing is applied exhibits smooth
transition while also achieving spatio-temporal consistency of the unedited
regions. We present extensive experimental results over various types of text
and videos, and demonstrate the superiority of the proposed method compared to
baselines in terms of background consistency, text alignment, and video editing
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Contrastive Unsupervised Learning of Physiological Signals from
  Video <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Speth, Nathan Vance, Patrick Flynn, Adam Czajka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subtle periodic signals such as blood volume pulse and respiration can be
extracted from RGB video, enabling remote health monitoring at low cost.
Advancements in remote pulse estimation -- or remote photoplethysmography
(rPPG) -- are currently driven by deep learning solutions. However, modern
approaches are trained and evaluated on benchmark datasets with associated
ground truth from contact-PPG sensors. We present the first non-contrastive
unsupervised learning framework for signal regression to break free from the
constraints of labelled video data. With minimal assumptions of periodicity and
finite bandwidth, our approach is capable of discovering the blood volume pulse
directly from unlabelled videos. We find that encouraging sparse power spectra
within normal physiological bandlimits and variance over batches of power
spectra is sufficient for learning visual features of periodic signals. We
perform the first experiments utilizing unlabelled video data not specifically
created for rPPG to train robust pulse rate estimators. Given the limited
inductive biases and impressive empirical results, the approach is
theoretically capable of discovering other periodic signals from video,
enabling multiple physiological measurements without the need for ground truth
signals. Codes to fully reproduce the experiments are made available along with
the paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Mesh Generation Through Sparse Latent Point <span class="highlight-title">Diffusion</span>
  Models <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesh generation is of great value in various applications involving computer
graphics and virtual content, yet designing generative models for meshes is
challenging due to their irregular data structure and inconsistent topology of
meshes in the same category. In this work, we design a novel sparse latent
point diffusion model for mesh generation. Our key insight is to regard point
clouds as an intermediate representation of meshes, and model the distribution
of point clouds instead. While meshes can be generated from point clouds via
techniques like Shape as Points (SAP), the challenges of directly generating
meshes can be effectively avoided. To boost the efficiency and controllability
of our mesh generation method, we propose to further encode point clouds to a
set of sparse latent points with point-wise semantic meaningful features, where
two DDPMs are trained in the space of sparse latent points to respectively
model the distribution of the latent point positions and features at these
latent points. We find that sampling in this latent space is faster than
directly sampling dense point clouds. Moreover, the sparse latent points also
enable us to explicitly control both the overall structures and local details
of the generated meshes. Extensive experiments are conducted on the ShapeNet
dataset, where our proposed sparse latent point diffusion model achieves
superior performance in terms of generation quality and controllability when
compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023. Project page is at https://slide-3d.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let 2D <span class="highlight-title">Diffusion</span> Model Know 3D-Consistency for Robust Text-to-3D
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-3D generation has shown rapid progress in recent days with the advent
of score distillation, a methodology of using pretrained text-to-2D diffusion
models to optimize neural radiance field (NeRF) in the zero-shot setting.
However, the lack of 3D awareness in the 2D diffusion models destabilizes score
distillation-based methods from reconstructing a plausible 3D scene. To address
this issue, we propose \ours, a novel framework that incorporates 3D awareness
into pretrained 2D diffusion models, enhancing the robustness and 3D
consistency of score distillation-based methods. We realize this by first
constructing a coarse 3D structure of a given text prompt and then utilizing
projected, view-specific depth map as a condition for the diffusion model.
Additionally, we introduce a training strategy that enables the 2D diffusion
model learns to handle the errors and sparsity within the coarse 3D structure
for robust generation, as well as a method for ensuring semantic consistency
throughout all viewpoints of the scene. Our framework surpasses the limitations
of prior arts, and has significant implications for 3D consistent generation of
2D diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://ku-cvlab.github.io/3DFuse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAA: A Delta Age AdaIN operation for age estimation via binary code
  <span class="highlight-title">transformer</span> <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Chen, Xingpeng Zhang, Ye Li, Ju Tao, Bin Xiao, Bing Wang, Zongjie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Naked eye recognition of age is usually based on comparison with the age of
others. However, this idea is ignored by computer tasks because it is difficult
to obtain representative contrast images of each age. Inspired by the transfer
learning, we designed the Delta Age AdaIN (DAA) operation to obtain the feature
difference with each age, which obtains the style map of each age through the
learned values representing the mean and standard deviation. We let the input
of transfer learning as the binary code of age natural number to obtain
continuous age feature information. The learned two groups of values in Binary
code mapping are corresponding to the mean and standard deviation of the
comparison ages. In summary, our method consists of four parts: FaceEncoder,
DAA operation, Binary code mapping, and AgeDecoder modules. After getting the
delta age via AgeDecoder, we take the average value of all comparison ages and
delta ages as the predicted age. Compared with state-of-the-art methods, our
method achieves better performance with fewer parameters on multiple facial age
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023; 8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisit Parameter-Efficient Transfer Learning: A Two-Stage Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhao, Hao Luo, Yuyang Zhao, Pichao Wang, Fan Wang, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Transfer Learning (PETL) aims at efficiently adapting
large models pre-trained on massive data to downstream tasks with limited
task-specific data. In view of the practicality of PETL, previous works focus
on tuning a small set of parameters for each downstream task in an end-to-end
manner while rarely considering the task distribution shift issue between the
pre-training task and the downstream task. This paper proposes a novel
two-stage paradigm, where the pre-trained model is first aligned to the target
distribution. Then the task-relevant information is leveraged for effective
adaptation. Specifically, the first stage narrows the task distribution shift
by tuning the scale and shift in the LayerNorm layers. In the second stage, to
efficiently learn the task-relevant information, we propose a Taylor
expansion-based importance score to identify task-relevant channels for the
downstream task and then only tune such a small portion of channels, making the
adaptation to be parameter-efficient. Overall, we present a promising new
direction for PETL, and the proposed paradigm achieves state-of-the-art
performance on the average accuracy of 19 downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-to-image <span class="highlight-title">Diffusion</span> Model in Generative AI: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey reviews text-to-image diffusion models in the context that
diffusion models have emerged to be popular for a wide range of generative
tasks. As a self-contained work, this survey starts with a brief introduction
of how a basic diffusion model works for image synthesis, followed by how
condition or guidance improves learning. Based on that, we present a review of
state-of-the-art methods on text-conditioned image synthesis, i.e.,
text-to-image. We further summarize applications beyond text-to-image
generation: text-guided creative generation and text-guided image editing.
Beyond the progress made so far, we discuss existing challenges and promising
future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First survey on the recent progress of text-to-image generation based
  on the diffusion model (under progress)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoEnsemble: Automated Ensemble Search Framework for Semantic
  Segmentation Using Image Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Ostrowski, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key bottleneck of employing state-of-the-art semantic segmentation networks
in the real world is the availability of training labels. Standard semantic
segmentation networks require massive pixel-wise annotated labels to reach
state-of-the-art prediction quality. Hence, several works focus on semantic
segmentation networks trained with only image-level annotations. However, when
scrutinizing the state-of-the-art results in more detail, we notice that
although they are very close to each other on average prediction quality,
different approaches perform better in different classes while providing low
quality in others. To address this problem, we propose a novel framework,
AutoEnsemble, which employs an ensemble of the "pseudo-labels" for a given set
of different segmentation techniques on a class-wise level. Pseudo-labels are
the pixel-wise predictions of the image-level semantic segmentation frameworks
used to train the final segmentation model. Our pseudo-labels seamlessly
combine the strong points of multiple segmentation techniques approaches to
reach superior prediction quality. We reach up to 2.4% improvement over
AutoEnsemble's components. An exhaustive analysis was performed to demonstrate
AutoEnsemble's effectiveness over state-of-the-art frameworks for image-level
semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted to a IEEE conference for peer review
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Ensemble Search Framework for Semantic Segmentation Using
  Medical Imaging Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Ostrowski, Bharath Srinivas Prabakaran, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable classification and detection of certain medical conditions, in
images, with state-of-the-art semantic segmentation networks, require vast
amounts of pixel-wise annotation. However, the public availability of such
datasets is minimal. Therefore, semantic segmentation with image-level labels
presents a promising alternative to this problem. Nevertheless, very few works
have focused on evaluating this technique and its applicability to the medical
sector. Due to their complexity and the small number of training examples in
medical datasets, classifier-based weakly supervised networks like class
activation maps (CAMs) struggle to extract useful information from them.
However, most state-of-the-art approaches rely on them to achieve their
improvements. Therefore, we propose a framework that can still utilize the
low-quality CAM predictions of complicated datasets to improve the accuracy of
our results. Our framework achieves that by first utilizing lower threshold
CAMs to cover the target object with high certainty; second, by combining
multiple low-threshold CAMs that even out their errors while highlighting the
target object. We performed exhaustive experiments on the popular multi-modal
BRATS and prostate DECATHLON segmentation challenge datasets. Using the
proposed framework, we have demonstrated an improved dice score of up to 8% on
BRATS and 6% on DECATHLON datasets compared to the previous state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Label based Semantic Segmentation Framework using Object
  Perimeters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Ostrowski, Bharath Srinivas Prabakaran, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-quality semantic segmentation predictions using only
image-level labels enables a new level of real-world applicability. Although
state-of-the-art networks deliver reliable predictions, the amount of
handcrafted pixel-wise annotations to enable these results are not feasible in
many real-world applications. Hence, several works have already targeted this
bottleneck, using classifier-based networks like Class Activation Maps (CAMs)
as a base. Addressing CAM's weaknesses of fuzzy borders and incomplete
predictions, state-of-the-art approaches rely only on adding regulations to the
classifier loss or using pixel-similarity-based refinement after the fact. We
propose a framework that introduces an additional module using object
perimeters for improved saliency. We define object perimeter information as the
line separating the object and background. Our new PerimeterFit module will be
applied to pre-refine the CAM predictions before using the
pixel-similarity-based network. In this way, our PerimeterFit increases the
quality of the CAM prediction while simultaneously improving the false negative
rate. We investigated a wide range of state-of-the-art unsupervised semantic
segmentation networks and edge detection techniques to create useful perimeter
maps, which enable our framework to predict object locations with sharper
perimeters. We achieved up to 1.5\% improvement over frameworks without our
PerimeterFit module. We conduct an exhaustive analysis to illustrate that our
framework enhances existing state-of-the-art frameworks for image-level-based
semantic segmentation. The framework is open-source and accessible online at
https://github.com/ErikOstrowski/Perimeter-based-Semantic-Segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaMask: Dynamic Mask Selection for Instance Segmentation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihuang Li, Chenhang He, Shuai Li, Yabin Zhang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The representative instance segmentation methods mostly segment different
object instances with a mask of the fixed resolution, e.g., 28*28 grid.
However, a low-resolution mask loses rich details, while a high-resolution mask
incurs quadratic computation overhead. It is a challenging task to predict the
optimal binary mask for each instance. In this paper, we propose to dynamically
select suitable masks for different object proposals. First, a dual-level
Feature Pyramid Network (FPN) with adaptive feature aggregation is developed to
gradually increase the mask grid resolution, ensuring high-quality segmentation
of objects. Specifically, an efficient region-level top-down path (r-FPN) is
introduced to incorporate complementary contextual and detailed information
from different stages of image-level FPN (i-FPN). Then, to alleviate the
increase of computation and memory costs caused by using large masks, we
develop a Mask Switch Module (MSM) with negligible computational cost to select
the most suitable mask resolution for each instance, achieving high efficiency
while maintaining high segmentation accuracy. Without bells and whistles, the
proposed method, namely DynaMask, brings consistent and noticeable performance
improvements over other state-of-the-arts at a moderate computation overhead.
The source code: https://github.com/lslrh/DynaMask.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Can Ground Earlier than See: An Effective and Efficient Pipeline for
  Temporal Sentence Grounding in Compressed Videos <span class="chip">CVPR-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fang, Daizong Liu, Pan Zhou, Guoshun Nan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a
target moment semantically according to a sentence query. Although previous
respectable works have made decent success, they only focus on high-level
visual features extracted from the consecutive decoded frames and fail to
handle the compressed videos for query modelling, suffering from insufficient
representation capability and significant computational complexity during
training and testing. In this paper, we pose a new setting, compressed-domain
TSG, which directly utilizes compressed videos rather than fully-decompressed
frames as the visual input. To handle the raw video bit-stream input, we
propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)
framework, which extracts and aggregates three kinds of low-level visual
features (I-frame, motion vector and residual features) for effective and
efficient grounding. Particularly, instead of encoding the whole decoded frames
like previous works, we capture the appearance representation by only learning
the I-frame feature to reduce delay or latency. Besides, we explore the motion
information not only by learning the motion vector feature, but also by
exploring the relations of neighboring frames via the residual feature. In this
way, a three-branch spatial-temporal attention layer with an adaptive
motion-appearance fusion module is further designed to extract and aggregate
both appearance and motion information for the final grounding. Experiments on
three challenging datasets shows that our TCSF achieves better performance than
other state-of-the-art methods with lower complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised
  Semantic Segmentation of Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Srinivas Prabakaran, Erik Ostrowski, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Semantic Segmentation (WSSS) with only image-level
supervision is a promising approach to deal with the need for Segmentation
networks, especially for generating a large number of pixel-wise masks in a
given dataset. However, most state-of-the-art image-level WSSS techniques lack
an understanding of the geometric features embedded in the images since the
network cannot derive any object boundary information from just image-level
labels. We define a boundary here as the line separating an object and its
background, or two different objects. To address this drawback, we propose our
novel BoundaryCAM framework, which deploys state-of-the-art class activation
maps combined with various post-processing techniques in order to achieve
fine-grained higher-accuracy segmentation masks. To achieve this, we
investigate a state-of-the-art unsupervised semantic segmentation network that
can be used to construct a boundary map, which enables BoundaryCAM to predict
object locations with sharper boundaries. By applying our method to WSSS
predictions, we were able to achieve up to 10% improvements even to the benefit
of the current state-of-the-art WSSS methods for medical imaging. The framework
is open-source and accessible online at
https://github.com/bharathprabakaran/BoundaryCAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network
  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Srinivas Prabakaran, Paul Hamelmann, Erik Ostrowski, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging is one of the most prominent technologies to evaluate the
growth, progression, and overall health of a fetus during its gestation.
However, the interpretation of the data obtained from such studies is best left
to expert physicians and technicians who are trained and well-versed in
analyzing such images. To improve the clinical workflow and potentially develop
an at-home ultrasound-based fetal monitoring platform, we present a novel fetus
phantom ultrasound dataset, FPUS23, which can be used to identify (1) the
correct diagnostic planes for estimating fetal biometric values, (2) fetus
orientation, (3) their anatomical features, and (4) bounding boxes of the fetus
phantom anatomies at 23 weeks gestation. The entire dataset is composed of
15,728 images, which are used to train four different Deep Neural Network
models, built upon a ResNet34 backbone, for detecting aforementioned fetus
features and use-cases. We have also evaluated the models trained using our
FPUS23 dataset, to show that the information learned by these models can be
used to substantially increase the accuracy on real-world ultrasound fetus
datasets. We make the FPUS23 dataset and the pre-trained models publicly
accessible at https://github.com/bharathprabakaran/FPUS23, which will further
facilitate future research on fetal ultrasound imaging and analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Stacked Autoregressive Model for Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Seo, Hakjin Lee, Doyi Kim, Junghoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future frame prediction has been approached through two primary methods:
autoregressive and non-autoregressive. Autoregressive methods rely on the
Markov assumption and can achieve high accuracy in the early stages of
prediction when errors are not yet accumulated. However, their performance
tends to decline as the number of time steps increases. In contrast,
non-autoregressive methods can achieve relatively high performance but lack
correlation between predictions for each time step. In this paper, we propose
an Implicit Stacked Autoregressive Model for Video Prediction (IAM4VP), which
is an implicit video prediction model that applies a stacked autoregressive
method. Like non-autoregressive methods, stacked autoregressive methods use the
same observed frame to estimate all future frames. However, they use their own
predictions as input, similar to autoregressive methods. As the number of time
steps increases, predictions are sequentially stacked in the queue. To evaluate
the effectiveness of IAM4VP, we conducted experiments on three common future
frame prediction benchmark datasets and weather\&climate prediction benchmark
datasets. The results demonstrate that our proposed model achieves
state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Facial Landmark Detection by Reference Heatmap <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wan, Jun Liu, Jie Zhou, Zhihui Lai, Linlin Shen, Hang Sun, Ping Xiong, Wenwen Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most facial landmark detection methods predict landmarks by mapping the input
facial appearance features to landmark heatmaps and have achieved promising
results. However, when the face image is suffering from large poses, heavy
occlusions and complicated illuminations, they cannot learn discriminative
feature representations and effective facial shape constraints, nor can they
accurately predict the value of each element in the landmark heatmap, limiting
their detection accuracy. To address this problem, we propose a novel Reference
Heatmap Transformer (RHT) by introducing reference heatmap information for more
precise facial landmark detection. The proposed RHT consists of a Soft
Transformation Module (STM) and a Hard Transformation Module (HTM), which can
cooperate with each other to encourage the accurate transformation of the
reference heatmap information and facial shape constraints. Then, a Multi-Scale
Feature Fusion Module (MSFFM) is proposed to fuse the transformed heatmap
features and the semantic features learned from the original face images to
enhance feature representations for producing more accurate target heatmaps. To
the best of our knowledge, this is the first study to explore how to enhance
facial landmark detection by transforming the reference heatmap information.
The experimental results from challenging benchmark datasets demonstrate that
our proposed method outperforms the state-of-the-art methods in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Image Processing, March 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quaternion Orthogonal <span class="highlight-title">Transformer</span> for Facial Expression Recognition in
  the Wild <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhou, Liyuan Guo, Lianghai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) is a challenging topic in artificial
intelligence. Recently, many researchers have attempted to introduce Vision
Transformer (ViT) to the FER task. However, ViT cannot fully utilize emotional
features extracted from raw images and requires a lot of computing resources.
To overcome these problems, we propose a quaternion orthogonal transformer
(QOT) for FER. Firstly, to reduce redundancy among features extracted from
pre-trained ResNet-50, we use the orthogonal loss to decompose and compact
these features into three sets of orthogonal sub-features. Secondly, three
orthogonal sub-features are integrated into a quaternion matrix, which
maintains the correlations between different orthogonal components. Finally, we
develop a quaternion vision transformer (Q-ViT) for feature classification. The
Q-ViT adopts quaternion operations instead of the original operations in ViT,
which improves the final accuracies with fewer parameters. Experimental results
on three in-the-wild FER datasets show that the proposed QOT outperforms
several state-of-the-art models and reduces the computations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Rotated Convolution for Rotated Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan, Zidong Wang, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rotated object detection aims to identify and locate objects in images with
arbitrary orientation. In this scenario, the oriented directions of objects
vary considerably across different images, while multiple orientations of
objects exist within an image. This intrinsic characteristic makes it
challenging for standard backbone networks to extract high-quality features of
these arbitrarily orientated objects. In this paper, we present Adaptive
Rotated Convolution (ARC) module to handle the aforementioned challenges. In
our ARC module, the convolution kernels rotate adaptively to extract object
features with varying orientations in different images, and an efficient
conditional computation mechanism is introduced to accommodate the large
orientation variations of objects within an image. The two designs work
seamlessly in rotated object detection problem. Moreover, ARC can conveniently
serve as a plug-and-play module in various vision backbones to boost their
representation ability to detect oriented objects accurately. Experiments on
commonly used benchmarks (DOTA and HRSC2016) demonstrate that equipped with our
proposed ARC module in the backbone network, the performance of multiple
popular oriented object detectors is significantly improved (e.g. +3.03% mAP on
Rotated RetinaNet and +4.16% on CFA). Combined with the highly competitive
method Oriented R-CNN, the proposed approach achieves state-of-the-art
performance on the DOTA dataset with 81.77% mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileVOS: Real-Time Video Object Segmentation Contrastive Learning
  meets Knowledge Distillation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Miles, Mehmet Kerim Yucel, Bruno Manganelli, Albert Saa-Garriga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the problem of semi-supervised video object segmentation
on resource-constrained devices, such as mobile phones. We formulate this
problem as a distillation task, whereby we demonstrate that small
space-time-memory networks with finite memory can achieve competitive results
with state of the art, but at a fraction of the computational cost (32
milliseconds per frame on a Samsung Galaxy S22). Specifically, we provide a
theoretically grounded framework that unifies knowledge distillation with
supervised contrastive representation learning. These models are able to
jointly benefit from both pixel-wise contrastive learning and distillation from
a pre-trained teacher. We validate this loss by achieving competitive J&F to
state of the art on both the standard DAVIS and YouTube benchmarks, despite
running up to 5x faster, and with 32x fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinematic Data-Based Action Segmentation for Surgical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Goldbraikh, Omer Shubi, Or Rubin, Carla M Pugh, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action segmentation is a challenging task in high-level process analysis,
typically performed on video or kinematic data obtained from various sensors.
In the context of surgical procedures, action segmentation is critical for
workflow analysis algorithms. This work presents two contributions related to
action segmentation on kinematic data. Firstly, we introduce two multi-stage
architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for
kinematic data. The architectures consist of a prediction generator with
intra-stage regularization and Bidirectional LSTM or GRU-based refinement
stages. Secondly, we propose two new data augmentation techniques, World Frame
Rotation and Horizontal-Flip, which utilize the strong geometric structure of
kinematic data to improve algorithm performance and robustness. We evaluate our
models on three datasets of surgical suturing tasks: the Variable Tissue
Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)
Dataset, both of which are open surgery simulation datasets collected by us, as
well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a
well-known benchmark in robotic surgery. Our methods achieve state-of-the-art
performance on all benchmark datasets and establish a strong baseline for the
BRS dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICICLE: Interpretable Class Incremental Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawid Rymarczyk, Joost van de Weijer, Bartosz Zieliński, Bartłomiej Twardowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning enables incremental learning of new tasks without
forgetting those previously learned, resulting in positive knowledge transfer
that can enhance performance on both new and old tasks. However, continual
learning poses new challenges for interpretability, as the rationale behind
model predictions may change over time, leading to interpretability concept
drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical
part-based approach. It consists of three crucial novelties: interpretability
regularization that distills previously learned concepts while preserving
user-friendly positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-recency bias
compensation devoted to prototypical parts. Our experimental results
demonstrate that ICICLE reduces the interpretability concept drift and
outperforms the existing exemplar-free methods of common class-incremental
learning when applied to concept-based models. We make the code available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, code will be shared after the acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Peng, Guanchun Wang, Lingxi Xie, Dongsheng Jiang, Wei Shen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seed area generation is usually the starting point of weakly supervised
semantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a
multi-label classification network is the de facto paradigm for seed area
generation, but CAMs generated from Convolutional Neural Networks (CNNs) and
Transformers are prone to be under- and over-activated, respectively, which
makes the strategies to refine CAMs for CNNs usually inappropriate for
Transformers, and vice versa. In this paper, we propose a Unified optimization
paradigm for Seed Area GEneration (USAGE) for both types of networks, in which
the objective function to be optimized consists of two terms: One is a
generation loss, which controls the shape of seed areas by a temperature
parameter following a deterministic principle for different types of networks;
The other is a regularization loss, which ensures the consistency between the
seed areas that are generated by self-adaptive network adjustment from
different views, to overturn false activation in seed areas. Experimental
results show that USAGE consistently improves seed area generation for both
CNNs and Transformers by large margins, e.g., outperforming state-of-the-art
methods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated
seed areas on Transformers, we achieve state-of-the-art WSSS results on both
PASCAL VOC and MS COCO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira, Oleksandr Maksymets, Dhruv Batra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a single neural network architecture composed of task-agnostic
components (ViTs, convolutions, and LSTMs) that achieves state-of-art results
on both the ImageNav ("go to location in <this picture>") and ObjectNav ("find
a chair") tasks without any task-specific modules like object detection,
segmentation, mapping, or planning modules. Such general-purpose methods offer
advantages of simplicity in design, positive scaling with available compute,
and versatile applicability to multiple tasks. Our work builds upon the recent
success of self-supervised learning (SSL) for pre-training vision transformers
(ViT). However, while the training recipes for convolutional networks are
mature and robust, the recipes for ViTs are contingent and brittle, and in the
case of ViTs for visual navigation, yet to be fully discovered. Specifically,
we find that vanilla ViTs do not outperform ResNets on visual navigation. We
propose the use of a compression layer operating over ViT patch representations
to preserve spatial information along with policy training improvements. These
improvements allow us to demonstrate positive scaling laws for the first time
in visual navigation tasks. Consequently, our model advances state-of-the-art
performance on ImageNav from 54.2% to 82.0% success and performs competitively
against concurrent state-of-art on ObjectNav with success rate of 64.0% vs.
65.0%. Overall, this work does not present a fundamentally new approach, but
rather recommendations for training a general-purpose architecture that
achieves state-of-art performance today and could serve as a strong baseline
for future methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Detection During Newborn Resuscitation Activities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Øyvind Meinich-Bache, Kjersti Engan, Ivar Austvoll, Trygve Eftestøl, Helge Myklebust, Ladislaus Blacy Yarrot, Hussein Kidanto, Hege Ersdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Birth asphyxia is a major newborn mortality problem in low-resource
countries. International guideline provides treatment recommendations; however,
the importance and effect of the different treatments are not fully explored.
The available data is collected in Tanzania, during newborn resuscitation, for
analysis of the resuscitation activities and the response of the newborn. An
important step in the analysis is to create activity timelines of the episodes,
where activities include ventilation, suction, stimulation etc. Methods: The
available recordings are noisy real-world videos with large variations. We
propose a two-step process in order to detect activities possibly overlapping
in time. The first step is to detect and track the relevant objects, like
bag-mask resuscitator, heart rate sensors etc., and the second step is to use
this information to recognize the resuscitation activities. The topic of this
paper is the first step, and the object detection and tracking are based on
convolutional neural networks followed by post processing. Results: The
performance of the object detection during activities were 96.97 %
(ventilations), 100 % (attaching/removing heart rate sensor) and 75 % (suction)
on a test set of 20 videos. The system also estimate the number of health care
providers present with a performance of 71.16 %. Conclusion: The proposed
object detection and tracking system provides promising results in noisy
newborn resuscitation videos. Significance: This is the first step in a
thorough analysis of newborn resuscitation episodes, which could provide
important insight about the importance and effect of different newborn
resuscitation activities
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activity Recognition From Newborn Resuscitation Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Øyvind Meinich-Bache, Simon Lennart Austnes, Kjersti Engan, Ivar Austvoll, Trygve Eftestøl, Helge Myklebust, Simeon Kusulla, Hussein Kidanto, Hege Ersdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Birth asphyxia is one of the leading causes of neonatal deaths. A
key for survival is performing immediate and continuous quality newborn
resuscitation. A dataset of recorded signals during newborn resuscitation,
including videos, has been collected in Haydom, Tanzania, and the aim is to
analyze the treatment and its effect on the newborn outcome. An important step
is to generate timelines of relevant resuscitation activities, including
ventilation, stimulation, suction, etc., during the resuscitation episodes.
Methods: We propose a two-step deep neural network system, ORAA-net, utilizing
low-quality video recordings of resuscitation episodes to do activity
recognition during newborn resuscitation. The first step is to detect and track
relevant objects using Convolutional Neural Networks (CNN) and post-processing,
and the second step is to analyze the proposed activity regions from step 1 to
do activity recognition using 3D CNNs. Results: The system recognized the
activities newborn uncovered, stimulation, ventilation and suction with a mean
precision of 77.67 %, a mean recall of 77,64 %, and a mean accuracy of 92.40 %.
Moreover, the accuracy of the estimated number of Health Care Providers (HCPs)
present during the resuscitation episodes was 68.32 %. Conclusion: The results
indicate that the proposed CNN-based two-step ORAAnet could be used for object
detection and activity recognition in noisy low-quality newborn resuscitation
videos. Significance: A thorough analysis of the effect the different
resuscitation activities have on the newborn outcome could potentially allow us
to optimize treatment guidelines, training, debriefing, and local quality
improvement in newborn resuscitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Free Sketch-Based Image Retrieval <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhra Chaudhuri, Ayan Kumar Bhunia, Yi-Zhe Song, Anjan Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rising concerns about privacy and anonymity preservation of deep learning
models have facilitated research in data-free learning (DFL). For the first
time, we identify that for data-scarce tasks like Sketch-Based Image Retrieval
(SBIR), where the difficulty in acquiring paired photos and hand-drawn sketches
limits data-dependent cross-modal learning algorithms, DFL can prove to be a
much more practical paradigm. We thus propose Data-Free (DF)-SBIR, where,
unlike existing DFL problems, pre-trained, single-modality classification
models have to be leveraged to learn a cross-modal metric-space for retrieval
without access to any training data. The widespread availability of pre-trained
classification models, along with the difficulty in acquiring paired
photo-sketch datasets for SBIR justify the practicality of this setting. We
present a methodology for DF-SBIR, which can leverage knowledge from models
independently trained to perform classification on photos and sketches. We
evaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks,
designing a variety of baselines based on state-of-the-art DFL literature, and
observe that our method surpasses all of them by significant margins. Our
method also achieves mAPs competitive with data-dependent approaches, all the
while requiring no training data. Implementation is available at
\url{https://github.com/abhrac/data-free-sbir}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Vision and Pattern Recognition (CVPR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imbalanced Domain Generalization for Robust Single Cell Classification
  in Hematological Cytomorphology <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Muhammad Umer, Armin Gruber, Sayedali Shetab Boushehri, Christian Metak, Carsten Marr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate morphological classification of white blood cells (WBCs) is an
important step in the diagnosis of leukemia, a disease in which nonfunctional
blast cells accumulate in the bone marrow. Recently, deep convolutional neural
networks (CNNs) have been successfully used to classify leukocytes by training
them on single-cell images from a specific domain. Most CNN models assume that
the distributions of the training and test data are similar, i.e., that the
data are independently and identically distributed. Therefore, they are not
robust to different staining protocols, magnifications, resolutions, scanners,
or imaging protocols, as well as variations in clinical centers or patient
cohorts. In addition, domain-specific data imbalances affect the generalization
performance of classifiers. Here, we train a robust CNN for WBC classification
by addressing cross-domain data imbalance and domain shifts. To this end, we
use two loss functions and demonstrate the effectiveness on out-of-distribution
(OOD) generalization. Our approach achieves the best F1 macro score compared to
other existing methods, and is able to consider rare cell types. This is the
first demonstration of imbalanced domain generalization in hematological
cytomorphology and paves the way for robust single cell classification methods
for the application in laboratories and clinics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a ICLR 2023 workshop paper: What do we need for
  successful domain generalization?</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Baseline for Supervised Surround-view Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianda Guo, Wenjie Yuan, Yunpeng Zhang, Tian Yang, Chenming Zhang, Zheng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation has been widely studied and serves as the fundamental step
of 3D perception for autonomous driving. Though significant progress has been
made for monocular depth estimation in the past decades, these attempts are
mainly conducted on the KITTI benchmark with only front-view cameras, which
ignores the correlations across surround-view cameras. In this paper, we
propose S3Depth, a Simple Baseline for Supervised Surround-view Depth
Estimation, to jointly predict the depth maps across multiple surrounding
cameras. Specifically, we employ a global-to-local feature extraction module
which combines CNN with transformer layers for enriched representations.
Further, the Adjacent-view Attention mechanism is proposed to enable the
intra-view and inter-view feature propagation. The former is achieved by the
self-attention module within each view, while the latter is realized by the
adjacent attention module, which computes the attention across multi-cameras to
exchange the multi-scale representations across surround-view feature maps.
Extensive experiments show that our method achieves superior performance over
existing state-of-the-art methods on both DDAD and nuScenes datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation-Guided Multi-Level Unified Network for Video Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Cheng, Xiangyu Wu, Dong Shen, Hezheng Lin, Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video grounding aims to locate the timestamps best matching the query
description within an untrimmed video. Prevalent methods can be divided into
moment-level and clip-level frameworks. Moment-level approaches directly
predict the probability of each transient moment to be the boundary in a global
perspective, and they usually perform better in coarse grounding. On the other
hand, clip-level ones aggregate the moments in different time windows into
proposals and then deduce the most similar one, leading to its advantage in
fine-grained grounding. In this paper, we propose a multi-level unified
framework to enhance performance by leveraging the merits of both moment-level
and clip-level methods. Moreover, a novel generation-guided paradigm in both
levels is adopted. It introduces a multi-modal generator to produce the
implicit boundary feature and clip feature, later regarded as queries to
calculate the boundary scores by a discriminator. The generation-guided
solution enhances video grounding from a two-unique-modals' match task to a
cross-modal attention task, which steps out of the previous framework and
obtains notable gains. The proposed Generation-guided Multi-level Unified
network (GMU) surpasses previous methods and reaches State-Of-The-Art on
various benchmarks with disparate features, e.g., Charades-STA, ActivityNet
captions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoG-CAN: local-global Class-aware Network for semantic segmentation of
  remote sensing images <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowen Ma, Mengting Ma, Chenlu Hu, Zhiyuan Song, Ziyan Zhao, Tian Feng, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing images are known of having complex backgrounds, high
intra-class variance and large variation of scales, which bring challenge to
semantic segmentation. We present LoG-CAN, a multi-scale semantic segmentation
network with a global class-aware (GCA) module and local class-aware (LCA)
modules to remote sensing images. Specifically, the GCA module captures the
global representations of class-wise context modeling to circumvent background
interference; the LCA modules generate local class representations as
intermediate aware elements, indirectly associating pixels with global class
representations to reduce variance within a class; and a multi-scale
architecture with GCA and LCA modules yields effective segmentation of objects
at different scales via cascaded refinement and fusion of features. Through the
evaluation on the ISPRS Vaihingen dataset and the ISPRS Potsdam dataset,
experimental results indicate that LoG-CAN outperforms the state-of-the-art
methods for general semantic segmentation, while significantly reducing network
parameters and computation. Code is available
at~\href{https://github.com/xwmaxwma/rssegmentation}{https://github.com/xwmaxwma/rssegmentation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliding at first order: Higher-order momentum distributions for
  discontinuous image registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lili Bao, Jiahao Lu, Shihui Ying, Stefan Sommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new approach to deformable image registration
that captures sliding motions. The large deformation diffeomorphic metric
mapping (LDDMM) registration method faces challenges in representing sliding
motion since it per construction generates smooth warps. To address this issue,
we extend LDDMM by incorporating both zeroth- and first-order momenta with a
non-differentiable kernel. This allows to represent both discontinuous
deformation at switching boundaries and diffeomorphic deformation in
homogeneous regions. We provide a mathematical analysis of the proposed
deformation model from the viewpoint of discontinuous systems. To evaluate our
approach, we conduct experiments on both artificial images and the publicly
available DIR-Lab 4DCT dataset. Results show the effectiveness of our approach
in capturing plausible sliding motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Cao, Yang Bai, Jingyao Wang, Ziqiang Cao, Liqiang Nie, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under the flourishing development in performance, current image-text
retrieval methods suffer from $N$-related time complexity, which hinders their
application in practice. Targeting at efficiency improvement, this paper
presents a simple and effective keyword-guided pre-screening framework for the
image-text retrieval. Specifically, we convert the image and text data into the
keywords and perform the keyword matching across modalities to exclude a large
number of irrelevant gallery samples prior to the retrieval network. For the
keyword prediction, we transfer it into a multi-label classification problem
and propose a multi-task learning scheme by appending the multi-label
classifiers to the image-text retrieval network to achieve a lightweight and
high-performance keyword prediction. For the keyword matching, we introduce the
inverted index in the search engine and create a win-win situation on both time
and space complexities for the pre-screening. Extensive experiments on two
widely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of
the proposed framework. The proposed framework equipped with only two embedding
layers achieves $O(1)$ querying time complexity, while improving the retrieval
efficiency and keeping its performance, when applied prior to the common
image-text retrieval methods. Our code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HALOS: Hallucination-free Or<span class="highlight-title">gan</span> Segmentation after Or<span class="highlight-title">gan</span> Resection
  Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne-Marie Rickmann, Murong Xu, Tom Nuno Wolf, Oksana Kovalenko, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide range of research in deep learning-based medical image segmentation
pushed the boundaries in a multitude of applications. A clinically relevant
problem that received less attention is the handling of scans with irregular
anatomy, e.g., after organ resection. State-of-the-art segmentation models
often lead to organ hallucinations, i.e., false-positive predictions of organs,
which cannot be alleviated by oversampling or post-processing. Motivated by the
increasing need to develop robust deep learning models, we propose HALOS for
abdominal organ segmentation in MR images that handles cases after organ
resection surgery. To this end, we combine missing organ classification and
multi-organ segmentation tasks into a multi-task model, yielding a
classification-assisted segmentation pipeline. The segmentation network learns
to incorporate knowledge about organ existence via feature fusion modules.
Extensive experiments on a small labeled test set and large-scale UK Biobank
data demonstrate the effectiveness of our approach in terms of higher
segmentation Dice scores and near-to-zero false positive prediction rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of Information Processing In Medical
  Imaging (IPMI) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BlinkFlow: A Dataset to Push the Limits of Event-based Optical Flow
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijin Li, Zhaoyang Huang, Shuo Chen, Xiaoyu Shi, Hongsheng Li, Hujun Bao, Zhaopeng Cui, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras provide high temporal precision, low data rates, and high
dynamic range visual perception, which are well-suited for optical flow
estimation. While data-driven optical flow estimation has obtained great
success in RGB cameras, its generalization performance is seriously hindered in
event cameras mainly due to the limited and biased training data. In this
paper, we present a novel simulator, BlinkSim, for the fast generation of
large-scale data for event-based optical flow. BlinkSim consists of a
configurable rendering engine and a flexible engine for event data simulation.
By leveraging the wealth of current 3D assets, the rendering engine enables us
to automatically build up thousands of scenes with different objects, textures,
and motion patterns and render very high-frequency images for realistic event
data simulation. Based on BlinkSim, we construct a large training dataset and
evaluation benchmark BlinkFlow that contains sufficient, diversiform, and
challenging event data with optical flow ground truth. Experiments show that
BlinkFlow improves the generalization performance of state-of-the-art methods
by more than 40% on average and up to 90%. Moreover, we further propose an
Event optical Flow transFormer (E-FlowFormer) architecture. Powered by our
BlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on MVSEC
dataset and 14% on DSEC dataset and presents the best generalization
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freehand 2D Ultrasound Probe Calibration for Image Fusion with 3D MRI/CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yogesh Langhe, Katrin Skerl, Adrien Bartoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this work is to implement a simple freehand ultrasound (US) probe
calibration technique. This will enable us to visualize US image data during
surgical procedures using augmented reality. The performance of the system was
evaluated with different experiments using two different pose estimation
techniques. A near-millimeter accuracy can be achieved with the proposed
approach. The developed system is cost-effective, simple and rapid with low
calibration error
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Face Arbitrary Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangwen Deng, Yingshuang Zou, Yuanhao Cai, Chendong Zhao, Yang Liu, Zhifang Liu, Yuxiao Liu, Jiawei Zhou, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style transfer of 3D faces has gained more and more attention. However,
previous methods mainly use images of artistic faces for style transfer while
ignoring arbitrary style images such as abstract paintings. To solve this
problem, we propose a novel method, namely Face-guided Dual Style Transfer
(FDST). To begin with, FDST employs a 3D decoupling module to separate facial
geometry and texture. Then we propose a style fusion strategy for facial
geometry. Subsequently, we design an optimization-based DDSG mechanism for
textures that can guide the style transfer by two style images. Besides the
normal style image input, DDSG can utilize the original face input as another
style input as the face prior. By this means, high-quality face arbitrary style
transfer results can be obtained. Furthermore, FDST can be applied in many
downstream tasks, including region-controllable style transfer, high-fidelity
face texture reconstruction, large-pose face reconstruction, and artistic face
reconstruction. Comprehensive quantitative and qualitative results show that
our method can achieve comparable performance. All source codes and pre-trained
weights will be released to the public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PATS: Patch Area Transportation with Subdivision for Local Feature
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ni, Yijin Li, Zhaoyang Huang, Hongsheng Li, Hujun Bao, Zhaopeng Cui, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local feature matching aims at establishing sparse correspondences between a
pair of images. Recently, detectorfree methods present generally better
performance but are not satisfactory in image pairs with large scale
differences. In this paper, we propose Patch Area Transportation with
Subdivision (PATS) to tackle this issue. Instead of building an expensive image
pyramid, we start by splitting the original image pair into equal-sized patches
and gradually resizing and subdividing them into smaller patches with the same
scale. However, estimating scale differences between these patches is
non-trivial since the scale differences are determined by both relative camera
poses and scene structures, and thus spatially varying over image pairs.
Moreover, it is hard to obtain the ground truth for real scenes. To this end,
we propose patch area transportation, which enables learning scale differences
in a self-supervised manner. In contrast to bipartite graph matching, which
only handles one-to-one matching, our patch area transportation can deal with
many-to-many relationships. PATS improves both matching accuracy and coverage,
and shows superior performance in downstream tasks, such as relative pose
estimation, visual localization, and optical flow estimation. The source code
will be released to benefit the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/pats</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisCoHead: Audio-and-Video-Driven Talking Head Generation by
  Disentangled Control of Head Pose and Facial Expressions <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geumbyeol Hwang, Sunwon Hong, Seunghyun Lee, Sungwoo Park, Gyeongsu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For realistic talking head generation, creating natural head motion while
maintaining accurate lip synchronization is essential. To fulfill this
challenging task, we propose DisCoHead, a novel method to disentangle and
control head pose and facial expressions without supervision. DisCoHead uses a
single geometric transformation as a bottleneck to isolate and extract head
motion from a head-driving video. Either an affine or a thin-plate spline
transformation can be used and both work well as geometric bottlenecks. We
enhance the efficiency of DisCoHead by integrating a dense motion estimator and
the encoder of a generator which are originally separate modules. Taking a step
further, we also propose a neural mix approach where dense motion is estimated
and applied implicitly by the encoder. After applying the disentangled head
motion to a source identity, DisCoHead controls the mouth region according to
speech audio, and it blinks eyes and moves eyebrows following a separate
driving video of the eye region, via the weight modulation of convolutional
neural networks. The experiments using multiple datasets show that DisCoHead
successfully generates realistic audio-and-video-driven talking heads and
outperforms state-of-the-art methods. Project page:
https://deepbrainai-research.github.io/discohead/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature representations useful for predicting image memorability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumi Harada, Hiroyuki Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting image memorability has attracted interest in various fields.
Consequently, prediction accuracy with convolutional neural network (CNN)
models has been approaching the empirical upper bound estimated based on human
consistency. However, identifying which feature representations embedded in CNN
models are responsible for such high prediction accuracy of memorability
remains an open question. To tackle this problem, this study sought to identify
memorability-related feature representations in CNN models using brain
similarity. Specifically, memorability prediction accuracy and brain similarity
were examined and assessed by Brain-Score across 16,860 layers in 64 CNN models
pretrained for object recognition. A clear tendency was shown in this
comprehensive analysis that layers with high memorability prediction accuracy
had higher brain similarity with the inferior temporal (IT) cortex, which is
the highest stage in the ventral visual pathway. Furthermore, fine-tuning the
64 CNN models revealed that brain similarity with the IT cortex at the
penultimate layer was positively correlated with memorability prediction
accuracy. This analysis also showed that the best fine-tuned model provided
accuracy comparable to the state-of-the-art CNN models developed specifically
for memorability prediction. Overall, this study's results indicated that the
CNN models' great success in predicting memorability relies on feature
representation acquisition similar to the IT cortex. This study advanced our
understanding of feature representations and its use for predicting image
memorability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sr-init: An interpretable layer pruning method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Tang, Yao Lu, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularization of deep neural networks (DNNs) in many fields, it
is still challenging to deploy state-of-the-art models to resource-constrained
devices due to high computational overhead. Model pruning provides a feasible
solution to the aforementioned challenges. However, the interpretation of
existing pruning criteria is always overlooked. To counter this issue, we
propose a novel layer pruning method by exploring the Stochastic
Re-initialization. Our SR-init method is inspired by the discovery that the
accuracy drop due to stochastic re-initialization of layer parameters differs
in various layers. On the basis of this observation, we come up with a layer
pruning criterion, i.e., those layers that are not sensitive to stochastic
re-initialization (low accuracy drop) produce less contribution to the model
and could be pruned with acceptable loss. Afterward, we experimentally verify
the interpretability of SR-init via feature visualization. The visual
explanation demonstrates that SR-init is theoretically feasible, thus we
compare it with state-of-the-art methods to further evaluate its
practicability. As for ResNet56 on CIFAR-10 and CIFAR-100, SR-init achieves a
great reduction in parameters (63.98% and 37.71%) with an ignorable drop in
top-1 accuracy (-0.56% and 0.8%). With ResNet50 on ImageNet, we achieve a
15.59% FLOPs reduction by removing 39.29% of the parameters, with only a drop
of 0.6% in top-1 accuracy. Our code is available at
https://github.com/huitang-zjut/SRinit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koos Classification of Vestibular Schwannoma via Image Translation-Based
  Unsupervised Cross-Modality Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang, Lisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koos grading scale is a classification system for vestibular schwannoma
(VS) used to characterize the tumor and its effects on adjacent brain
structures. The Koos classification captures many of the characteristics of
treatment deci-sions and is often used to determine treatment plans. Although
both contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2)
scanning can be used for Koos Classification, hrT2 scanning is gaining interest
because of its higher safety and cost-effectiveness. However, in the absence of
annotations for hrT2 scans, deep learning methods often inevitably suffer from
performance deg-radation due to unsupervised learning. If ceT1 scans and their
annotations can be used for unsupervised learning of hrT2 scans, the
performance of Koos classifi-cation using unlabeled hrT2 scans will be greatly
improved. In this regard, we propose an unsupervised cross-modality domain
adaptation method based on im-age translation by transforming annotated ceT1
scans into hrT2 modality and us-ing their annotations to achieve supervised
learning of hrT2 modality. Then, the VS and 7 adjacent brain structures related
to Koos classification in hrT2 scans were segmented. Finally, handcrafted
features are extracted from the segmenta-tion results, and Koos grade is
classified using a random forest classifier. The proposed method received rank
1 on the Koos classification task of the Cross-Modality Domain Adaptation
(crossMoDA 2022) challenge, with Macro-Averaged Mean Absolute Error (MA-MAE) of
0.2148 for the validation set and 0.26 for the test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Salient Object Detection with Co-Representation Purification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Zhu, Zhao Zhang, Zheng Lin, Xing Sun, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-salient object detection (Co-SOD) aims at discovering the common objects
in a group of relevant images. Mining a co-representation is essential for
locating co-salient objects. Unfortunately, the current Co-SOD method does not
pay enough attention that the information not related to the co-salient object
is included in the co-representation. Such irrelevant information in the
co-representation interferes with its locating of co-salient objects. In this
paper, we propose a Co-Representation Purification (CoRP) method aiming at
searching noise-free co-representation. We search a few pixel-wise embeddings
probably belonging to co-salient regions. These embeddings constitute our
co-representation and guide our prediction. For obtaining purer
co-representation, we use the prediction to iteratively reduce irrelevant
embeddings in our co-representation. Experiments on three datasets demonstrate
that our CoRP achieves state-of-the-art performances on the benchmark datasets.
Our source code is available at https://github.com/ZZY816/CoRP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One scalar is all you need -- absolute depth estimation using monocular
  self-supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Dana, Nadav Carmel, Amit Shomer, Ofer Manela, Tomer Peleg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimators can be trained or fine-tuned on
new scenes using only images and no ground-truth depth data, achieving good
accuracy. However, these estimators suffer from the inherent ambiguity of the
depth scale, significantly limiting their applicability. In this work, we
present a method for transferring the depth-scale from existing source datasets
collected with ground-truth depths to depth estimators that are trained using
self-supervision on a newly collected target dataset consisting of images only,
solving a significant limiting factor. We show that self-supervision based on
projective geometry results in predicted depths that are linearly correlated
with their ground-truth depths. Moreover, the linearity of this relationship
also holds when jointly training on images from two different (real or
synthetic) source and target domains. We utilize this observed property and
model the relationship between the ground-truth and the predicted up-to-scale
depths of images from the source domain using a single global scalar. Then, we
scale the predicted up-to-scale depths of images from the target domain using
the estimated global scaling factor, performing depth-scale transfer between
the two domains. This suggested method was evaluated on the target KITTI and
DDAD datasets, while using other real or synthetic source datasets, that have a
larger field-of-view, other image style or structural content. Our approach
achieves competitive accuracy on KITTI, even without using the specially
tailored vKITTI or vKITTI2 datasets, and higher accuracy on DDAD, when using
both real or synthetic source datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from
  Multi-view Images <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Ye, Renjiao Yi, Zhirui Gao, Chenyang Zhu, Zhiping Cai, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Normalization for Robust Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Faye, Mohamed-Djallel Dilmi, Hanane Azzag, Mustapha Lebbah, Fangchen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalization is a pre-processing step that converts the data into a more
usable representation. As part of the deep neural networks (DNNs), the batch
normalization (BN) technique uses normalization to address the problem of
internal covariate shift. It can be packaged as general modules, which have
been extensively integrated into various DNNs, to stabilize and accelerate
training, presumably leading to improved generalization. However, the effect of
BN is dependent on the mini-batch size and it does not take into account any
groups or clusters that may exist in the dataset when estimating population
statistics. This study proposes a new normalization technique, called context
normalization, for image data. This approach adjusts the scaling of features
based on the characteristics of each sample, which improves the model's
convergence speed and performance by adapting the data values to the context of
the target task. The effectiveness of context normalization is demonstrated on
various datasets, and its performance is compared to other standard
normalization techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimFLE: Simple Facial Landmark Encoding for <span class="highlight-title">Self-Supervised</span> Facial
  Expression Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyong Moon, Seongsik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the key issues in facial expression recognition in the wild (FER-W) is
that curating large-scale labeled facial images is challenging due to the
inherent complexity and ambiguity of facial images. Therefore, in this paper,
we propose a self-supervised simple facial landmark encoding (SimFLE) method
that can learn effective encoding of facial landmarks, which are important
features for improving the performance of FER-W, without expensive labels.
Specifically, we introduce novel FaceMAE module for this purpose. FaceMAE
reconstructs masked facial images with elaborately designed semantic masking.
Unlike previous random masking, semantic masking is conducted based on channel
information processed in the backbone, so rich semantics of channels can be
explored. Additionally, the semantic masking process is fully trainable,
enabling FaceMAE to guide the backbone to learn spatial details and contextual
properties of fine-grained facial landmarks. Experimental results on several
FER-W benchmarks prove that the proposed SimFLE is superior in facial landmark
localization and noticeably improved performance compared to the supervised
baseline and other self-supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Image-based Table Recognition Using Weakly Supervised Methods <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Tuan Ly, Atsuhiro Takasu, Phuc Nguyen, Hideaki Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the previous methods for table recognition rely on training datasets
containing many richly annotated table images. Detailed table image annotation,
e.g., cell or text bounding box annotation, however, is costly and often
subjective. In this paper, we propose a weakly supervised model named WSTabNet
for table recognition that relies only on HTML (or LaTeX) code-level
annotations of table images. The proposed model consists of three main parts:
an encoder for feature extraction, a structure decoder for generating table
structure, and a cell decoder for predicting the content of each cell in the
table. Our system is trained end-to-end by stochastic gradient descent
algorithms, requiring only table images and their ground-truth HTML (or LaTeX)
representations. To facilitate table recognition with deep learning, we create
and release WikiTableSet, the largest publicly available image-based table
recognition dataset built from Wikipedia. WikiTableSet contains nearly 4
million English table images, 590K Japanese table images, and 640k French table
images with corresponding HTML representation and cell bounding boxes. The
extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet
and PubTabNet demonstrate that the proposed weakly supervised model achieves
better, or similar accuracies compared to the state-of-the-art models on all
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, ICPRAM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via
  Raytracing in Neural SDFs <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Dianbing Xi, Lisha Wang, Rui Tang, Wei Hua, Hujun Bao, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene
reconstruction and editing using differentiable Monte Carlo raytracing on
neural signed distance fields (SDFs). Our holistic neural SDF-based framework
jointly recovers the underlying shapes, incident radiance and materials from
multi-view images. We introduce a novel bubble loss for fine-grained small
objects and error-guided adaptive sampling scheme to largely improve the
reconstruction quality on large-scale indoor scenes. Further, we propose to
decompose the neural radiance field into spatially-varying material of the
scene as a neural field through surface-based, differentiable Monte Carlo
raytracing and emitter semantic segmentations, which enables physically based
and photorealistic scene relighting and editing applications. Through a number
of qualitative and quantitative experiments, we demonstrate the superior
quality of our method on indoor scene reconstruction, novel view synthesis, and
scene editing compared to state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlanarTrack: A Large-scale Challenging Benchmark for Planar Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Liu, Xiaoqiong Liu, Ziruo Yi, Xin Zhou, Thanh Le, Libo Zhang, Yan Huang, Qing Yang, Heng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planar object tracking is a critical computer vision problem and has drawn
increasing interest owing to its key roles in robotics, augmented reality, etc.
Despite rapid progress, its further development, especially in the deep
learning era, is largely hindered due to the lack of large-scale challenging
benchmarks. Addressing this, we introduce PlanarTrack, a large-scale
challenging planar tracking benchmark. Specifically, PlanarTrack consists of
1,000 videos with more than 490K images. All these videos are collected in
complex unconstrained scenarios from the wild, which makes PlanarTrack,
compared with existing benchmarks, more challenging but realistic for
real-world applications. To ensure the high-quality annotation, each frame in
PlanarTrack is manually labeled using four corners with multiple-round careful
inspection and refinement. To our best knowledge, PlanarTrack, to date, is the
largest and most challenging dataset dedicated to planar object tracking. In
order to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and
conduct comprehensive comparisons and in-depth analysis. Our results, not
surprisingly, demonstrate that current top-performing planar trackers
degenerate significantly on the challenging PlanarTrack and more efforts are
needed to improve planar tracking in the future. In addition, we further derive
a variant named PlanarTrack$_{\mathbf{BB}}$ for generic object tracking from
PlanarTrack. Our evaluation of 10 excellent generic trackers on
PlanarTrack$_{\mathrm{BB}}$ manifests that, surprisingly,
PlanarTrack$_{\mathrm{BB}}$ is even more challenging than several popular
generic tracking benchmarks and more attention should be paid to handle such
planar objects, though they are rigid. All benchmarks and evaluations will be
released at the project webpage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech. Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Phrase Grounding with Region-Phrase Context Contrastive
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Chen, Yang Zhou, Anh Tran, Junting Zhao, Liang Wan, Gideon Ooi, Lionel Cheng, Choon Hua Thng, Xinxing Xu, Yong Liu, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical phrase grounding (MPG) aims to locate the most relevant region in a
medical image, given a phrase query describing certain medical findings, which
is an important task for medical image analysis and radiological diagnosis.
However, existing visual grounding methods rely on general visual features for
identifying objects in natural images and are not capable of capturing the
subtle and specialized features of medical findings, leading to sub-optimal
performance in MPG. In this paper, we propose MedRPG, an end-to-end approach
for MPG. MedRPG is built on a lightweight vision-language transformer encoder
and directly predicts the box coordinates of mentioned medical findings, which
can be trained with limited medical data, making it a valuable tool in medical
image analysis. To enable MedRPG to locate nuanced medical findings with better
region-phrase correspondences, we further propose Tri-attention Context
contrastive alignment (TaCo). TaCo seeks context alignment to pull both the
features and attention outputs of relevant region-phrase pairs close together
while pushing those of irrelevant regions far away. This ensures that the final
box prediction depends more on its finding-specific regions and phrases.
Experimental results on three MPG datasets demonstrate that our MedRPG
outperforms state-of-the-art visual grounding approaches by a large margin.
Additionally, the proposed TaCo strategy is effective in enhancing finding
localization ability and reducing spurious region-phrase correlations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variation of Gender Biases in Visual Recognition Models Before and After
  Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaspreet Ranjit, Tianlu Wang, Baishakhi Ray, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework to measure how biases change before and after
fine-tuning a large scale visual recognition model for a downstream task. Deep
learning models trained on increasing amounts of data are known to encode
societal biases. Many computer vision systems today rely on models typically
pretrained on large scale datasets. While bias mitigation techniques have been
developed for tuning models for downstream tasks, it is currently unclear what
are the effects of biases already encoded in a pretrained model. Our framework
incorporates sets of canonical images representing individual and pairs of
concepts to highlight changes in biases for an array of off-the-shelf
pretrained models across model sizes, dataset sizes, and training objectives.
Through our analyses, we find that (1) supervised models trained on datasets
such as ImageNet-21k are more likely to retain their pretraining biases
regardless of the target dataset compared to self-supervised models. We also
find that (2) models finetuned on larger scale datasets are more likely to
introduce new biased associations. Our results also suggest that (3) biases can
transfer to finetuned models and the finetuning objective and dataset can
impact the extent of transferred biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Robust Spiking Neural Networks with ViewPoint Transform and
  SpatioTemporal Stretching <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Shen, Juyu Xiao, Yihao Luo, Xiang Cao, Liangqi Zhang, Tianjiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic vision sensors (event cameras) simulate biological visual
perception systems and have the advantages of high temporal resolution, less
data redundancy, low power consumption, and large dynamic range. Since both
events and spikes are modeled from neural signals, event cameras are inherently
suitable for spiking neural networks (SNNs), which are considered promising
models for artificial intelligence (AI) and theoretical neuroscience. However,
the unconventional visual signals of these cameras pose a great challenge to
the robustness of spiking neural networks. In this paper, we propose a novel
data augmentation method, ViewPoint Transform and SpatioTemporal Stretching
(VPT-STS). It improves the robustness of SNNs by transforming the rotation
centers and angles in the spatiotemporal domain to generate samples from
different viewpoints. Furthermore, we introduce the spatiotemporal stretching
to avoid potential information loss in viewpoint transformation. Extensive
experiments on prevailing neuromorphic datasets demonstrate that VPT-STS is
broadly effective on multi-event representations and significantly outperforms
pure spatial geometric transformations. Notably, the SNNs model with VPT-STS
achieves a state-of-the-art accuracy of 84.4\% on the DVS-CIFAR10 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023. arXiv admin note: text overlap with
  arXiv:2207.11659</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSNet: a deep learning model based digital phase shifting algorithm from
  a single fringe image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoshuai Qi, Xiaojun Liu, Xiaolin Liu, Jiaqi Yang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the gold standard for phase retrieval, phase-shifting algorithm (PS) has
been widely used in optical interferometry, fringe projection profilometry,
etc. However, capturing multiple fringe patterns in PS limits the algorithm to
only a narrow range of application. To this end, a deep learning (DL) model
based digital PS algorithm from only a single fringe image is proposed. By
training on a simulated dataset of PS fringe patterns, the learnt model,
denoted PSNet, can predict fringe patterns with other PS steps when given a
pattern with the first PS step. Simulation and experiment results demonstrate
the PSNet's promising performance on accurate prediction of digital PS
patterns, and robustness to complex scenarios such as surfaces with varying
curvature and reflectance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages9 figures, a letter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Continuous Motion for 3D Point Cloud Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Luo, Gongjie Zhang, Changqing Zhou, Zhonghua Wu, Qingyi Tao, Lewei Lu, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of 3D single object tracking (SOT) with LiDAR point clouds is
crucial for various applications, such as autonomous driving and robotics.
However, existing approaches have primarily relied on appearance matching or
motion modeling within only two successive frames, thereby overlooking the
long-range continuous motion property of objects in 3D space. To address this
issue, this paper presents a novel approach that views each tracklet as a
continuous stream: at each timestamp, only the current frame is fed into the
network to interact with multi-frame historical features stored in a memory
bank, enabling efficient exploitation of sequential information. To achieve
effective cross-frame message passing, a hybrid attention mechanism is designed
to account for both long-range relation modeling and local geometric feature
extraction. Furthermore, to enhance the utilization of multi-frame features for
robust tracking, a contrastive sequence enhancement strategy is designed, which
uses ground truth tracklets to augment training sequences and promote
discrimination against false positives in a contrastive manner. Extensive
experiments demonstrate that the proposed method outperforms the
state-of-the-art method by significant margins (approximately 8%, 6%, and 12%
improvements in the success performance on KITTI, nuScenes, and Waymo,
respectively).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle
  Cooperative Perception <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu, Bolei Zhou, Jiaqi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern perception systems of autonomous vehicles are known to be sensitive to
occlusions and lack the capability of long perceiving range. It has been one of
the key bottlenecks that prevents Level 5 autonomy. Recent research has
demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system
has great potential to revolutionize the autonomous driving industry. However,
the lack of a real-world dataset hinders the progress of this field. To
facilitate the development of cooperative perception, we present V2V4Real, the
first large-scale real-world multi-modal dataset for V2V perception. The data
is collected by two vehicles equipped with multi-modal sensors driving together
through diverse scenarios. Our V2V4Real dataset covers a driving area of 410
km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding
boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real
introduces three perception tasks, including cooperative 3D object detection,
cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative
perception. We provide comprehensive benchmarks of recent cooperative
perception algorithms on three tasks. The V2V4Real dataset and codebase can be
found at https://github.com/ucla-mobility/V2V4Real.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023. Code link:
  https://github.com/ucla-mobility/V2V4Real</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdPE: Adversarial Positional Embeddings for <span class="highlight-title">Pretrain</span>ing Vision
  <span class="highlight-title">Transformer</span>s via MAE+ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Ying Wang, Ziwei Xuan, Guo-Jun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning of vision transformers seeks to pretrain an encoder via
pretext tasks without labels. Among them is the Masked Image Modeling (MIM)
aligned with pretraining of language transformers by predicting masked patches
as a pretext task. A criterion in unsupervised pretraining is the pretext task
needs to be sufficiently hard to prevent the transformer encoder from learning
trivial low-level features not generalizable well to downstream tasks. For this
purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It
distorts the local visual structures by perturbing the position encodings so
that the learned transformer cannot simply use the locally correlated patches
to predict the missing ones. We hypothesize that it forces the transformer
encoder to learn more discriminative features in a global context with stronger
generalizability to downstream tasks. We will consider both absolute and
relative positional encodings, where adversarial positions can be imposed both
in the embedding mode and the coordinate mode. We will also present a new MAE+
baseline that brings the performance of the MIM pretraining to a new level with
the AdPE. The experiments demonstrate that our approach can improve the
fine-tuning accuracy of MAE by $0.8\%$ and $0.4\%$ over 1600 epochs of
pretraining ViT-B and ViT-L on Imagenet1K. For the transfer learning task, it
outperforms the MAE with the ViT-B backbone by $2.6\%$ in mIoU on ADE20K, and
by $3.2\%$ in AP$^{bbox}$ and $1.6\%$ in AP$^{mask}$ on COCO, respectively.
These results are obtained with the AdPE being a pure MIM approach that does
not use any extra models or external datasets for pretraining. The code is
available at https://github.com/maple-research-lab/AdPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-Modulated Point Cloud Rendering with Easy Editing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Xiaoyang Huang, Bingbing Ni, Teng Li, Wenjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an effective point cloud rendering pipeline for novel view
synthesis, which enables high fidelity local detail reconstruction, real-time
rendering and user-friendly editing. In the heart of our pipeline is an
adaptive frequency modulation module called Adaptive Frequency Net (AFNet),
which utilizes a hypernetwork to learn the local texture frequency encoding
that is consecutively injected into adaptive frequency activation layers to
modulate the implicit radiance signal. This mechanism improves the frequency
expressive ability of the network with richer frequency basis support, only at
a small computational budget. To further boost performance, a preprocessing
module is also proposed for point cloud geometry optimization via point opacity
estimation. In contrast to implicit rendering, our pipeline supports
high-fidelity interactive editing based on point cloud manipulation. Extensive
experimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples
datasets demonstrate the superior performances achieved by our method in terms
of PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrated Teacher for Sparsely Annotated Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Wang, Liang Liu, Boshen Zhang, Jiangning Zhang, Wuhao Zhang, Zhenye Gan, Yabiao Wang, Chengjie Wang, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully supervised object detection requires training images in which all
instances are annotated. This is actually impractical due to the high labor and
time costs and the unavoidable missing annotations. As a result, the incomplete
annotation in each image could provide misleading supervision and harm the
training. Recent works on sparsely annotated object detection alleviate this
problem by generating pseudo labels for the missing annotations. Such a
mechanism is sensitive to the threshold of the pseudo label score. However, the
effective threshold is different in different training stages and among
different object detectors. Therefore, the current methods with fixed
thresholds have sub-optimal performance, and are difficult to be applied to
other detectors. In order to resolve this obstacle, we propose a Calibrated
Teacher, of which the confidence estimation of the prediction is well
calibrated to match its real precision. In this way, different detectors in
different training stages would share a similar distribution of the output
confidence, so that multiple detectors could share the same fixed threshold and
achieve better performance. Furthermore, we present a simple but effective
Focal IoU Weight (FIoU) for the classification loss. FIoU aims at reducing the
loss weight of false negative samples caused by the missing annotation, and
thus works as the complement of the teacher-student paradigm. Extensive
experiments show that our methods set new state-of-the-art under all different
sparse settings in COCO. Code will be available at
https://github.com/Whileherham/CalibratedTeacher.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensitive Region-based Metamorphic Testing Framework using Explainable
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuma Torikoshi, Yasuharu Nishi, Juichi Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) is one of the most popular research topics in machine
learning and DL-driven image recognition systems have developed rapidly. Recent
research has used metamorphic testing (MT) to detect misclassified images. Most
of them discuss metamorphic relations (MR), with little discussion on which
regions should be transformed. We focus on the fact that there are sensitive
regions where even a small transformation can easily change the prediction
results and propose an MT framework that efficiently tests for regions prone to
misclassification by transforming the sensitive regions. Our evaluation showed
that the sensitive regions can be specified by Explainable AI (XAI) and our
framework effectively detects faults.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Zhou, Yi Chang, Wending Yan, Luxin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow has achieved great success under clean scenes, but suffers from
restricted performance under foggy scenes. To bridge the clean-to-foggy domain
gap, the existing methods typically adopt the domain adaptation to transfer the
motion knowledge from clean to synthetic foggy domain. However, these methods
unexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous
when applied to real-world scenes. To handle the practical optical flow under
real foggy scenes, in this work, we propose a novel unsupervised cumulative
domain adaptation optical flow (UCDA-Flow) framework: depth-association motion
adaptation and correlation-alignment motion adaptation. Specifically, we
discover that depth is a key ingredient to influence the optical flow: the
deeper depth, the inferior optical flow, which motivates us to design a
depth-association motion adaptation module to bridge the clean-to-foggy domain
gap. Moreover, we figure out that the cost volume correlation shares similar
distribution of the synthetic and real foggy images, which enlightens us to
devise a correlation-alignment motion adaptation module to distill motion
knowledge of the synthetic foggy domain to the real foggy domain. Note that
synthetic fog is designed as the intermediate domain. Under this unified
framework, the proposed cumulative adaptation progressively transfers knowledge
from clean scenes to real foggy scenes. Extensive experiments have been
performed to verify the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Computer Vision Applications for Spatial AI Object
  Recognition in Orange County, California 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostas Alexandridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide an integrated and systematic automation approach to spatial object
recognition and positional detection using AI machine learning and computer
vision algorithms for Orange County, California. We describe a comprehensive
methodology for multi-sensor, high-resolution field data acquisition, along
with post-field processing and pre-analysis processing tasks. We developed a
series of algorithmic formulations and workflows that integrate convolutional
deep neural network learning with detected object positioning estimation in
360{\deg} equirectancular photosphere imagery. We provide examples of
application processing more than 800 thousand cardinal directions in
photosphere images across two areas in Orange County, and present detection
results for stop-sign and fire hydrant object recognition. We discuss the
efficiency and effectiveness of our approach, along with broader inferences
related to the performance and implications of this approach for future
technological innovations, including automation of spatial data and public
asset inventories, and near real-time AI field data systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 15 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HazardNet: Road Debris Detection by Augmentation of Synthetic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae Eun Choe, Jane Wu, Xiaolin Lin, Karen Kwon, Minwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm to detect unseen road debris using a small set of
synthetic models. Early detection of road debris is critical for safe
autonomous or assisted driving, yet the development of a robust road debris
detection model has not been widely discussed. There are two main challenges to
building a road debris detector: first, data collection of road debris is
challenging since hazardous objects on the road are rare to encounter in real
driving scenarios; second, the variability of road debris is broad, ranging
from a very small brick to a large fallen tree. To overcome these challenges,
we propose a novel approach to few-shot learning of road debris that uses
semantic augmentation and domain randomization to augment real road images with
synthetic models. We constrain the problem domain to uncommon objects on the
road and allow the deep neural network, HazardNet, to learn the semantic
meaning of road debris to eventually detect unseen road debris. Our results
demonstrate that HazardNet is able to accurately detect real road debris when
only trained on synthetic objects in augmented images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit and Explicit Commonsense for Multi-sentence Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Han Chou, James J. Little, Leonid Sigal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing dense or paragraph video captioning approaches rely on holistic
representations of videos, possibly coupled with learned object/action
representations, to condition hierarchical language decoders. However, they
fundamentally lack the commonsense knowledge of the world required to reason
about progression of events, causality, and even function of certain objects
within a scene. To address this limitation we propose a novel video captioning
Transformer-based model, that takes into account both implicit (visuo-lingual
and purely linguistic) and explicit (knowledge-base) commonsense knowledge. We
show that these forms of knowledge, in isolation and in combination, enhance
the quality of produced captions. Further, inspired by imitation learning, we
propose a new task of instruction generation, where the goal is to produce a
set of linguistic instructions from a video demonstration of its performance.
We formalize the task using ALFRED dataset [52] generated using an AI2-THOR
environment. While instruction generation is conceptually similar to paragraph
captioning, it differs in the fact that it exhibits stronger object
persistence, as well as spatially-aware and causal sentence structure. We show
that our commonsense knowledge enhanced approach produces significant
improvements on this task (up to 57% in METEOR and 8.5% in CIDEr), as well as
the state-of-the-art result on more traditional video captioning in the
ActivityNet Captions dataset [29].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WDiscOOD: Out-of-Distribution Detection via Whitened Linear
  Discriminative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Yunzhi Lin, Ruinian Xu, Patricio A. Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are susceptible to generating overconfident yet
erroneous predictions when presented with data beyond known concepts. This
challenge underscores the importance of detecting out-of-distribution (OOD)
samples in the open world. In this work, we propose a novel feature-space OOD
detection score that jointly reasons with both class-specific and
class-agnostic information. Specifically, our approach utilizes Whitened Linear
Discriminative Analysis to project features into two subspaces - the
discriminative and residual subspaces - in which the ID classes are maximally
separated and closely clustered, respectively. The OOD score is then determined
by combining the deviation from the input data to the ID distribution in both
subspaces. The efficacy of our method, named WDiscOOD, is verified on the
large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety
of distribution shifts. WDiscOOD demonstrates superior performance on deep
classifiers with diverse backbone architectures, including CNN and vision
transformer. Furthermore, we also show that our method can more effectively
detect novel concepts in representation space trained with contrastive
objectives, including supervised contrastive loss and multi-modality
contrastive loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial
  Wedge Pressure from Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun C. Tripathi, Mohammod N. I. Suvon, Lawrence Schobs, Shuo Zhou, Samer Alabed, Andrew J. Swift, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart failure is a serious and life-threatening condition that can lead to
elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure
(PAWP) is an important surrogate marker indicating high pressure in the left
ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an
invasive procedure. A non-invasive method is useful in quickly identifying
high-risk patients from a large population. In this work, we develop a tensor
learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic
Resonance Imaging (MRI). This pipeline extracts spatial and temporal features
from high-dimensional scans. For quality control, we incorporate an epistemic
uncertainty-based binning strategy to identify poor-quality training samples.
To improve the performance, we learn complementary information by integrating
features from multimodal data: cardiac MRI with short-axis and four-chamber
views, and Electronic Health Records. The experimental analysis on a large
cohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation
indicates that the proposed pipeline has a diagnostic value and can produce
promising performance with significant improvement over the baseline in
clinical practice (i.e., $\Delta$AUC $=0.10$, $\Delta$Accuracy $=0.06$, and
$\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical
utility of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Images Are Counterfactual Samples for Robust Fine-tuning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are challenged by the distribution shift between the
training data and test data. Recently, the large models pre-trained on diverse
data demonstrate unprecedented robustness to various distribution shifts.
However, fine-tuning on these models can lead to a trade-off between
in-distribution (ID) performance and out-of-distribution (OOD) robustness.
Existing methods for tackling this trade-off do not explicitly address the OOD
robustness problem. In this paper, based on causal analysis on the
aforementioned problems, we propose a novel fine-tuning method, which use
masked images as counterfactual samples that help improving the robustness of
the fine-tuning model. Specifically, we mask either the semantics-related or
semantics-unrelated patches of the images based on class activation map to
break the spurious correlation, and refill the masked patches with patches from
other images. The resulting counterfactual samples are used in feature-based
distillation with the pre-trained model. Extensive experiments verify that
regularizing the fine-tuning with the proposed masked images can achieve a
better trade-off between ID and OOD performance, surpassing previous methods on
the OOD performance. Our code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023 (v2: improve the clarity)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training set cleansing of backdoor poisoning by <span class="highlight-title">self-supervised</span>
  representation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Wang, S. Karami, O. Dia, H. Ritter, E. Emamjomeh-Zadeh, J. Chen, Z. Xiang, D. J. Miller, G. Kesidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A backdoor or Trojan attack is an important type of data poisoning attack
against deep neural network (DNN) classifiers, wherein the training dataset is
poisoned with a small number of samples that each possess the backdoor pattern
(usually a pattern that is either imperceptible or innocuous) and which are
mislabeled to the attacker's target class. When trained on a backdoor-poisoned
dataset, a DNN behaves normally on most benign test samples but makes incorrect
predictions to the target class when the test sample has the backdoor pattern
incorporated (i.e., contains a backdoor trigger). Here we focus on image
classification tasks and show that supervised training may build stronger
association between the backdoor pattern and the associated target class than
that between normal features and the true class of origin. By contrast,
self-supervised representation learning ignores the labels of samples and
learns a feature embedding based on images' semantic content. %We thus propose
to use unsupervised representation learning to avoid emphasising
backdoor-poisoned training samples and learn a similar feature embedding for
samples of the same class. Using a feature embedding found by self-supervised
representation learning, a data cleansing method, which combines sample
filtering and re-labeling, is developed. Experiments on CIFAR-10 benchmark
datasets show that our method achieves state-of-the-art performance in
mitigating backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Neural Networks Trained to Identify Words Provide a
  Surprisingly Good Account of Visual Form Priming Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Yin, Valerio Biscione, Jeffrey Bowers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A wide variety of orthographic coding schemes and models of visual word
identification have been developed to account for masked priming data that
provide a measure of orthographic similarity between letter strings. These
models tend to include hand-coded orthographic representations with single unit
coding for specific forms of knowledge (e.g., units coding for a letter in a
given position). Here we assess how well a range of these coding schemes and
models account for the pattern of form priming effects taken from the Form
Priming Project and compare these findings to results observed with 11 standard
deep neural network models (DNNs) developed in computer science. We find that
deep convolutional networks (CNNs) perform as well or better than the coding
schemes and word recognition models, whereas transformer networks did less
well. The success of CNNs is remarkable as their architectures were not
developed to support word recognition (they were designed to perform well on
object recognition), they classify pixel images of words (rather than
artificial encodings of letter strings), and their training was highly
simplified (not respecting many key aspects of human experience). In addition
to these form priming effects, we find that the DNNs can account for visual
similarity effects on priming that are beyond all current psychological models
of priming. The findings add to the recent work of (Hannagan et al., 2021) and
suggest that CNNs should be given more attention in psychology as models of
human visual word recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicted Embedding Power Regression for Large-Scale Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Yang, William Gebhardt, Alexander G. Ororbia, Travis Desell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) inputs can compromise the performance and safety of
real world machine learning systems. While many methods exist for OOD detection
and work well on small scale datasets with lower resolution and few classes,
few methods have been developed for large-scale OOD detection. Existing
large-scale methods generally depend on maximum classification probability,
such as the state-of-the-art grouped softmax method. In this work, we develop a
novel approach that calculates the probability of the predicted class label
based on label distributions learned during the training process. Our method
performs better than current state-of-the-art methods with only a negligible
increase in compute cost. We evaluate our method against contemporary methods
across $14$ datasets and achieve a statistically significant improvement with
respect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking
  Neural Networks with Learnable Neuronal Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumar Kosta, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras have recently shown great potential for high-speed motion
estimation owing to their ability to capture temporally rich information
asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired
event-driven processing can efficiently handle such asynchronous data, while
neuron models such as the leaky-integrate and fire (LIF) can keep track of the
quintessential timing information contained in the inputs. SNNs achieve this by
maintaining a dynamic state in the neuron memory, retaining important
information while forgetting redundant data over time. Thus, we posit that SNNs
would allow for better performance on sequential regression tasks compared to
similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult
to train due to vanishing spikes at later layers. To that effect, we propose an
adaptive fully-spiking framework with learnable neuronal dynamics to alleviate
the spike vanishing problem. We utilize surrogate gradient-based
backpropagation through time (BPTT) to train our deep SNNs from scratch. We
validate our approach for the task of optical flow estimation on the
Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset.
Our experiments on these datasets show an average reduction of 13% in average
endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several
down-scaled models and observe that our SNN models consistently outperform
similarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the
importance of SNNs for smaller models and their suitability at the edge. In
terms of efficiency, our SNNs offer substantial savings in network parameters
(48.3x) and computational energy (10.2x) while attaining ~10% lower EPE
compared to the state-of-the-art ANN implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian <span class="highlight-title">Prompt</span> Learning for Image-Language Model Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi da Costa, Cees G. M. Snoek, Georgios Tzimiropoulos, Brais Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational image-language models have generated considerable interest due
to their efficient adaptation to downstream tasks by prompt learning. Prompt
learning treats part of the language model input as trainable while freezing
the rest, and optimizes an Empirical Risk Minimization objective. However,
Empirical Risk Minimization is known to suffer from distributional shifts which
hurt generalizability to prompts unseen during training. By leveraging the
regularization ability of Bayesian methods, we frame prompt learning from the
Bayesian perspective and formulate it as a variational inference problem. Our
approach regularizes the prompt space, reduces overfitting to the seen prompts
and improves the prompt generalization on unseen prompts. Our framework is
implemented by modeling the input prompt space in a probabilistic manner, as an
a priori distribution which makes our proposal compatible with prompt learning
approaches that are unconditional or conditional on the image. We demonstrate
empirically on 15 benchmarks that Bayesian prompt learning provides an
appropriate coverage of the prompt space, prevents learning spurious features,
and exploits transferable invariant features. This results in better
generalization of unseen prompts, even across different datasets and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Performance of Object Detection using the Mechanisms of Visual
  Recognition in Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Ghasemi, Nasrin Bayat, Fatemeh Mottaghian, Akram Bayat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object recognition systems are usually trained and evaluated on high
resolution images. However, in real world applications, it is common that the
images have low resolutions or have small sizes. In this study, we first track
the performance of the state-of-the-art deep object recognition network,
Faster- RCNN, as a function of image resolution. The results reveals negative
effects of low resolution images on recognition performance. They also show
that different spatial frequencies convey different information about the
objects in recognition process. It means multi-resolution recognition system
can provides better insight into optimal selection of features that results in
better recognition of objects. This is similar to the mechanisms of the human
visual systems that are able to implement multi-scale representation of a
visual scene simultaneously. Then, we propose a multi-resolution object
recognition framework rather than a single-resolution network. The proposed
framework is evaluated on the PASCAL VOC2007 database. The experimental results
show the performance of our adapted multi-resolution Faster-RCNN framework
outperforms the single-resolution Faster-RCNN on input images with various
resolutions with an increase in the mean Average Precision (mAP) of 9.14%
across all resolutions and 1.2% on the full-spectrum images. Furthermore, the
proposed model yields robustness of the performance over a wide range of
spatial frequencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Recognition with Deep Nearest Centroids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenguan Wang, Cheng Han, Tianfei Zhou, Dongfang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We devise deep nearest centroids (DNC), a conceptually elegant yet
surprisingly effective network for large-scale visual recognition, by
revisiting Nearest Centroids, one of the most classic and simple classifiers.
Current deep models learn the classifier in a fully parametric manner, ignoring
the latent data structure and lacking simplicity and explainability. DNC
instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids
of training samples to describe class distributions and clearly explains the
classification as the proximity of test data and the class sub-centroids in the
feature space. Due to the distance-based nature, the network output
dimensionality is flexible, and all the learnable parameters are only for data
embedding. That means all the knowledge learnt for ImageNet classification can
be completely transferred for pixel recognition learning, under the
"pre-training and fine-tuning" paradigm. Apart from its nested simplicity and
intuitive decision-making mechanism, DNC can even possess ad-hoc explainability
when the sub-centroids are selected as actual training images that humans can
view and inspect. Compared with parametric counterparts, DNC performs better on
image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition
(ADE20K, Cityscapes), with improved transparency and fewer learnable
parameters, using various network architectures (ResNet, Swin) and segmentation
models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights
into related fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuS-X: Training-Free Name-Only Transfer of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishaal Udandarao, Ankush Gupta, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free "name-only transfer" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Softmax Information for Selective Classification with
  Out-of-Distribution Data <span class="chip">ACCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a task that is receiving an
increasing amount of research attention in the domain of deep learning for
computer vision. However, the performance of detection methods is generally
evaluated on the task in isolation, rather than also considering potential
downstream tasks in tandem. In this work, we examine selective classification
in the presence of OOD data (SCOD). That is to say, the motivation for
detecting OOD samples is to reject them so their impact on the quality of
predictions is reduced. We show under this task specification, that existing
post-hoc methods perform quite differently compared to when evaluated only on
OOD detection. This is because it is no longer an issue to conflate
in-distribution (ID) data with OOD data if the ID data is going to be
misclassified. However, the conflation within ID data of correct and incorrect
predictions becomes undesirable. We also propose a novel method for SCOD,
Softmax Information Retaining Combination (SIRC), that augments softmax-based
confidence scores with feature-agnostic information such that their ability to
identify OOD samples is improved without sacrificing separation between correct
and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale
datasets and convolutional neural network architectures show that SIRC is able
to consistently match or outperform the baseline for SCOD, whilst existing OOD
detection methods fail to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2022 (Best Paper Award)
  https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position and Rotation Invariant Sign Language Recognition from 3D Kinect
  Data with Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.12669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.12669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun Roy, Saumik Bhattacharya, Partha Pratim Roy, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language is a gesture-based symbolic communication medium among speech
and hearing impaired people. It also serves as a communication bridge between
non-impaired and impaired populations. Unfortunately, in most situations, a
non-impaired person is not well conversant in such symbolic languages
restricting the natural information flow between these two categories.
Therefore, an automated translation mechanism that seamlessly translates sign
language into natural language can be highly advantageous. In this paper, we
attempt to perform recognition of 30 basic Indian sign gestures. Gestures are
represented as temporal sequences of 3D maps (RGB + depth), each consisting of
3D coordinates of 20 body joints captured by the Kinect sensor. A recurrent
neural network (RNN) is employed as the classifier. To improve the classifier's
performance, we use geometric transformation for the alignment correction of
depth frames. In our experiments, the model achieves 84.81% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Entropy as an Inductive Bias for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Peruzzo, Enver Sangineto, Yahui Liu, Marco De Nadai, Wei Bi, Bruno Lepri, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on Vision Transformers (VTs) showed that introducing a local
inductive bias in the VT architecture helps reducing the number of samples
necessary for training. However, the architecture modifications lead to a loss
of generality of the Transformer backbone, partially contradicting the push
towards the development of uniform architectures, shared, e.g., by both the
Computer Vision and the Natural Language Processing areas. In this work, we
propose a different and complementary direction, in which a local bias is
introduced using an auxiliary self-supervised task, performed jointly with
standard supervised training. Specifically, we exploit the observation that the
attention maps of VTs, when trained with self-supervision, can contain a
semantic segmentation structure which does not spontaneously emerge when
training is supervised. Thus, we explicitly encourage the emergence of this
spatial clustering as a form of training regularization. In more detail, we
exploit the assumption that, in a given image, objects usually correspond to
few connected regions, and we propose a spatial formulation of the information
entropy to quantify this object-based inductive bias. By minimizing the
proposed spatial entropy, we include an additional self-supervised signal
during training. Using extensive experiments, we show that the proposed
regularization leads to equivalent or better results than other VT proposals
which include a local bias by changing the basic Transformer architecture, and
it can drastically boost the VT final accuracy when using small-medium training
sets. The code is available at https://github.com/helia95/SAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth image-to-image translations with latent space interpolations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00841v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00841v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Marco De Nadai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-domain image-to-image (I2I) translations can transform a source image
according to the style of a target domain. One important, desired
characteristic of these transformations, is their graduality, which corresponds
to a smooth change between the source and the target image when their
respective latent-space representations are linearly interpolated. However,
state-of-the-art methods usually perform poorly when evaluated using
inter-domain interpolations, often producing abrupt changes in the appearance
or non-realistic intermediate images. In this paper, we argue that one of the
main reasons behind this problem is the lack of sufficient inter-domain
training data and we propose two different regularization methods to alleviate
this issue: a new shrinkage loss, which compacts the latent space, and a Mixup
data-augmentation strategy, which flattens the style representations between
domains. We also propose a new metric to quantitatively evaluate the degree of
the interpolation smoothness, an aspect which is not sufficiently covered by
the existing I2I translation metrics. Using both our proposed metric and
standard evaluation protocols, we show that our regularization techniques can
improve the state-of-the-art multi-domain I2I translations by a large margin.
Our code will be made publicly available upon the acceptance of this article.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoNIC Challenge: Pushing the Frontiers of Nuclear Detection,
  Segmentation, Classification and Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Graham, Quoc Dang Vu, Mostafa Jahanifar, Martin Weigert, Uwe Schmidt, Wenhua Zhang, Jun Zhang, Sen Yang, Jinxi Xiang, Xiyue Wang, Josef Lorenz Rumberger, Elias Baumann, Peter Hirsch, Lihao Liu, Chenyang Hong, Angelica I. Aviles-Rivero, Ayushi Jain, Heeyoung Ahn, Yiyu Hong, Hussam Azzuni, Min Xu, Mohammad Yaqub, Marie-Claire Blache, Benoît Piégu, Bertrand Vernay, Tim Scherr, Moritz Böhland, Katharina Löffler, Jiachen Li, Weiqin Ying, Chixin Wang, Dagmar Kainmueller, Carola-Bibiane Schönlieb, Shuolin Liu, Dhairya Talsania, Yughender Meda, Prakash Mishra, Muhammad Ridzuan, Oliver Neumann, Marcel P. Schilling, Markus Reischl, Ralf Mikut, Banban Huang, Hsiang-Chin Chien, Ching-Ping Wang, Chia-Yen Lee, Hong-Kun Lin, Zaiyi Liu, Xipeng Pan, Chu Han, Jijun Cheng, Muhammad Dawood, Srijay Deshpande, Raja Muhammad Saad Bashir, Adam Shephard, Pedro Costa, João D. Nunes, Aurélio Campilho, Jaime S. Cardoso, Hrishikesh P S, Densen Puthussery, Devika R G, Jiji C V, Ye Zhang, Zijie Fang, Zhifan Lin, Yongbing Zhang, Chunhui Lin, Liukun Zhang, Lijian Mao, Min Wu, Vi Thi-Tuong Vo, Soo-Hyung Kim, Taebum Lee, Satoshi Kondo, Satoshi Kasai, Pranay Dumbhare, Vedant Phuse, Yash Dubey, Ankush Jamthikar, Trinh Thi Le Vuong, Jin Tae Kwak, Dorsa Ziaei, Hyun Jung, Tianyi Miao, David Snead, Shan E Ahmed Raza, Fayyaz Minhas, Nasir M. Rajpoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclear detection, segmentation and morphometric profiling are essential in
helping us further understand the relationship between histology and patient
outcome. To drive innovation in this area, we setup a community-wide challenge
using the largest available dataset of its kind to assess nuclear segmentation
and cellular composition. Our challenge, named CoNIC, stimulated the
development of reproducible algorithms for cellular recognition with real-time
result inspection on public leaderboards. We conducted an extensive
post-challenge analysis based on the top-performing models using 1,658
whole-slide images of colon tissue. With around 700 million detected nuclei per
model, associated features were used for dysplasia grading and survival
analysis, where we demonstrated that the challenge's improvement over the
previous state-of-the-art led to significant boosts in downstream performance.
Our findings also suggest that eosinophils and neutrophils play an important
role in the tumour microevironment. We release challenge models and WSI-level
results to foster the development of further methods for biomarker discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CapDet: Unifying Dense Captioning and Open-World Detection <span class="highlight-title">Pretrain</span>ing <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from large-scale vision-language pre-training on image-text pairs,
open-world detection methods have shown superior generalization ability under
the zero-shot or few-shot detection settings. However, a pre-defined category
space is still required during the inference stage of existing methods and only
the objects belonging to that space will be predicted. To introduce a "real"
open-world detector, in this paper, we propose a novel method named CapDet to
either predict under a given category list or directly generate the category of
predicted bounding boxes. Specifically, we unify the open-world detection and
dense caption tasks into a single yet effective framework by introducing an
additional dense captioning head to generate the region-grounded captions.
Besides, adding the captioning task will in turn benefit the generalization of
detection performance since the captioning dataset covers more concepts.
Experiment results show that by unifying the dense caption task, our CapDet has
obtained significant performance improvements (e.g., +2.1% mAP on LVIS rare
classes) over the baseline method on LVIS (1203 classes). Besides, our CapDet
also achieves state-of-the-art performance on dense captioning tasks, e.g.,
15.44% mAP on VG V1.2 and 13.98% on the VG-COCO dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global
  Cross-Modal Fusion <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yuchen Yang, Youquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-camera fusion methods have shown impressive performance in 3D object
detection. Recent advanced multi-modal methods mainly perform global fusion,
where image features and point cloud features are fused across the whole scene.
Such practice lacks fine-grained region-level information, yielding suboptimal
fusion performance. In this paper, we present the novel Local-to-Global fusion
network (LoGoNet), which performs LiDAR-camera fusion at both local and global
levels. Concretely, the Global Fusion (GoF) of LoGoNet is built upon previous
literature, while we exclusively use point centroids to more precisely
represent the position of voxel features, thus achieving better cross-modal
alignment. As to the Local Fusion (LoF), we first divide each proposal into
uniform grids and then project these grid centers to the images. The image
features around the projected grid points are sampled to be fused with
position-decorated point cloud features, maximally utilizing the rich
contextual information around the proposals. The Feature Dynamic Aggregation
(FDA) module is further proposed to achieve information interaction between
these locally and globally fused features, thus producing more informative
multi-modal features. Extensive experiments on both Waymo Open Dataset (WOD)
and KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D
detection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection
leaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy
that, for the first time, the detection performance on three classes surpasses
80 APH (L2) simultaneously. Code will be available at
\url{https://github.com/sankin97/LoGoNet}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided
  Distance Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a learning-based method, namely GeoUDF,to tackle the long-standing
and challenging problem of reconstructing a discrete surface from a sparse
point cloud.To be specific, we propose a geometry-guided learning method for
UDF and its gradient estimation that explicitly formulates the unsigned
distance of a query point as the learnable affine averaging of its distances to
the tangent planes of neighboring points on the surface. Besides,we model the
local geometric structure of the input point clouds by explicitly learning a
quadratic polynomial for each point. This not only facilitates upsampling the
input sparse point cloud but also naturally induces unoriented normal, which
further augments UDF estimation. Finally, to extract triangle meshes from the
predicted UDF we propose a customized edge-based marching cube module. We
conduct extensive experiments and ablation studies to demonstrate the
significant advantages of our method over state-of-the-art methods in terms of
reconstruction accuracy, efficiency, and generality. The source code is
publicly available at https://github.com/rsy6318/GeoUDF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Discriminative Representations for Skeleton Based Action
  Recognition <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Zhou, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human action recognition aims at classifying the category of human action
from a segment of a video. Recently, people have dived into designing GCN-based
models to extract features from skeletons for performing this task, because
skeleton representations are much more efficient and robust than other
modalities such as RGB frames. However, when employing the skeleton data, some
important clues like related items are also discarded. It results in some
ambiguous actions that are hard to be distinguished and tend to be
misclassified. To alleviate this problem, we propose an auxiliary feature
refinement head (FR Head), which consists of spatial-temporal decoupling and
contrastive feature refinement, to obtain discriminative representations of
skeletons. Ambiguous samples are dynamically discovered and calibrated in the
feature space. Furthermore, FR Head could be imposed on different stages of
GCNs to build a multi-level refinement for stronger supervision. Extensive
experiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.
Our proposed models obtain competitive results from state-of-the-art methods
and can help to discriminate those ambiguous samples. Codes are available at
https://github.com/zhysora/FR-Head.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023. 10 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D <span class="highlight-title">GAN</span> Inversion with Facial Symmetry Prior <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Yin, Yong Zhang, Xuan Wang, Tengfei Wang, Xiaoyu Li, Yuan Gong, Yanbo Fan, Xiaodong Cun, Ying Shan, Cengiz Oztireli, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a surge of high-quality 3D-aware GANs have been proposed, which
leverage the generative power of neural rendering. It is natural to associate
3D GANs with GAN inversion methods to project a real image into the generator's
latent space, allowing free-view consistent synthesis and editing, referred as
3D GAN inversion. Although with the facial prior preserved in pre-trained 3D
GANs, reconstructing a 3D portrait with only one monocular image is still an
ill-pose problem. The straightforward application of 2D GAN inversion methods
focuses on texture similarity only while ignoring the correctness of 3D
geometry shapes. It may raise geometry collapse effects, especially when
reconstructing a side face under an extreme pose. Besides, the synthetic
results in novel views are prone to be blurry. In this work, we propose a novel
method to promote 3D GAN inversion by introducing facial symmetry prior. We
design a pipeline and constraints to make full use of the pseudo auxiliary view
obtained via image flipping, which helps obtain a robust and reasonable
geometry shape during the inversion process. To enhance texture fidelity in
unobserved viewpoints, pseudo labels from depth-guided 3D warping can provide
extra supervision. We design constraints aimed at filtering out conflict areas
for optimization in asymmetric situations. Comprehensive quantitative and
qualitative evaluations on image reconstruction and editing demonstrate the
superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page is at https://feiiyin.github.io/SPI/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blind2Sound: <span class="highlight-title">Self-Supervised</span> Image Denoising without Residual Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zejin Wang, Jiazheng Liu, Hao Zhai, Hua Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised blind denoising for Poisson-Gaussian noise remains a
challenging task. Pseudo-supervised pairs constructed from single noisy images
re-corrupt the signal and degrade the performance. The visible blindspots solve
the information loss in masked inputs. However, without explicitly noise
sensing, mean square error as an objective function cannot adjust denoising
intensities for dynamic noise levels, leading to noticeable residual noise. In
this paper, we propose Blind2Sound, a simple yet effective approach to overcome
residual noise in denoised images. The proposed adaptive re-visible loss senses
noise levels and performs personalized denoising without noise residues while
retaining the signal lossless. The theoretical analysis of intermediate medium
gradients guarantees stable training, while the Cramer Gaussian loss acts as a
regularization to facilitate the accurate perception of noise levels and
improve the performance of the denoiser. Experiments on synthetic and
real-world datasets show the superior performance of our method, especially for
single-channel images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CECT: Controllable Ensemble CNN and <span class="highlight-title">Transformer</span> for COVID-19 Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoshan Liu, Lei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most computer vision models are developed based on either convolutional
neural network (CNN) or transformer, while the former (latter) method captures
local (global) features. To relieve model performance limitations due to the
lack of global (local) features, we develop a novel classification network CECT
by controllable ensemble CNN and transformer. CECT is composed of a
convolutional encoder block, a transposed-convolutional decoder block, and a
transformer classification block. Different from conventional CNN- or
transformer-based methods, our CECT can capture features at both multi-local
and global scales. Besides, the contribution of local features at different
scales can be controlled with the proposed ensemble coefficients. We evaluate
CECT on two public COVID-19 datasets and it outperforms existing
state-of-the-art methods on all evaluation metrics. With remarkable feature
capture ability, we believe CECT can be extended to other medical image
classification scenarios as a diagnosis assistant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Hamdi, Bernard Ghanem, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel
view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels
for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage
machine learning and adoption of SRFs as a 3D representation, we present SPARF,
a large-scale ShapeNet-based synthetic dataset for novel view synthesis
consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at
high resolution (400 X 400 pixels). The dataset is orders of magnitude larger
than existing synthetic datasets for novel view synthesis and includes more
than one million 3D-optimized radiance fields with multiple voxel resolutions.
Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate
sparse voxel radiance fields from only few views. This is done by using the
densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs
partial SRFs from few/one images and a specialized SRF loss to learn to
generate high-quality sparse voxel radiance fields that can be rendered from
novel views. Our approach achieves state-of-the-art results in the task of
unconstrained novel view synthesis based on few views on ShapeNet as compared
to recent baselines. The SPARF dataset will be made public with the code and
models on the project website https://abdullahamdi.com/sparf/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating <span class="highlight-title">Diffusion</span> Sampling with Classifier-based Feature
  Distillation <span class="chip">ICME 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujie Sun, Defang Chen, Can Wang, Deshi Ye, Yan Feng, Chun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although diffusion model has shown great potential for generating higher
quality images than GANs, slow sampling speed hinders its wide application in
practice. Progressive distillation is thus proposed for fast sampling by
progressively aligning output images of $N$-step teacher sampler with
$N/2$-step student sampler. In this paper, we argue that this
distillation-based accelerating method can be further improved, especially for
few-step samplers, with our proposed \textbf{C}lassifier-based \textbf{F}eature
\textbf{D}istillation (CFD). Instead of aligning output images, we distill
teacher's sharpened feature distribution into the student with a
dataset-independent classifier, making the student focus on those important
features to improve performance. We also introduce a dataset-oriented loss to
further optimize the model. Experiments on CIFAR-10 show the superiority of our
method in achieving high quality and fast sampling. Code is provided at
\url{https://github.com/zju-SWJ/RCFD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Class Learning using Variational Autoencoders with
  Similarity Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.01303v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.01303v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huo, Terence L. van Zyl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting in neural networks during incremental learning
remains a challenging problem. Previous research investigated catastrophic
forgetting in fully connected networks, with some earlier work exploring
activation functions and learning algorithms. Applications of neural networks
have been extended to include similarity learning. Understanding how similarity
learning loss functions would be affected by catastrophic forgetting is of
significant interest. Our research investigates catastrophic forgetting for
four well-known similarity-based loss functions during incremental class
learning. The loss functions are Angular, Contrastive, Center, and Triplet
loss. Our results show that the catastrophic forgetting rate differs across
loss functions on multiple datasets. The Angular loss was least affected,
followed by Contrastive, Triplet loss, and Center loss with good mining
techniques. We implemented three existing incremental learning techniques,
iCaRL, EWC, and EBLL. We further proposed a novel technique using Variational
Autoencoders (VAEs) to generate representation as exemplars passed through the
network's intermediate layers. Our method outperformed three existing
state-of-the-art techniques. We show that one does not require stored images
(exemplars) for incremental learning with similarity learning. The generated
representations from VAEs help preserve regions of the embedding space used by
prior knowledge so that new knowledge does not ``overwrite'' it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAT: LoCalization and IdentificAtion Cascade Detection <span class="highlight-title">Transformer</span> for
  Open-World Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01970v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01970v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuailei Ma, Yuefeng Wang, Jiaqi Fan, Ying Wei, Thomas H. Li, Hongli Liu, Fanbing Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-world object detection (OWOD), as a more general and challenging goal,
requires the model trained from data on known objects to detect both known and
unknown objects and incrementally learn to identify these unknown objects. The
existing works which employ standard detection framework and fixed
pseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion
of detecting unknown objects substantially reduces the model's ability to
detect known ones. (ii) The PLM does not adequately utilize the priori
knowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee
that the model is trained in the right direction. We observe that humans
subconsciously prefer to focus on all foreground objects and then identify each
one in detail, rather than localize and identify a single object
simultaneously, for alleviating the confusion. This motivates us to propose a
novel solution called CAT: LoCalization and IdentificAtion Cascade Detection
Transformer which decouples the detection process via the shared decoder in the
cascade decoding way. In the meanwhile, we propose the self-adaptive
pseudo-labelling mechanism which combines the model-driven with input-driven
PLM and self-adaptively generates robust pseudo-labels for unknown objects,
significantly improving the ability of CAT to retrieve unknown objects.
Comprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL
VOC, show that our model outperforms the state-of-the-art in terms of all
metrics in the task of OWOD, incremental object detection (IOD) and open-set
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Region Normalization for Image Inpainting <span class="chip">AAAI-2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.10375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.10375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yu, Zongyu Guo, Xin Jin, Shilin Wu, Zhibo Chen, Weiping Li, Zhizheng Zhang, Sen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature Normalization (FN) is an important technique to help neural network
training, which typically normalizes features across spatial dimensions. Most
previous image inpainting methods apply FN in their networks without
considering the impact of the corrupted regions of the input image on
normalization, e.g. mean and variance shifts. In this work, we show that the
mean and variance shifts caused by full-spatial FN limit the image inpainting
network training and we propose a spatial region-wise normalization named
Region Normalization (RN) to overcome the limitation. RN divides spatial pixels
into different regions according to the input mask, and computes the mean and
variance in each region for normalization. We develop two kinds of RN for our
image inpainting network: (1) Basic RN (RN-B), which normalizes pixels from the
corrupted and uncorrupted regions separately based on the original inpainting
mask to solve the mean and variance shift problem; (2) Learnable RN (RN-L),
which automatically detects potentially corrupted and uncorrupted regions for
separate normalization, and performs global affine transformation to enhance
their fusion. We apply RN-B in the early layers and RN-L in the latter layers
of the network respectively. Experiments show that our method outperforms
current state-of-the-art methods quantitatively and qualitatively. We further
generalize RN to other inpainting networks and achieve consistent performance
improvements. Our code is available at https://github.com/geekyutao/RN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-2020. Code URL:https://github.com/geekyutao/RN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Next Boundary Detection for Instance Segmentation of Tree
  Rings in Microscopy Images of Shrub Cross Sections <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Gillert, Giulia Resente, Alba Anadon-Rosell, Martin Wilmking, Uwe Freiherr von Lukas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of detecting tree rings in microscopy images of shrub
cross sections. This can be regarded as a special case of the instance
segmentation task with several unique challenges such as the concentric
circular ring shape of the objects and high precision requirements that result
in inadequate performance of existing methods. We propose a new iterative
method which we term Iterative Next Boundary Detection (INBD). It intuitively
models the natural growth direction, starting from the center of the shrub
cross section and detecting the next ring boundary in each iteration step. In
our experiments, INBD shows superior performance to generic instance
segmentation methods and is the only one with a built-in notion of
chronological order. Our dataset and source code are available at
http://github.com/alexander-g/INBD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TexPose: Neural Texture Learning for <span class="highlight-title">Self-Supervised</span> 6D Object Pose
  Estimation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhi Chen, Fabian Manhardt, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce neural texture learning for 6D object pose
estimation from synthetic data and a few unlabelled real images. Our major
contribution is a novel learning scheme which removes the drawbacks of previous
works, namely the strong dependency on co-modalities or additional refinement.
These have been previously necessary to provide training signals for
convergence. We formulate such a scheme as two sub-optimisation problems on
texture learning and pose learning. We separately learn to predict realistic
texture of objects from real image collections and learn pose estimation from
pixel-perfect synthetic data. Combining these two capabilities allows then to
synthesise photorealistic novel views to supervise the pose estimator with
accurate geometry. To alleviate pose noise and segmentation imperfection
present during the texture learning phase, we propose a surfel-based
adversarial training loss together with texture regularisation from synthetic
data. We demonstrate that the proposed approach significantly outperforms the
recent state-of-the-art methods without ground-truth pose annotations and
demonstrates substantial generalisation improvements towards unseen scenes.
Remarkably, our scheme improves the adopted pose estimators substantially even
when initialised with much inferior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thought Flow Nets: From Single Predictions to Trains of Model Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.12220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.12220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Schuff, Heike Adel, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When humans solve complex problems, they typically create a sequence of ideas
(involving an intuitive decision, reflection, error correction, etc.) in order
to reach a conclusive decision. Contrary to this, today's models are mostly
trained to map an input to one single and fixed output. In this paper, we
investigate how we can give models the opportunity of a second, third and
$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the
concept of a thought flow which creates a sequence of predictions. We present a
self-correction mechanism that is trained to estimate the model's correctness
and performs iterative prediction updates based on the correctness prediction's
gradient. We introduce our method at the example of question answering and
conduct extensive experiments that demonstrate (i) our method's ability to
correct its own predictions and (ii) its potential to notably improve model
performances. In addition, we conduct a qualitative analysis of thought flow
correction patterns and explore how thought flow predictions affect human users
within a crowdsourcing study. We find that (iii) thought flows enable improved
user performance and are perceived as more natural, correct, and intelligent as
single and/or top-3 predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Physics-Based Object Pose Tracking during Non-Prehensile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zisong Xu, Rafael Papallas, Mehmet Dogar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to track the 6D pose of an object over time, while the
object is under non-prehensile manipulation by a robot. At any given time
during the manipulation of the object, we assume access to the robot joint
controls and an image from a camera. We use the robot joint controls to perform
a physics-based prediction of how the object might be moving. We then combine
this prediction with the observation coming from the camera, to estimate the
object pose as accurately as possible. We use a particle filtering approach to
combine the control information with the visual information. We compare the
proposed method with two baselines: (i) using only an image-based pose
estimation system at each time-step, and (ii) a particle filter which does not
perform the computationally expensive physics predictions, but assumes the
object moves with constant velocity. Our results show that making physics-based
predictions is worth the computational cost, resulting in more accurate
tracking, and estimating object pose even when the object is not clearly
visible to the camera.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ed ViTs Yield Versatile Representations For Medical Images <span class="chip">ICCV
  2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Matsoukas, Johan Fredin Haslum, Magnus Söderberg, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have reigned for a decade as the de
facto approach to automated medical image diagnosis, pushing the
state-of-the-art in classification, detection and segmentation tasks. Over the
last years, vision transformers (ViTs) have appeared as a competitive
alternative to CNNs, yielding impressive levels of performance in the natural
image domain, while possessing several interesting properties that could prove
beneficial for medical imaging tasks. In this work, we explore the benefits and
drawbacks of transformer-based models for medical image classification. We
conduct a series of experiments on several standard 2D medical image benchmark
datasets and tasks. Our findings show that, while CNNs perform better if
trained from scratch, off-the-shelf vision transformers can perform on par with
CNNs when pretrained on ImageNet, both in a supervised and self-supervised
setting, rendering them as a viable alternative to CNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of arXiv:2108.09038 originally published at the ICCV
  2021 Workshop on Computer Vision for Automated Medical Diagnosis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-aligned supervision for Real Image Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Fan, Fei Guo, Jianjun Qian, Xiang Li, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing haze from real-world images is challenging due to unpredictable
weather conditions, resulting in misaligned hazy and clear image pairs. In this
paper, we propose a non-aligned supervision framework that consists of three
networks - dehazing, airlight, and transmission. In particular, we explore a
non-alignment setting by utilizing a clear reference image that is not aligned
with the hazy input image to supervise the dehazing network through a
multi-scale reference loss that compares the features of the two images. Our
setting makes it easier to collect hazy/clear image pairs in real-world
environments, even under conditions of misalignment and shift views. To
demonstrate this, we have created a new hazy dataset called "Phone-Hazy", which
was captured using mobile phones in both rural and urban areas. Additionally,
we present a mean and variance self-attention network to model the infinite
airlight using dark channel prior as position guidance, and employ a channel
attention network to estimate the three-channel transmission. Experimental
results show that our framework outperforms current state-of-the-art methods in
the real-world image dehazing. Phone-Hazy and code will be available at
https://github.com/hello2377/NSDNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified BEV Model for Joint Learning of 3D Local Features and Overlap
  Estimation <span class="chip">ICRA-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Li, Wendong Ding, Yongkun Wen, Yufei Liang, Yong Liu, Guowei Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pairwise point cloud registration is a critical task for many applications,
which heavily depends on finding correct correspondences from the two point
clouds. However, the low overlap between input point clouds causes the
registration to fail easily, leading to mistaken overlapping and mismatched
correspondences, especially in scenes where non-overlapping regions contain
similar structures. In this paper, we present a unified bird's-eye view (BEV)
model for jointly learning of 3D local features and overlap estimation to
fulfill pairwise registration and loop closure. Feature description is
performed by a sparse UNet-like network based on BEV representation, and 3D
keypoints are extracted by a detection head for 2D locations, and a regression
head for heights. For overlap detection, a cross-attention module is applied
for interacting contextual information of input point clouds, followed by a
classification head to estimate the overlapping region. We evaluate our unified
model extensively on the KITTI dataset and Apollo-SouthBay dataset. The
experiments demonstrate that our method significantly outperforms existing
methods on overlap estimation, especially in scenes with small overlaps. It
also achieves top registration performance on both datasets in terms of
translation and rotation errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted by ICRA-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image
  Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Schrader, Tobias Alt, Joachim Weickert, Michael Ertel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Euler's elastica constitute an appealing variational image inpainting model.
It minimises an energy that involves the total variation as well as the level
line curvature. These components are transparent and make it attractive for
shape completion tasks. However, its gradient flow is a singular, anisotropic,
and nonlinear PDE of fourth order, which is numerically challenging: It is
difficult to find efficient algorithms that offer sharp edges and good rotation
invariance. As a remedy, we design the first neural algorithm that simulates
inpainting with Euler's Elastica. We use the deep energy concept which employs
the variational energy as neural network loss. Furthermore, we pair it with a
deep image prior where the network architecture itself acts as a prior. This
yields better inpaintings by steering the optimisation trajectory closer to the
desired solution. Our results are qualitatively on par with state-of-the-art
algorithms on elastica-based shape completion. They combine good rotation
invariance with sharp edges. Moreover, we benefit from the high efficiency and
effortless parallelisation within a neural framework. Our neural elastica
approach only requires 3x3 central difference stencils. It is thus much simpler
than other well-performing algorithms for elastica inpainting. Last but not
least, it is unsupervised as it requires no ground truth training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 10th European Workshop on Visual Information
  Processing, Lisbon, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Diffusion</span> Model Predicts 3D Shapes from 2D Microscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik J. E. Waibel, Ernst Röell, Bastian Rieck, Raja Giryes, Carsten Marr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a special type of generative model, capable of
synthesising new data from a learnt distribution. We introduce DISPR, a
diffusion-based model for solving the inverse problem of three-dimensional (3D)
cell shape prediction from two-dimensional (2D) single cell microscopy images.
Using the 2D microscopy image as a prior, DISPR is conditioned to predict
realistic 3D shape reconstructions. To showcase the applicability of DISPR as a
data augmentation tool in a feature-based single cell classification task, we
extract morphological features from the red blood cells grouped into six highly
imbalanced classes. Adding features from the DISPR predictions to the three
minority classes improved the macro F1 score from $F1_\text{macro} = 55.2 \pm
4.6\%$ to $F1_\text{macro} = 72.2 \pm 4.9\%$. We thus demonstrate that
diffusion models can be successfully applied to inverse biomedical problems,
and that they learn to reconstruct 3D shapes with realistic morphological
features from 2D microscopy images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Geometry Encoding Volume for Stereo Matching <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangwei Xu, Xianqi Wang, Xiaohuan Ding, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in
matching tasks. However, all-pairs correlations lack non-local geometry
knowledge and have difficulties tackling local ambiguities in ill-posed
regions. In this paper, we propose Iterative Geometry Encoding Volume
(IGEV-Stereo), a new deep network architecture for stereo matching. The
proposed IGEV-Stereo builds a combined geometry encoding volume that encodes
geometry and context information as well as local matching details, and
iteratively indexes it to update the disparity map. To speed up the
convergence, we exploit GEV to regress an accurate starting point for ConvGRUs
iterations. Our IGEV-Stereo ranks $1^{st}$ on KITTI 2015 and 2012 (Reflective)
among all published methods and is the fastest among the top 10 methods. In
addition, IGEV-Stereo has strong cross-dataset generalization as well as high
inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e.
IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is
available at https://github.com/gangweiX/IGEV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't PANIC: Prototypical Additive Neural Network for Interpretable
  Classification of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Nuno Wolf, Sebastian Pölsterl, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) has a complex and multifactorial etiology, which
requires integrating information about neuroanatomy, genetics, and
cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep
learning approaches combined image and tabular information to improve
diagnostic performance. However, the black-box nature of such neural networks
is still a barrier for clinical applications, in which understanding the
decision of a heterogeneous model is integral. We propose PANIC, a prototypical
additive neural network for interpretable AD classification that integrates 3D
image and tabular data. It is interpretable by design and, thus, avoids the
need for post-hoc explanations that try to approximate the decision of a
network. Our results demonstrate that PANIC achieves state-of-the-art
performance in AD classification, while directly providing local and global
explanations. Finally, we show that PANIC extracts biologically meaningful
signatures of AD, and satisfies a set of desirable desiderata for trustworthy
machine learning. Our implementation is available at
https://github.com/ai-med/PANIC .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of Information Processing In Medical
  Imaging 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied
  Agents under Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated embodied agent learning protects the data privacy of individual
visual environments by keeping data locally at each client (the individual
environment) during training. However, since the local data is inaccessible to
the server under federated learning, attackers may easily poison the training
data of the local client to build a backdoor in the agent without notice.
Deploying such an agent raises the risk of potential harm to humans, as the
attackers may easily navigate and control the agent as they wish via the
backdoor. Towards Byzantine-robust federated embodied agent learning, in this
paper, we study the attack and defense for the task of vision-and-language
navigation (VLN), where the agent is required to follow natural language
instructions to navigate indoor environments. First, we introduce a simple but
effective attack strategy, Navigation as Wish (NAW), in which the malicious
client manipulates local trajectory data to implant a backdoor into the global
model. Results on two VLN datasets (R2R and RxR) show that NAW can easily
navigate the deployed VLN agent regardless of the language instruction, without
affecting its performance on normal test sets. Then, we propose a new
Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated
VLN, which provides the server with a ''prompt'' of the vision-and-language
alignment variance between the benign and malicious clients so that they can be
distinguished during training. We validate the effectiveness of the PBA method
on protecting the global model from the NAW attack, which outperforms other
state-of-the-art defense methods by a large margin in the defense metrics on
R2R and RxR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep model with built-in cross-attention alignment for acoustic echo
  cancellation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Indenbom, Nicolae-Cătălin Ristea, Ando Saabas, Tanel Pärnamaa, Jegor Gužvin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent research advances, deep learning models have become an attractive
choice for acoustic echo cancellation (AEC) in real-time teleconferencing
applications. Since acoustic echo is one of the major sources of poor audio
quality, a wide variety of deep models have been proposed. However, an
important but often omitted requirement for good echo cancellation quality is
the synchronization of the microphone and far end signals. Typically
implemented using classical algorithms based on cross-correlation, the
alignment module is a separate functional block with known design limitations.
In our work we propose a deep learning architecture with built-in
self-attention based alignment, which is able to handle unaligned inputs,
improving echo cancellation performance while simplifying the communication
pipeline. Moreover, we show that our approach achieves significant improvements
for difficult delay estimation cases on real recordings from AEC Challenge data
set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-Candidate Rectification for Weakly Supervised Semantic
  Segmentation <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised semantic segmentation is typically inspired by class
activation maps, which serve as pseudo masks with class-discriminative regions
highlighted. Although tremendous efforts have been made to recall precise and
complete locations for each class, existing methods still commonly suffer from
the unsolicited Out-of-Candidate (OC) error predictions that not belongs to the
label candidates, which could be avoidable since the contradiction with
image-level class tags is easy to be detected. In this paper, we develop a
group ranking-based Out-of-Candidate Rectification (OCR) mechanism in a
plug-and-play fashion. Firstly, we adaptively split the semantic categories
into In-Candidate (IC) and OC groups for each OC pixel according to their prior
annotation correlation and posterior prediction correlation. Then, we derive a
differentiable rectification loss to force OC pixels to shift to the IC group.
Incorporating our OCR with seminal baselines (e.g., AffinityNet, SEAM,
MCTformer), we can achieve remarkable performance gains on both Pascal VOC
(+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with
negligible extra training overhead, which justifies the effectiveness and
generality of our OCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Vertex <span class="highlight-title">Diffusion</span> for Unified Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesen Cheng, Kehan Li, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified visual grounding pursues a simple and generic technical route to
leverage multi-task data with less task-specific design. The most advanced
methods typically present boxes and masks as vertex sequences to model
referring detection and segmentation as an autoregressive sequential vertex
generation paradigm. However, generating high-dimensional vertex sequences
sequentially is error-prone because the upstream of the sequence remains static
and cannot be refined based on downstream vertex information, even if there is
a significant location gap. Besides, with limited vertexes, the inferior
fitting of objects with complex contours restricts the performance upper bound.
To deal with this dilemma, we propose a parallel vertex generation paradigm for
superior high-dimension scalability with a diffusion model by simply modifying
the noise dimension. An intuitive materialization of our paradigm is Parallel
Vertex Diffusion (PVD) to directly set vertex coordinates as the generation
target and use a diffusion model to train and infer. We claim that it has two
flaws: (1) unnormalized coordinate caused a high variance of loss value; (2)
the original training objective of PVD only considers point consistency but
ignores geometry consistency. To solve the first flaw, Center Anchor Mechanism
(CAM) is designed to convert coordinates as normalized offset values to
stabilize the training loss value. For the second flaw, Angle summation loss
(ASL) is designed to constrain the geometry difference of prediction and ground
truth vertexes for geometry-level consistency. Empirical results show that our
PVD achieves state-of-the-art in both referring detection and segmentation, and
our paradigm is more scalable and efficient than sequential vertex generation
with high-dimension data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recursive Generalization <span class="highlight-title">Transformer</span> for Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architectures have exhibited remarkable performance in image
super-resolution (SR). Since the quadratic computational complexity of the
self-attention (SA) in Transformer, existing methods tend to adopt SA in a
local region to reduce overheads. However, the local design restricts the
global context exploitation, which is critical for accurate image
reconstruction. In this work, we propose the Recursive Generalization
Transformer (RGT) for image SR, which can capture global spatial information
and is suitable for high-resolution images. Specifically, we propose the
recursive-generalization self-attention (RG-SA). It recursively aggregates
input features into representative feature maps, and then utilizes
cross-attention to extract global information. Meanwhile, the channel
dimensions of attention matrices (query, key, and value) are further scaled for
a better trade-off between computational overheads and performance.
Furthermore, we combine the RG-SA with local self-attention to enhance the
exploitation of the global context, and propose the hybrid adaptive integration
(HAI) for module integration. The HAI allows the direct and effective fusion
between features at different levels (local or global). Extensive experiments
demonstrate that our RGT outperforms recent state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Component Segmentation of Engineering Drawings Using Graph Convolutional
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentai Zhang, Joe Joseph, Yue Yin, Liuyue Xie, Tomotake Furuhata, Soji Yamakawa, Kenji Shimada, Levent Burak Kara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a data-driven framework to automate the vectorization and machine
interpretation of 2D engineering part drawings. In industrial settings, most
manufacturing engineers still rely on manual reads to identify the topological
and manufacturing requirements from drawings submitted by designers. The
interpretation process is laborious and time-consuming, which severely inhibits
the efficiency of part quotation and manufacturing tasks. While recent advances
in image-based computer vision methods have demonstrated great potential in
interpreting natural images through semantic segmentation approaches, the
application of such methods in parsing engineering technical drawings into
semantically accurate components remains a significant challenge. The severe
pixel sparsity in engineering drawings also restricts the effective
featurization of image-based data-driven methods. To overcome these challenges,
we propose a deep learning based framework that predicts the semantic type of
each vectorized component. Taking a raster image as input, we vectorize all
components through thinning, stroke tracing, and cubic bezier fitting. Then a
graph of such components is generated based on the connectivity between the
components. Finally, a graph convolutional neural network is trained on this
graph data to identify the semantic type of each component. We test our
framework in the context of semantic segmentation of text, dimension and,
contour components in engineering drawings. Results show that our method yields
the best performance compared to recent image, and graph-based segmentation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted to Computers in Industry</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Super-Resolution using Efficient Striped Window <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpeng Shi, Hui Li, Tianle Liu, Yulong Liu, Mingjian Zhang, Jinchen Zhu, Ling Zheng, Shizhuang Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable results in single-image
super-resolution (SR). However, the challenge of balancing model performance
and complexity has hindered their application in lightweight SR (LSR). To
tackle this challenge, we propose an efficient striped window transformer
(ESWT). We revisit the normalization layer in the transformer and design a
concise and efficient transformer structure to build the ESWT. Furthermore, we
introduce a striped window mechanism to model long-term dependencies more
efficiently. To fully exploit the potential of the ESWT, we propose a novel
flexible window training strategy that can improve the performance of the ESWT
without additional cost. Extensive experiments show that ESWT outperforms
state-of-the-art LSR transformers, and achieves a better trade-off between
model performance and complexity. The ESWT requires fewer parameters, incurs
faster inference, smaller FLOPs, and less memory consumption, making it a
promising solution for LSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SOTA lightweight super-resolution transformer. 8 pages, 9 figures and
  6 tables. The Code is available at
  https://github.com/Fried-Rice-Lab/FriedRiceLab</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Assignment for Geometry Aware Local Feature Matching <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dihe Huang, Ying Chen, Shang Xu, Yong Liu, Wenlong Wu, Yikang Ding, Chengjie Wang, Fan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detector-free feature matching approaches are currently attracting great
attention thanks to their excellent performance. However, these methods still
struggle at large-scale and viewpoint variations, due to the geometric
inconsistency resulting from the application of the mutual nearest neighbour
criterion (\ie, one-to-one assignment) in patch-level matching.Accordingly, we
introduce AdaMatcher, which first accomplishes the feature correlation and
co-visible area estimation through an elaborate feature interaction module,
then performs adaptive assignment on patch-level matching while estimating the
scales between images, and finally refines the co-visible matches through scale
alignment and sub-pixel regression module.Extensive experiments show that
AdaMatcher outperforms solid baselines and achieves state-of-the-art results on
many downstream tasks. Additionally, the adaptive assignment and sub-pixel
refinement module can be used as a refinement network for other matching
methods, such as SuperGlue, to boost their performance further. The code will
be publicly available at https://github.com/AbyssGaze/AdaMatcher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dataset Distillation Using Parameter Pruning <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14609v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14609v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many fields, the acquisition of advanced models depends on large datasets,
making data storage and model training expensive. As a solution, dataset
distillation can synthesize a small dataset that preserves most information of
the original large dataset. The recently proposed dataset distillation method
by matching network parameters has been proven effective for several datasets.
However, the dimensions of network parameters are typically large. Furthermore,
some parameters are difficult to match during the distillation process,
degrading distillation performance. Based on this observation, this study
proposes a novel dataset distillation method based on parameter pruning that
solves the problem. The proposed method can synthesize more robust distilled
datasets and improve distillation performance by pruning difficult-to-match
parameters during the distillation process. Experimental results on three
datasets show that the proposed method outperforms other state-of-the-art
dataset distillation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted as a journal paper at IEEE SPL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V2XP-ASG: Generating Adversarial Scenes for Vehicle-to-Everything
  Perception <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13679v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13679v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xiang, Runsheng Xu, Xin Xia, Zhaoliang Zheng, Bolei Zhou, Jiaqi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vehicle-to-Everything communication technology have
enabled autonomous vehicles to share sensory information to obtain better
perception performance. With the rapid growth of autonomous vehicles and
intelligent infrastructure, the V2X perception systems will soon be deployed at
scale, which raises a safety-critical question: \textit{how can we evaluate and
improve its performance under challenging traffic scenarios before the
real-world deployment?} Collecting diverse large-scale real-world test scenes
seems to be the most straightforward solution, but it is expensive and
time-consuming, and the collections can only cover limited scenarios. To this
end, we propose the first open adversarial scene generator V2XP-ASG that can
produce realistic, challenging scenes for modern LiDAR-based multi-agent
perception systems. V2XP-ASG learns to construct an adversarial collaboration
graph and simultaneously perturb multiple agents' poses in an adversarial and
plausible manner. The experiments demonstrate that V2XP-ASG can effectively
identify challenging scenes for a large range of V2X perception systems.
Meanwhile, by training on the limited number of generated challenging scenes,
the accuracy of V2X perception systems can be further improved by 12.3\% on
challenging and 4\% on normal scenes. Our code will be released at
https://github.com/XHwind/V2XP-ASG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2023, see https://github.com/XHwind/V2XP-ASG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegViz: A federated-learning based framework for multi-or<span class="highlight-title">gan</span>
  segmentation on heterogeneous data sets with partial annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adway U. Kanhere, Pranav Kulkarni, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation is one of the most primary tasks in deep learning for medical
imaging, owing to its multiple downstream clinical applications. However,
generating manual annotations for medical images is time-consuming, requires
high skill, and is an expensive effort, especially for 3D images. One potential
solution is to aggregate knowledge from partially annotated datasets from
multiple groups to collaboratively train global models using Federated
Learning. To this end, we propose SegViz, a federated learning-based framework
to train a segmentation model from distributed non-i.i.d datasets with partial
annotations. The performance of SegViz was compared against training individual
models separately on each dataset as well as centrally aggregating all the
datasets in one place and training a single model. The SegViz framework using
FedBN as the aggregation strategy demonstrated excellent performance on the
external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for
segmentation of liver, spleen, pancreas, and kidneys, respectively,
significantly ($p<0.05$) better (except spleen) than the dice scores of 0.87,
0.83, 0.42, and 0.48 for the baseline models. In contrast, the central
aggregation model significantly ($p<0.05$) performed poorly on the test dataset
with dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the
potential of the SegViz framework to train multi-task models from distributed
datasets with partial labels. All our implementations are open-source and
available at https://anonymous.4open.science/r/SegViz-B746
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Observation-Centric SORT: Rethinking SORT for Robust Multi-Object
  Tracking <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.14360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.14360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirodkar, Kris Kitani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kalman filter (KF) based methods for multi-object tracking (MOT) make an
assumption that objects move linearly. While this assumption is acceptable for
very short periods of occlusion, linear estimates of motion for prolonged time
can be highly inaccurate. Moreover, when there is no measurement available to
update Kalman filter parameters, the standard convention is to trust the priori
state estimations for posteriori update. This leads to the accumulation of
errors during a period of occlusion. The error causes significant motion
direction variance in practice. In this work, we show that a basic Kalman
filter can still obtain state-of-the-art tracking performance if proper care is
taken to fix the noise accumulated during occlusion. Instead of relying only on
the linear state estimate (i.e., estimation-centric approach), we use object
observations (i.e., the measurements by object detector) to compute a virtual
trajectory over the occlusion period to fix the error accumulation of filter
parameters during the occlusion period. This allows more time steps to correct
errors accumulated during occlusion. We name our method Observation-Centric
SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves
robustness during occlusion and non-linear motion. Given off-the-shelf
detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves
state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head
tracking, and especially DanceTrack where the object motion is highly
non-linear. The code and models are available at
\url{https://github.com/noahcao/OC_SORT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. 8 pages + 10 pages of appendix. Renamed OOS as
  Observation-centric Re-Update (ORU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Bridging the Performance Gaps of Joint Energy-based Models <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulong Yang, Qing Su, Shihao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we train a hybrid discriminative-generative model within a single
network? This question has recently been answered in the affirmative,
introducing the field of Joint Energy-based Model (JEM), which achieves high
classification accuracy and image generation quality simultaneously. Despite
recent advances, there remain two performance gaps: the accuracy gap to the
standard softmax classifier, and the generation quality gap to state-of-the-art
generative models. In this paper, we introduce a variety of training techniques
to bridge the accuracy gap and the generation quality gap of JEM. 1) We
incorporate a recently proposed sharpness-aware minimization (SAM) framework to
train JEM, which promotes the energy landscape smoothness and the
generalizability of JEM. 2) We exclude data augmentation from the maximum
likelihood estimate pipeline of JEM, and mitigate the negative impact of data
augmentation to image generation quality. Extensive experiments on multiple
datasets demonstrate that our SADA-JEM achieves state-of-the-art performances
and outperforms JEM in image classification, image generation, calibration,
out-of-distribution detection and adversarial robustness by a notable margin.
Our code is available at https://github.com/sndnyang/SADAJEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Robust Spiking Neural Networks on Neuromorphic Data with
  Spatiotemporal Fragments <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11659v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11659v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic vision sensors (event cameras) are inherently suitable for
spiking neural networks (SNNs) and provide novel neuromorphic vision data for
this biomimetic model. Due to the spatiotemporal characteristics, novel data
augmentations are required to process the unconventional visual signals of
these cameras. In this paper, we propose a novel Event SpatioTemporal Fragments
(ESTF) augmentation method. It preserves the continuity of neuromorphic data by
drifting or inverting fragments of the spatiotemporal event stream to simulate
the disturbance of brightness variations, leading to more robust spiking neural
networks. Extensive experiments are performed on prevailing neuromorphic
datasets. It turns out that ESTF provides substantial improvements over pure
geometric transformations and outperforms other event data augmentation
methods. It is worth noting that the SNNs with ESTF achieve the
state-of-the-art accuracy of 83.9\% on the CIFAR10-DVS dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Stronger Spiking Neural Networks with Biomimetic Adaptive
  Internal Association Neurons <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the third generation of neural networks, spiking neural networks (SNNs)
are dedicated to exploring more insightful neural mechanisms to achieve
near-biological intelligence. Intuitively, biomimetic mechanisms are crucial to
understanding and improving SNNs. For example, the associative long-term
potentiation (ALTP) phenomenon suggests that in addition to learning mechanisms
between neurons, there are associative effects within neurons. However, most
existing methods only focus on the former and lack exploration of the internal
association effects. In this paper, we propose a novel Adaptive Internal
Association~(AIA) neuron model to establish previously ignored influences
within neurons. Consistent with the ALTP phenomenon, the AIA neuron model is
adaptive to input stimuli, and internal associative learning occurs only when
both dendrites are stimulated at the same time. In addition, we employ weighted
weights to measure internal associations and introduce intermediate caches to
reduce the volatility of associations. Extensive experiments on prevailing
neuromorphic datasets show that the proposed method can potentiate or depress
the firing of spikes more specifically, resulting in better performance with
fewer spikes. It is worth noting that without adding any parameters at
inference, the AIA model achieves state-of-the-art performance on
DVS-CIFAR10~(83.9\%) and N-CARS~(95.64\%) datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, Diana Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation aims to segment an image into semantic
regions according to text descriptions, which may not have been seen during
training. Recent two-stage methods first generate class-agnostic mask proposals
and then leverage pre-trained vision-language models, e.g., CLIP, to classify
masked regions. We identify the performance bottleneck of this paradigm to be
the pre-trained CLIP model, since it does not perform well on masked images. To
address this, we propose to finetune CLIP on a collection of masked image
regions and their corresponding text descriptions. We collect training data by
mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to
match masked image regions to nouns in the image captions. Compared with the
more precise and manually annotated segmentation labels with fixed classes
(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain
CLIP's generalization ability. Along with finetuning the entire model, we
utilize the "blank" areas in masked images using a method we dub mask prompt
tuning. Experiments demonstrate mask prompt tuning brings significant
improvement without modifying any weights of CLIP, and it can further improve a
fully finetuned model. In particular, when trained on COCO and evaluated on
ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the
previous state-of-the-art. For the first time, open-vocabulary generalist
models match the performance of supervised specialist models in 2017 without
dataset-specific adaptations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://jeff-liangf.github.io/projects/ovseg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Video Compression with Diverse Contexts <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14402v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14402v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Li, Bin Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any video codecs, the coding efficiency highly relies on whether the
current signal to be encoded can find the relevant contexts from the previous
reconstructed signals. Traditional codec has verified more contexts bring
substantial coding gain, but in a time-consuming manner. However, for the
emerging neural video codec (NVC), its contexts are still limited, leading to
low compression ratio. To boost NVC, this paper proposes increasing the context
diversity in both temporal and spatial dimensions. First, we guide the model to
learn hierarchical quality patterns across frames, which enriches long-term and
yet high-quality temporal contexts. Furthermore, to tap the potential of
optical flow-based coding framework, we introduce a group-based offset
diversity where the cross-group interaction is proposed for better context
mining. In addition, this paper also adopts a quadtree-based partition to
increase spatial context diversity when encoding the latent representation in
parallel. Experiments show that our codec obtains 23.5% bitrate saving over
previous SOTA NVC. Better yet, our codec has surpassed the under-developing
next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in
terms of PSNR. The codes are at https://github.com/microsoft/DCVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta Architecture for Point Cloud Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojia Lin, Xiawu Zheng, Lijiang Li, Fei Chao, Shanshan Wang, Yan Wang, Yonghong Tian, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D point cloud analysis bring a diverse set of network
architectures to the field. However, the lack of a unified framework to
interpret those networks makes any systematic comparison, contrast, or analysis
challenging, and practically limits healthy development of the field. In this
paper, we take the initiative to explore and propose a unified framework called
PointMeta, to which the popular 3D point cloud analysis approaches could fit.
This brings three benefits. First, it allows us to compare different approaches
in a fair manner, and use quick experiments to verify any empirical
observations or assumptions summarized from the comparison. Second, the big
picture brought by PointMeta enables us to think across different components,
and revisit common beliefs and key design decisions made by the popular
approaches. Third, based on the learnings from the previous two analyses, by
doing simple tweaks on the existing approaches, we are able to derive a basic
building block, termed PointMetaBase. It shows very strong performance in
efficiency and effectiveness through extensive experiments on challenging
benchmarks, and thus verifies the necessity and benefits of high-level
interpretation, contrast, and comparison like PointMeta. In particular,
PointMetaBase surpasses the previous state-of-the-art method by 0.7%/1.4/%2.1%
mIoU with only 2%/11%/13% of the computation cost on the S3DIS datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Loop Method Combining Active and Semi-Supervised Learning for
  Domain Adaptive Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13361v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13361v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Licong Guan, Xue Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation is an important technique for environment perception in
intelligent transportation systems. With the rapid development of convolutional
neural networks (CNNs), road scene analysis can usually achieve satisfactory
results in the source domain. However, guaranteeing good generalization to
different target domain scenarios remains a significant challenge. Recently,
semi-supervised learning and active learning have been proposed to alleviate
this problem. Semisupervised learning can improve model accuracy with massive
unlabeled data, but some pseudo labels containing noise would be generated with
limited or imbalanced training data. And there will be suboptimal models if
human guidance is absent. Active learning can select more effective data to
intervene, while the model accuracy can not be improved because the massive
unlabeled data are not used. And the probability of querying sub-optimal
samples will increase when the domain difference is too large, increasing
annotation cost. This paper proposes an iterative loop method combining active
and semisupervised learning for domain adaptive semantic segmentation. The
method first uses semi-supervised to learn massive unlabeled data to improve
model accuracy and provide more accurate selection models for active learning.
Secondly, combined with the predictive uncertainty sample selection strategy of
active learning, manual intervention is used to correct the pseudo-labels.
Finally, flexible iterative loops achieve the best performance with minimal
labeling cost. Extensive experiments show that our method establishes
state-of-the-art performance on tasks of GTAV to Cityscapes, SYNTHIA to
Cityscapes, improving by 4.9% mIoU and 5.2% mIoU, compared to the previous best
method, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Domain Calibration and Uncertainty Estimation for Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.04446v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04446v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre-Yves Lajoie, Giovanni Beltrame
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition techniques based on deep learning, which have
imposed themselves as the state-of-the-art in recent years, do not generalize
well to environments visually different from the training set. Thus, to achieve
top performance, it is sometimes necessary to fine-tune the networks to the
target environment. To this end, we propose a self-supervised domain
calibration procedure based on robust pose graph optimization from Simultaneous
Localization and Mapping (SLAM) as the supervision signal without requiring GPS
or manual labeling. Moreover, we leverage the procedure to improve uncertainty
estimation for place recognition matches which is important in safety critical
applications. We show that our approach can improve the performance of a
state-of-the-art technique on a target environment dissimilar from its training
set and that we can obtain uncertainty estimates. We believe that this approach
will help practitioners to deploy robust place recognition solutions in
real-world applications. Our code is available publicly:
https://github.com/MISTLab/vpr-calibration-and-uncertainty
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Teacher: Semi-Supervised Object Detection for YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07577v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07577v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Xu, Mingtao Chen, Wenlong Guan, Lulu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-Supervised Object Detection (SSOD) has been successful in improving the
performance of both R-CNN series and anchor-free detectors. However, one-stage
anchor-based detectors lack the structure to generate high-quality or flexible
pseudo labels, leading to serious inconsistency problems in SSOD. In this
paper, we propose the Efficient Teacher framework for scalable and effective
one-stage anchor-based SSOD training, consisting of Dense Detector, Pseudo
Label Assigner, and Epoch Adaptor. Dense Detector is a baseline model that
extends RetinaNet with dense sampling techniques inspired by YOLOv5. The
Efficient Teacher framework introduces a novel pseudo label assignment
mechanism, named Pseudo Label Assigner, which makes more refined use of pseudo
labels from Dense Detector. Epoch Adaptor is a method that enables a stable and
efficient end-to-end semi-supervised training schedule for Dense Detector. The
Pseudo Label Assigner prevents the occurrence of bias caused by a large number
of low-quality pseudo labels that may interfere with the Dense Detector during
the student-teacher mutual learning mechanism, and the Epoch Adaptor utilizes
domain and distribution adaptation to allow Dense Detector to learn globally
distributed consistent features, making the training independent of the
proportion of labeled data. Our experiments show that the Efficient Teacher
framework achieves state-of-the-art results on VOC, COCO-standard, and
COCO-additional using fewer FLOPs than previous methods. To the best of our
knowledge, this is the first attempt to apply Semi-Supervised Object Detection
to YOLOv5.Code is available:
https://github.com/AlibabaResearch/efficientteacher
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ eDiff-I: Text-to-Image <span class="highlight-title">Diffusion</span> Models with an Ensemble of Expert
  Denoisers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01324v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01324v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale diffusion-based generative models have led to breakthroughs in
text-conditioned high-resolution image synthesis. Starting from random noise,
such text-to-image diffusion models gradually synthesize images in an iterative
fashion while conditioning on text prompts. We find that their synthesis
behavior qualitatively changes throughout this process: Early in sampling,
generation strongly relies on the text prompt to generate text-aligned content,
while later, the text conditioning is almost entirely ignored. This suggests
that sharing model parameters throughout the entire generation process may not
be ideal. Therefore, in contrast to existing works, we propose to train an
ensemble of text-to-image diffusion models specialized for different synthesis
stages. To maintain training efficiency, we initially train a single model,
which is then split into specialized models that are trained for the specific
stages of the iterative generation process. Our ensemble of diffusion models,
called eDiff-I, results in improved text alignment while maintaining the same
inference computation cost and preserving high visual quality, outperforming
previous large-scale text-to-image diffusion models on the standard benchmark.
In addition, we train our model to exploit a variety of embeddings for
conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We
show that these different embeddings lead to different behaviors. Notably, the
CLIP image embedding allows an intuitive way of transferring the style of a
reference image to the target text-to-image output. Lastly, we show a technique
that enables eDiff-I's "paint-with-words" capability. A user can select the
word in the input text and paint it in a canvas to control the output, which is
very handy for crafting the desired image in mind. The project page is
available at https://deepimagination.cc/eDiff-I/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving information retrieval through correspondence analysis instead
  of latent semantic analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Qi, David J. Hessen, Peter G. M. van der Heijden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both latent semantic analysis (LSA) and correspondence analysis (CA) are
dimensionality reduction techniques that use singular value decomposition (SVD)
for information retrieval. Theoretically, the results of LSA display both the
association between documents and terms, and marginal effects; in comparison,
CA only focuses on the associations between documents and terms. Marginal
effects are usually not relevant for information retrieval, and therefore, from
a theoretical perspective CA is more suitable for information retrieval.
  In this paper, we empirically compare LSA and CA. The elements of the raw
document-term matrix are weighted, and the weighting exponent of singular
values is adjusted to improve the performance of LSA. We explore whether these
two weightings also improve the performance of CA. In addition, we compare the
optimal singular value weighting exponents for LSA and CA to identify what the
initial dimensions in LSA correspond to.
  The results for four empirical datasets show that CA always performs better
than LSA. Weighting the elements of the raw data matrix can improve CA;
however, it is data dependent and the improvement is small. Adjusting the
singular value weighting exponent usually improves the performance of CA;
however, the extent of the improved performance depends on the dataset and
number of dimensions. In general, CA needs a larger singular value weighting
exponent than LSA to obtain the optimal performance. This indicates that CA
emphasizes initial dimensions more than LSA, and thus, margins play an
important role in the initial dimensions in LSA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Graph Social Recommendation <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghao Xia, Yizhen Shao, Chao Huang, Yong Xu, Huance Xu, Jian Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social recommender systems have drawn a lot of attention in many online web
services, because of the incorporation of social information between users in
improving recommendation results. Despite the significant progress made by
existing solutions, we argue that current methods fall short in two
limitations: (1) Existing social-aware recommendation models only consider
collaborative similarity between items, how to incorporate item-wise semantic
relatedness is less explored in current recommendation paradigms. (2) Current
social recommender systems neglect the entanglement of the latent factors over
heterogeneous relations (e.g., social connections, user-item interactions).
Learning the disentangled representations with relation heterogeneity poses
great challenge for social recommendation. In this work, we design a
Disentangled Graph Neural Network (DGNN) with the integration of latent memory
units, which empowers DGNN to maintain factorized representations for
heterogeneous types of user and item connections. Additionally, we devise new
memory-augmented message propagation and aggregation schemes under the graph
neural architecture, allowing us to recursively distill semantic relatedness
into the representations of users and items in a fully automatic manner.
Extensive experiments on three benchmark datasets verify the effectiveness of
our model by achieving great improvement over state-of-the-art recommendation
techniques. The source code is publicly available at:
https://github.com/HKUDS/DGNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICDE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated <span class="highlight-title">Self-Supervised</span> Learning for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, Ben Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm
for collaborative filtering (CF). To improve the representation quality over
limited labeled data, contrastive learning has attracted attention in
recommendation and benefited graph-based CF model recently. However, the
success of most contrastive methods heavily relies on manually generating
effective contrastive views for heuristic-based data augmentation. This does
not generalize across different datasets and downstream recommendation tasks,
which is difficult to be adaptive for data augmentation and robust to noise
perturbation. To fill this crucial gap, this work proposes a unified Automated
Collaborative Filtering (AutoCF) to automatically perform data augmentation for
recommendation. Specifically, we focus on the generative self-supervised
learning framework with a learnable augmentation paradigm that benefits the
automated distillation of important self-supervised signals. To enhance the
representation discrimination ability, our masked graph autoencoder is designed
to aggregate global information during the augmentation via reconstructing the
masked subgraph structures. Experiments and ablation studies are performed on
several public datasets for recommending products, venues, and locations.
Results demonstrate the superiority of AutoCF against various baseline methods.
We release the model implementation at https://github.com/HKUDS/AutoCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM The Web Conference, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query2doc: Query Expansion with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a simple yet effective query expansion approach,
denoted as query2doc, to improve both sparse and dense retrieval systems. The
proposed method first generates pseudo-documents by few-shot prompting large
language models (LLMs), and then expands the query with generated
pseudo-documents. LLMs are trained on web-scale text corpora and are adept at
knowledge memorization. The pseudo-documents from LLMs often contain highly
relevant information that can aid in query disambiguation and guide the
retrievers. Experimental results demonstrate that query2doc boosts the
performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and
TREC DL, without any model fine-tuning. Furthermore, our method also benefits
state-of-the-art dense retrievers in terms of both in-domain and out-of-domain
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoMeta: Enhancing Meta Embeddings with Collaborative Information in
  Cold-start Problem of Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Hu, Dazhong Rong, Jianhai Chen, Qinming He, Zhenguang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cold-start problem is quite challenging for existing recommendation
models. Specifically, for the new items with only a few interactions, their ID
embeddings are trained inadequately, leading to poor recommendation
performance. Some recent studies introduce meta learning to solve the
cold-start problem by generating meta embeddings for new items as their initial
ID embeddings. However, we argue that the capability of these methods is
limited, because they mainly utilize item attribute features which only contain
little information, but ignore the useful collaborative information contained
in the ID embeddings of users and old items. To tackle this issue, we propose
CoMeta to enhance the meta embeddings with the collaborative information.
CoMeta consists of two submodules: B-EG and S-EG. Specifically, for a new item:
B-EG calculates the similarity-based weighted sum of the ID embeddings of old
items as its base embedding; S-EG generates its shift embedding not only with
its attribute features but also with the average ID embedding of the users who
interacted with it. The final meta embedding is obtained by adding up the base
embedding and the shift embedding. We conduct extensive experiments on two
public datasets. The experimental results demonstrate both the effectiveness
and the compatibility of CoMeta.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Concept Knowledge Graph for User Next Intent Prediction at Alipay <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00503v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00503v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacheng He, Qianghuai Jia, Lin Yuan, Ruopeng Li, Yixin Ou, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper illustrates the technologies of user next intent prediction with a
concept knowledge graph. The system has been deployed on the Web at Alipay,
serving more than 100 million daily active users. To explicitly characterize
user intent, we propose AlipayKG, which is an offline concept knowledge graph
in the Life-Service domain modeling the historical behaviors of users, the rich
content interacted by users and the relations between them. We further
introduce a Transformer-based model which integrates expert rules from the
knowledge graph to infer the online user's next intent. Experimental results
demonstrate that the proposed system can effectively enhance the performance of
the downstream tasks while retaining explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Discrimination to Generation: Knowledge Graph Completion with
  Generative <span class="highlight-title">Transformer</span> <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02113v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02113v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui Chen, Feiyu Xiong, Mosha Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2022 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relphormer: Relational Graph <span class="highlight-title">Transformer</span> for Knowledge Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10852v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10852v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Ningyu Zhang, Qiang Chen, Feiyu Xiong, Wei Guo, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical
  Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhou, Xianshuai Cao, Wenhao Li, Lin Bo, Kun Zhang, Chuan Luo, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-scenario & multi-task learning has been widely applied to many
recommendation systems in industrial applications, wherein an effective and
practical approach is to carry out multi-scenario transfer learning on the
basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based
method, which aims to project all information in the same feature space, cannot
effectively deal with the complex relationships inherent among various
scenarios and tasks, resulting in unsatisfactory performance. To tackle the
problem, we propose a Hierarchical information extraction Network (HiNet) for
multi-scenario and multi-task recommendation, which achieves hierarchical
extraction based on coarse-to-fine knowledge transfer scheme. The multiple
extraction layers of the hierarchical network enable the model to enhance the
capability of transferring valuable information across scenarios while
preserving specific features of scenarios and tasks. Furthermore, a novel
scenario-aware attentive network module is proposed to model correlations
between scenarios explicitly. Comprehensive experiments conducted on real-world
industrial datasets from Meituan Meishi platform demonstrate that HiNet
achieves a new state-of-the-art performance and significantly outperforms
existing solutions. HiNet is currently fully deployed in two scenarios and has
achieved 2.87% and 1.75% order quantity gain respectively.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesh<span class="highlight-title">Diffusion</span>: Score-based Generative 3D Mesh Modeling <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of generating realistic 3D shapes, which is useful for a
variety of applications such as automatic scene generation and physical
simulation. Compared to other 3D representations like voxels and point clouds,
meshes are more desirable in practice, because (1) they enable easy and
arbitrary manipulation of shapes for relighting and simulation, and (2) they
can fully leverage the power of modern graphics pipelines which are mostly
optimized for meshes. Previous scalable methods for generating meshes typically
rely on sub-optimal post-processing, and they tend to produce overly-smooth or
noisy surfaces without fine-grained geometric details. To overcome these
shortcomings, we take advantage of the graph structure of meshes and use a
simple yet very effective generative modeling method to generate 3D meshes.
Specifically, we represent meshes with deformable tetrahedral grids, and then
train a diffusion model on this direct parametrization. We demonstrate the
effectiveness of our model on multiple generative tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2023 (Spotlight, Notable-top-25%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CB2: Collaborative Natural Language Interaction Research Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Sharf, Mustafa Omer Gul, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CB2 is a multi-agent platform to study collaborative natural language
interaction in a grounded task-oriented scenario. It includes a 3D game
environment, a backend server designed to serve trained models to human agents,
and various tools and processes to enable scalable studies. We deploy CB2 at
https://cb2.ai as a system demonstration with a learned instruction following
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do <span class="highlight-title">Transformer</span>s Parse while Predicting the Masked Word? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhao, Abhishek Panigrahi, Rong Ge, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have been shown to encode linguistic structures,
e.g. dependency and constituency parse trees, in their embeddings while being
trained on unsupervised loss functions like masked language modeling. Some
doubts have been raised whether the models actually are doing parsing or only
some computation weakly correlated with it. We study questions: (a) Is it
possible to explicitly describe transformers with realistic embedding
dimension, number of heads, etc. that are capable of doing parsing -- or even
approximate parsing? (b) Why do pre-trained models capture parsing structure?
This paper takes a step toward answering these questions in the context of
generative modeling with PCFGs. We show that masked language models like BERT
or RoBERTa of moderate sizes can approximately execute the Inside-Outside
algorithm for the English PCFG [Marcus et al, 1993]. We also show that the
Inside-Outside algorithm is optimal for masked language modeling loss on the
PCFG-generated data. We also give a construction of transformers with $50$
layers, $15$ attention heads, and $1275$ dimensional embeddings in average such
that using its embeddings it is possible to do constituency parsing with
$>70\%$ F1 score on PTB dataset. We conduct probing experiments on models
pre-trained on PCFG-generated data to show that this not only allows recovery
of approximate parse tree, but also recovers marginal span probabilities
computed by the Inside-Outside algorithm, which suggests an implicit bias of
masked language modeling towards this algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simfluence: Modeling the Influence of Individual Training Examples by
  Simulating Training Runs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, Tolga Bolukbasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data attribution (TDA) methods offer to trace a model's prediction
on any given example back to specific influential training examples. Existing
approaches do so by assigning a scalar influence score to each training
example, under a simplifying assumption that influence is additive. But in
reality, we observe that training examples interact in highly non-additive ways
due to factors such as inter-example redundancy, training order, and curriculum
learning effects.
  To study such interactions, we propose Simfluence, a new paradigm for TDA
where the goal is not to produce a single influence score per example, but
instead a training run simulator: the user asks, ``If my model had trained on
example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on
$z_{test}$?''; the simulator should then output a simulated training run, which
is a time series predicting the loss on $z_{test}$ at every step of the
simulated run. This enables users to answer counterfactual questions about what
their model would have learned under different training curricula, and to
directly see where in training that learning would occur.
  We present a simulator, Simfluence-Linear, that captures non-additive
interactions and is often able to predict the spiky trajectory of individual
example losses with surprising fidelity. Furthermore, we show that existing TDA
methods such as TracIn and influence functions can be viewed as special cases
of Simfluence-Linear. This enables us to directly compare methods in terms of
their simulation accuracy, subsuming several prior TDA approaches to
evaluation. In experiments on large language model (LLM) fine-tuning, we show
that our method predicts loss trajectories with much higher accuracy than
existing TDA methods (doubling Spearman's correlation and reducing mean-squared
error by 75%) across several tasks, models, and training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eliciting Latent Predictions from <span class="highlight-title">Transformer</span>s with the Tuned Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze transformers from the perspective of iterative inference, seeking
to understand how model predictions are refined layer by layer. To do so, we
train an affine probe for each block in a frozen pretrained model, making it
possible to decode every hidden state into a distribution over the vocabulary.
Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit
lens'' technique, which yielded useful insights but is often brittle.
  We test our method on various autoregressive language models with up to 20B
parameters, showing it to be more predictive, reliable and unbiased than the
logit lens. With causal experiments, we show the tuned lens uses similar
features to the model itself. We also find the trajectory of latent predictions
can be used to detect malicious inputs with high accuracy. All code needed to
reproduce our results can be found at
https://github.com/AlignmentResearch/tuned-lens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-based route following by an embodied insect-inspired sparse
  neural network <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Yihe, Rana Alkhoury Maroun, Barbara Webb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We compared the efficiency of the FlyHash model, an insect-inspired sparse
neural network (Dasgupta et al., 2017), to similar but non-sparse models in an
embodied navigation task. This requires a model to control steering by
comparing current visual inputs to memories stored along a training route. We
concluded the FlyHash model is more efficient than others, especially in terms
of data encoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures; work-in-progress submission, accepted as a poster
  at ICLR 2023 Workshop on Sparsity in Neural Networks; non-archival</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Eldowa, Nicolò Cesa-Bianchi, Alberto Maria Metelli, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of bandits with expert advice when the experts are
fixed and known distributions over the actions. Improving on previous analyses,
we show that the regret in this setting is controlled by information-theoretic
quantities that measure the similarity between experts. In some natural special
cases, this allows us to obtain the first regret bound for EXP4 that can get
arbitrarily close to zero if the experts are similar enough. While for a
different algorithm, we provide another bound that describes the similarity
between the experts in terms of the KL-divergence, and we show that this bound
can be smaller than the one of EXP4 in some cases. Additionally, we provide
lower bounds for certain classes of experts showing that the algorithms we
analyzed are nearly optimal in some cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explanation Shift: Investigating Interactions between Models and
  Shifting Data Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Mougan, Klaus Broelemann, David Masip, Gjergji Kasneci, Thanassis Thiropanis, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As input data distributions evolve, the predictive performance of machine
learning models tends to deteriorate. In practice, new input data tend to come
without target labels. Then, state-of-the-art techniques model input data
distributions or model prediction distributions and try to understand issues
regarding the interactions between learned models and shifting distributions.
We suggest a novel approach that models how explanation characteristics shift
when affected by distribution shifts. We find that the modeling of explanation
shifts can be a better indicator for detecting out-of-distribution model
behaviour than state-of-the-art techniques. We analyze different types of
distribution shifts using synthetic examples and real-world data sets. We
provide an algorithmic method that allows us to inspect the interaction between
data set features and learned models and compare them to the state-of-the-art.
We release our methods in an open-source Python package, as well as the code
used to reproduce our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2210.12369</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Rates for Maximum Entropy Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Pierre Perrault, Yunhao Tang, Michal Valko, Pierre Menard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the reinforcement learning (RL) setting, in which the agent has
to act in unknown environment driven by a Markov Decision Process (MDP) with
sparse or even reward free signals. In this situation, exploration becomes the
main challenge. In this work, we study the maximum entropy exploration problem
of two different types. The first type is visitation entropy maximization that
was previously considered by Hazan et al. (2019) in the discounted setting. For
this type of exploration, we propose an algorithm based on a game theoretic
representation that has $\widetilde{\mathcal{O}}(H^3 S^2 A / \varepsilon^2)$
sample complexity thus improving the $\varepsilon$-dependence of Hazan et al.
(2019), where $S$ is a number of states, $A$ is a number of actions, $H$ is an
episode length, and $\varepsilon$ is a desired accuracy. The second type of
entropy we study is the trajectory entropy. This objective function is closely
related to the entropy-regularized MDPs, and we propose a simple modification
of the UCBVI algorithm that has a sample complexity of order
$\widetilde{\mathcal{O}}(1/\varepsilon)$ ignoring dependence in $S, A, H$.
Interestingly enough, it is the first theoretical result in RL literature
establishing that the exploration problem for the regularized MDPs can be
statistically strictly easier (in terms of sample complexity) than for the
ordinary MDPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Hardware Design With Multi-model Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Ghaffari, Masoud Asgharian, Yvon Savaria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rising complexity of numerous novel applications that serve our
modern society comes the strong need to design efficient computing platforms.
Designing efficient hardware is, however, a complex multi-objective problem
that deals with multiple parameters and their interactions. Given that there
are a large number of parameters and objectives involved in hardware design,
synthesizing all possible combinations is not a feasible method to find the
optimal solution. One promising approach to tackle this problem is statistical
modeling of a desired hardware performance. Here, we propose a model-based
active learning approach to solve this problem. Our proposed method uses
Bayesian models to characterize various aspects of hardware performance. We
also use transfer learning and Gaussian regression bootstrapping techniques in
conjunction with active learning to create more accurate models. Our proposed
statistical modeling method provides hardware models that are sufficiently
accurate to perform design space exploration as well as performance prediction
simultaneously. We use our proposed method to perform design space exploration
and performance prediction for various hardware setups, such as
micro-architecture design and OpenCL kernels for FPGA targets. Our experiments
show that the number of samples required to create performance models
significantly reduces while maintaining the predictive power of our proposed
statistical models. For instance, in our performance prediction setting, the
proposed method needs 65\% fewer samples to create the model, and in the design
space exploration setting, our proposed method can find the best parameter
settings by exploring less than 50 samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demographic Parity Inspector: Fairness Audits via the Explanation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Mougan, Laura State, Antonio Ferrara, Salvatore Ruggieri, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even if deployed with the best intentions, machine learning methods can
perpetuate, amplify or even create social biases. Measures of (un-)fairness
have been proposed as a way to gauge the (non-)discriminatory nature of machine
learning models. However, proxies of protected attributes causing
discriminatory effects remain challenging to address. In this work, we propose
a new algorithmic approach that measures group-wise demographic parity
violations and allows us to inspect the causes of inter-group discrimination.
Our method relies on the novel idea of measuring the dependence of a model on
the protected attribute based on the explanation space, an informative space
that allows for more sensitive audits than the primary space of input data or
prediction distributions, and allowing for the assertion of theoretical
demographic parity auditing guarantees. We provide a mathematical analysis,
synthetic examples, and experimental evaluation of real-world data. We release
an open-source Python package with methods, routines, and tutorials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISimDL: Importance Sampling-Driven Acceleration of Fault Injection
  Simulations for Evaluating the Robustness of Deep Learning <span class="chip">IJCNN2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Colucci, Andreas Steininger, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) systems have proliferated in many applications, requiring
specialized hardware accelerators and chips. In the nano-era, devices have
become increasingly more susceptible to permanent and transient faults.
Therefore, we need an efficient methodology for analyzing the resilience of
advanced DL systems against such faults, and understand how the faults in
neural accelerator chips manifest as errors at the DL application level, where
faults can lead to undetectable and unrecoverable errors. Using fault
injection, we can perform resilience investigations of the DL system by
modifying neuron weights and outputs at the software-level, as if the hardware
had been affected by a transient fault. Existing fault models reduce the search
space, allowing faster analysis, but requiring a-priori knowledge on the model,
and not allowing further analysis of the filtered-out search space. Therefore,
we propose ISimDL, a novel methodology that employs neuron sensitivity to
generate importance sampling-based fault-scenarios. Without any a-priori
knowledge of the model-under-test, ISimDL provides an equivalent reduction of
the search space as existing works, while allowing long simulations to cover
all the possible faults, improving on existing model requirements. Our
experiments show that the importance sampling provides up to 15x higher
precision in selecting critical faults than the random uniform sampling,
reaching such precision in less than 100 faults. Additionally, we showcase
another practical use-case for importance sampling for reliable DNN design,
namely Fault Aware Training (FAT). By using ISimDL to select the faults leading
to errors, we can insert the faults during the DNN training process to harden
the DNN against such faults. Using importance sampling in FAT reduces the
overhead required for finding faults that lead to a predetermined drop in
accuracy by more than 12x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IJCNN2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BODEGA: Benchmark for Adversarial Example Generation in Credibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Przybyła, Alexander Shvets, Horacio Saggion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we introduce BODEGA: a benchmark for testing both victim models
and attack methods on four misinformation detection tasks in an evaluation
framework designed to simulate real use-cases of content moderation. We also
systematically test the robustness of popular text classifiers against
available attacking techniques and discover that, indeed, in some cases barely
significant changes in input text can mislead the models. We openly share the
BODEGA code and data in hope of enhancing the comparability and replicability
of further research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical Regression Chain Framework for Affective Vocal Burst
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchao Li, Xixin Wu, Kaitao Song, Dongsheng Li, Xunying Liu, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a common way of emotion signaling via non-linguistic vocalizations, vocal
burst (VB) plays an important role in daily social interaction. Understanding
and modeling human vocal bursts are indispensable for developing robust and
general artificial intelligence. Exploring computational approaches for
understanding vocal bursts is attracting increasing research attention. In this
work, we propose a hierarchical framework, based on chain regression models,
for affective recognition from VBs, that explicitly considers multiple
relationships: (i) between emotional states and diverse cultures; (ii) between
low-dimensional (arousal & valence) and high-dimensional (10 emotion classes)
emotion spaces; and (iii) between various emotion classes within the
high-dimensional space. To address the challenge of data sparsity, we also use
self-supervised learning (SSL) representations with layer-wise and temporal
aggregation modules. The proposed systems participated in the ACII Affective
Vocal Burst (A-VB) Challenge 2022 and ranked first in the "TWO'' and "CULTURE''
tasks. Experimental results based on the ACII Challenge 2022 dataset
demonstrate the superior performance of the proposed system and the
effectiveness of considering multiple relationships using hierarchical
regression chain models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">Pretrain</span>ed Representations with Task-related Keywords for
  Alzheimer's Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchao Li, Kaitao Song, Junan Li, Bo Zheng, Dongsheng Li, Xixin Wu, Xunying Liu, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the global population aging rapidly, Alzheimer's disease (AD) is
particularly prominent in older adults, which has an insidious onset and leads
to a gradual, irreversible deterioration in cognitive domains (memory,
communication, etc.). Speech-based AD detection opens up the possibility of
widespread screening and timely disease intervention. Recent advances in
pre-trained models motivate AD detection modeling to shift from low-level
features to high-level representations. This paper presents several efficient
methods to extract better AD-related cues from high-level acoustic and
linguistic features. Based on these features, the paper also proposes a novel
task-oriented approach by modeling the relationship between the participants'
description and the cognitive task. Experiments are carried out on the ADReSS
dataset in a binary classification setup, and models are evaluated on the
unseen test set. Results and comparison with recent literature demonstrate the
efficiency and superior performance of proposed acoustic, linguistic and
task-oriented methods. The findings also show the importance of semantic and
syntactic information, and feasibility of automation and generalization with
the promising audio-only and task-oriented methods for the AD detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Beamforming at Terahertz Bands: Are Causal Representations the
  Way Forward? <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christo Kurisummoottil Thomas, Walid Saad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future wireless services, such as the metaverse require high information
rate, reliability, and low latency. Multi-user wireless systems can meet such
requirements by utilizing the abundant terahertz bandwidth with a massive
number of antennas, creating narrow beamforming solutions. However, existing
solutions lack proper modeling of channel dynamics, resulting in inaccurate
beamforming solutions in high-mobility scenarios. Herein, a dynamic,
semantically aware beamforming solution is proposed for the first time,
utilizing novel artificial intelligence algorithms in variational causal
inference to compute the time-varying dynamics of the causal representation of
multi-modal data and the beamforming. Simulations show that the proposed
causality-guided approach for Terahertz (THz) beamforming outperforms classical
MIMO beamforming techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep
  Ensembles are More Efficient than Single Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Ensembles are a simple, reliable, and effective method of improving both
the predictive performance and uncertainty estimates of deep learning
approaches. However, they are widely criticised as being computationally
expensive, due to the need to deploy multiple independent models. Recent work
has challenged this view, showing that for predictive accuracy, ensembles can
be more computationally efficient (at inference) than scaling single models
within an architecture family. This is achieved by cascading ensemble members
via an early-exit approach. In this work, we investigate extending these
efficiency gains to tasks related to uncertainty estimation. As many such
tasks, e.g. selective classification, are binary classification, our key novel
insight is to only pass samples within a window close to the binary decision
boundary to later cascade stages. Experiments on ImageNet-scale data across a
number of network architectures and uncertainty tasks show that the proposed
window-based early-exit approach is able to achieve a superior
uncertainty-computation trade-off compared to scaling single models. For
example, a cascaded EfficientNet-B2 ensemble is able to achieve similar
coverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.
We also find that cascades/ensembles give more reliable improvements on OOD
data vs scaling models up. Code for this work is available at:
https://github.com/Guoxoug/window-early-exit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the Needle in a Haystack: Unsupervised Rationale Extraction from
  Long Text Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Bujel, Andrew Caines, Helen Yannakoudakis, Marek Rei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-sequence transformers are designed to improve the representation of
longer texts by language models and their performance on downstream
document-level tasks. However, not much is understood about the quality of
token-level predictions in long-form models. We investigate the performance of
such architectures in the context of document classification with unsupervised
rationale extraction. We find standard soft attention methods to perform
significantly worse when combined with the Longformer language model. We
propose a compositional soft attention architecture that applies RoBERTa
sentence-wise to extract plausible rationales at the token-level. We find this
method to significantly outperform Longformer-driven baselines on sentiment
classification datasets, while also exhibiting significantly lower runtimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partial Neural Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milena Gazdieva, Alexander Korotin, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel neural method to compute partial optimal transport (OT)
maps, i.e., OT maps between parts of measures of the specified masses. We test
our partial neural optimal transport algorithm on synthetic examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practically Solving LPN in High Noise Regimes Faster Using Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Jiang, Kaiyue Wen, Yilei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct a systematic study of solving the learning parity with noise
problem (LPN) using neural networks. Our main contribution is designing
families of two-layer neural networks that practically outperform classical
algorithms in high-noise, low-dimension regimes. We consider three settings
where the numbers of LPN samples are abundant, very limited, and in between. In
each setting we provide neural network models that solve LPN as fast as
possible. For some settings we are also able to provide theories that explain
the rationale of the design of our models. Comparing with the previous
experiments of Esser, Kubler, and May (CRYPTO 2017), for dimension $n = 26$,
noise rate $\tau = 0.498$, the ''Guess-then-Gaussian-elimination'' algorithm
takes 3.12 days on 64 CPU cores, whereas our neural network algorithm takes 66
minutes on 8 GPUs. Our algorithm can also be plugged into the hybrid algorithms
for solving middle or large dimension LPN instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theory of Emergent In-Context Learning as Implicit Structure Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hahn, Navin Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling large language models (LLMs) leads to an emergent capacity to learn
in-context from example demonstrations. Despite progress, theoretical
understanding of this phenomenon remains limited. We argue that in-context
learning relies on recombination of compositional operations found in natural
language data. We derive an information-theoretic bound showing how in-context
learning abilities arise from generic next-token prediction when the
pretraining distribution has sufficient amounts of compositional structure,
under linguistically motivated assumptions. A second bound provides a
theoretical justification for the empirical success of prompting LLMs to output
intermediate steps towards an answer. To validate theoretical predictions, we
introduce a controlled setup for inducing in-context learning; unlike previous
approaches, it accounts for the compositional nature of language. Trained
transformers can perform in-context learning for a range of tasks, in a manner
consistent with the theoretical results. Mirroring real-world LLMs in a
miniature setup, in-context learning emerges when scaling parameters and data,
and models perform better when prompted to output intermediate steps. Probing
shows that in-context learning is supported by a representation of the input's
compositional structure. Taken together, these results provide a step towards
theoretical understanding of emergent behavior in large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Connection between Concept Drift and Uncertainty in Industrial
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus L. Lobo, Ibai Laña, Eneko Osaba, Javier Del Ser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-based digital twins are at the leading edge of the Industry 4.0
revolution, which are technologically empowered by the Internet of Things and
real-time data analysis. Information collected from industrial assets is
produced in a continuous fashion, yielding data streams that must be processed
under stringent timing constraints. Such data streams are usually subject to
non-stationary phenomena, causing that the data distribution of the streams may
change, and thus the knowledge captured by models used for data analysis may
become obsolete (leading to the so-called concept drift effect). The early
detection of the change (drift) is crucial for updating the model's knowledge,
which is challenging especially in scenarios where the ground truth associated
to the stream data is not readily available. Among many other techniques, the
estimation of the model's confidence has been timidly suggested in a few
studies as a criterion for detecting drifts in unsupervised settings. The goal
of this manuscript is to confirm and expose solidly the connection between the
model's confidence in its output and the presence of a concept drift,
showcasing it experimentally and advocating for a major consideration of
uncertainty estimation in comparative studies to be reported in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 1 figure, 2023 IEEE Conference on Artificial Intelligence
  (IEEE CAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Model Complexity for temporal tabular and multi-variate
  time series, case study with Numerai data science tournament 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wong, Prof. Mauricio Barahona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the use of different feature engineering and
dimensionality reduction methods in multi-variate time-series modelling. Using
a feature-target cross correlation time series dataset created from Numerai
tournament, we demonstrate under over-parameterised regime, both the
performance and predictions from different feature engineering methods converge
to the same equilibrium, which can be characterised by the reproducing kernel
Hilbert space. We suggest a new Ensemble method, which combines different
random non-linear transforms followed by ridge regression for modelling high
dimensional time-series. Compared to some commonly used deep learning models
for sequence modelling, such as LSTM and transformers, our method is more
robust (lower model variance over different random seeds and less sensitive to
the choice of architecture) and more efficient. An additional advantage of our
method is model simplicity as there is no need to use sophisticated deep
learning frameworks such as PyTorch. The learned feature rankings are then
applied to the temporal tabular prediction problem in the Numerai tournament,
and the predictive power of feature rankings obtained from our method is better
than the baseline prediction model based on moving averages
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Accented Speech Recognition with Multi-Domain Training <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Maison, Yannick Estève
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the rise of self-supervised learning, automatic speech recognition
(ASR) systems now achieve near-human performance on a wide variety of datasets.
However, they still lack generalization capability and are not robust to domain
shifts like accent variations. In this work, we use speech audio representing
four different French accents to create fine-tuning datasets that improve the
robustness of pre-trained ASR models. By incorporating various accents in the
training set, we obtain both in-domain and out-of-domain improvements. Our
numerical experiments show that we can reduce error rates by up to 25%
(relative) on African and Belgian accents compared to single-domain training
while keeping a good performance on standard French.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures. Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reachability Analysis of Neural Networks with Uncertain Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre-Jean Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature on reachability analysis methods for neural networks currently
only focuses on uncertainties on the network's inputs. In this paper, we
introduce two new approaches for the reachability analysis of neural networks
with additional uncertainties on their internal parameters (weight matrices and
bias vectors of each layer), which may open the field of formal methods on
neural networks to new topics, such as safe training or network repair. The
first and main method that we propose relies on existing reachability analysis
approach based on mixed monotonicity (initially introduced for dynamical
systems). The second proposed approach extends the ESIP (Error-based Symbolic
Interval Propagation) approach which was first implemented in the verification
tool Neurify, and first mentioned in the publication of the tool VeriNet.
Although the ESIP approach has been shown to often outperform the
mixed-monotonicity reachability analysis in the classical case with
uncertainties only on the network's inputs, we show in this paper through
numerical simulations that the situation is greatly reversed (in terms of
precision, computation time, memory usage, and broader applicability) when
dealing with uncertainties on the weights and biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-to-image <span class="highlight-title">Diffusion</span> Model in Generative AI: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey reviews text-to-image diffusion models in the context that
diffusion models have emerged to be popular for a wide range of generative
tasks. As a self-contained work, this survey starts with a brief introduction
of how a basic diffusion model works for image synthesis, followed by how
condition or guidance improves learning. Based on that, we present a review of
state-of-the-art methods on text-conditioned image synthesis, i.e.,
text-to-image. We further summarize applications beyond text-to-image
generation: text-guided creative generation and text-guided image editing.
Beyond the progress made so far, we discuss existing challenges and promising
future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First survey on the recent progress of text-to-image generation based
  on the diffusion model (under progress)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalised Scale-Space Properties for Probabilistic <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Peter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic diffusion models enjoy increasing popularity in the deep
learning community. They generate convincing samples from a learned
distribution of input images with a wide field of practical applications.
Originally, these approaches were motivated from drift-diffusion processes, but
these origins find less attention in recent, practice-oriented publications.
  We investigate probabilistic diffusion models from the viewpoint of
scale-space research and show that they fulfil generalised scale-space
properties on evolving probability distributions. Moreover, we discuss
similarities and differences between interpretations of the physical core
concept of drift-diffusion in the deep learning and model-based world. To this
end, we examine relations of probabilistic diffusion to osmosis filters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DualMix: Unleashing the Potential of Data Augmentation for Online
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Fan, Wenchao Xu, Haozhao Wang, Jiaqi Zhu, Junxiao Wang, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Class-Incremental (OCI) learning has sparked new approaches to expand
the previously trained model knowledge from sequentially arriving data streams
with new classes. Unfortunately, OCI learning can suffer from catastrophic
forgetting (CF) as the decision boundaries for old classes can become
inaccurate when perturbated by new ones. Existing literature have applied the
data augmentation (DA) to alleviate the model forgetting, while the role of DA
in OCI has not been well understood so far. In this paper, we theoretically
show that augmented samples with lower correlation to the original data are
more effective in preventing forgetting. However, aggressive augmentation may
also reduce the consistency between data and corresponding labels, which
motivates us to exploit proper DA to boost the OCI performance and prevent the
CF problem. We propose the Enhanced Mixup (EnMix) method that mixes the
augmented samples and their labels simultaneously, which is shown to enhance
the sample diversity while maintaining strong consistency with corresponding
labels. Further, to solve the class imbalance problem, we design an Adaptive
Mixup (AdpMix) method to calibrate the decision boundaries by mixing samples
from both old and new classes and dynamically adjusting the label mixing ratio.
Our approach is demonstrated to be effective on several benchmark datasets
through extensive experiments, and it is shown to be compatible with other
replay-based techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised
  Semantic Segmentation of Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Srinivas Prabakaran, Erik Ostrowski, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Semantic Segmentation (WSSS) with only image-level
supervision is a promising approach to deal with the need for Segmentation
networks, especially for generating a large number of pixel-wise masks in a
given dataset. However, most state-of-the-art image-level WSSS techniques lack
an understanding of the geometric features embedded in the images since the
network cannot derive any object boundary information from just image-level
labels. We define a boundary here as the line separating an object and its
background, or two different objects. To address this drawback, we propose our
novel BoundaryCAM framework, which deploys state-of-the-art class activation
maps combined with various post-processing techniques in order to achieve
fine-grained higher-accuracy segmentation masks. To achieve this, we
investigate a state-of-the-art unsupervised semantic segmentation network that
can be used to construct a boundary map, which enables BoundaryCAM to predict
object locations with sharper boundaries. By applying our method to WSSS
predictions, we were able to achieve up to 10% improvements even to the benefit
of the current state-of-the-art WSSS methods for medical imaging. The framework
is open-source and accessible online at
https://github.com/bharathprabakaran/BoundaryCAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network
  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Srinivas Prabakaran, Paul Hamelmann, Erik Ostrowski, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging is one of the most prominent technologies to evaluate the
growth, progression, and overall health of a fetus during its gestation.
However, the interpretation of the data obtained from such studies is best left
to expert physicians and technicians who are trained and well-versed in
analyzing such images. To improve the clinical workflow and potentially develop
an at-home ultrasound-based fetal monitoring platform, we present a novel fetus
phantom ultrasound dataset, FPUS23, which can be used to identify (1) the
correct diagnostic planes for estimating fetal biometric values, (2) fetus
orientation, (3) their anatomical features, and (4) bounding boxes of the fetus
phantom anatomies at 23 weeks gestation. The entire dataset is composed of
15,728 images, which are used to train four different Deep Neural Network
models, built upon a ResNet34 backbone, for detecting aforementioned fetus
features and use-cases. We have also evaluated the models trained using our
FPUS23 dataset, to show that the information learned by these models can be
used to substantially increase the accuracy on real-world ultrasound fetus
datasets. We make the FPUS23 dataset and the pre-trained models publicly
accessible at https://github.com/bharathprabakaran/FPUS23, which will further
facilitate future research on fetal ultrasound imaging and analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for Real-time Deployment of a Screening Tool for
  Depression Detection Using Actigraphy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajanikant Ghate, Nayan Kalnad, Rahee Walambe, Ketan Kotecha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated depression screening and diagnosis is a highly relevant problem
today. There are a number of limitations of the traditional depression
detection methods, namely, high dependence on clinicians and biased
self-reporting. In recent years, research has suggested strong potential in
machine learning (ML) based methods that make use of the user's passive data
collected via wearable devices. However, ML is data hungry. Especially in the
healthcare domain primary data collection is challenging. In this work, we
present an approach based on transfer learning, from a model trained on a
secondary dataset, for the real time deployment of the depression screening
tool based on the actigraphy data of users. This approach enables machine
learning modelling even with limited primary data samples. A modified version
of leave one out cross validation approach performed on the primary set
resulted in mean accuracy of 0.96, where in each iteration one subject's data
from the primary set was set aside for testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, conference, to be published in UKSIM23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-efficient Adversarial Imitation Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahuin Jung, Hyungyu Lee, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning, in which learning is performed by demonstration, has been
studied and advanced for sequential decision-making tasks in which a reward
function is not predefined. However, imitation learning methods still require
numerous expert demonstration samples to successfully imitate an expert's
behavior. To improve sample efficiency, we utilize self-supervised
representation learning, which can generate vast training signals from the
given data. In this study, we propose a self-supervised representation-based
adversarial imitation learning method to learn state and action representations
that are robust to diverse distortions and temporally predictive, on non-image
control tasks. In particular, in comparison with existing self-supervised
learning methods for tabular data, we propose a different corruption method for
state and action representations that is robust to diverse distortions. We
theoretically and empirically observe that making an informative feature
manifold with less sample complexity significantly improves the performance of
imitation learning. The proposed method shows a 39% relative improvement over
existing adversarial imitation learning methods on MuJoCo in a setting limited
to 100 expert state-action pairs. Moreover, we conduct comprehensive ablations
and additional experiments using demonstrations with varying optimality to
provide insights into a range of factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preliminary version of this manuscript was presented at Deep RL
  Workshop, NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinematic Data-Based Action Segmentation for Surgical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Goldbraikh, Omer Shubi, Or Rubin, Carla M Pugh, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action segmentation is a challenging task in high-level process analysis,
typically performed on video or kinematic data obtained from various sensors.
In the context of surgical procedures, action segmentation is critical for
workflow analysis algorithms. This work presents two contributions related to
action segmentation on kinematic data. Firstly, we introduce two multi-stage
architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for
kinematic data. The architectures consist of a prediction generator with
intra-stage regularization and Bidirectional LSTM or GRU-based refinement
stages. Secondly, we propose two new data augmentation techniques, World Frame
Rotation and Horizontal-Flip, which utilize the strong geometric structure of
kinematic data to improve algorithm performance and robustness. We evaluate our
models on three datasets of surgical suturing tasks: the Variable Tissue
Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)
Dataset, both of which are open surgery simulation datasets collected by us, as
well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a
well-known benchmark in robotic surgery. Our methods achieve state-of-the-art
performance on all benchmark datasets and establish a strong baseline for the
BRS dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICICLE: Interpretable Class Incremental Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawid Rymarczyk, Joost van de Weijer, Bartosz Zieliński, Bartłomiej Twardowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning enables incremental learning of new tasks without
forgetting those previously learned, resulting in positive knowledge transfer
that can enhance performance on both new and old tasks. However, continual
learning poses new challenges for interpretability, as the rationale behind
model predictions may change over time, leading to interpretability concept
drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical
part-based approach. It consists of three crucial novelties: interpretability
regularization that distills previously learned concepts while preserving
user-friendly positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-recency bias
compensation devoted to prototypical parts. Our experimental results
demonstrate that ICICLE reduces the interpretability concept drift and
outperforms the existing exemplar-free methods of common class-incremental
learning when applied to concept-based models. We make the code available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, code will be shared after the acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GAN</span>N: Graph Alignment Neural Network for Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxuan Song, Wenxuan Tu, Sihang Zhou, Xinwang Liu, En Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have been widely investigated in the field of
semi-supervised graph machine learning. Most methods fail to exploit adequate
graph information when labeled data is limited, leading to the problem of
oversmoothing. To overcome this issue, we propose the Graph Alignment Neural
Network (GANN), a simple and effective graph neural architecture. A unique
learning algorithm with three alignment rules is proposed to thoroughly explore
hidden information for insufficient labels. Firstly, to better investigate
attribute specifics, we suggest the feature alignment rule to align the inner
product of both the attribute and embedding matrices. Secondly, to properly
utilize the higher-order neighbor information, we propose the cluster center
alignment rule, which involves aligning the inner product of the cluster center
matrix with the unit matrix. Finally, to get reliable prediction results with
few labels, we establish the minimum entropy alignment rule by lining up the
prediction probability matrix with its sharpened result. Extensive studies on
graph benchmark datasets demonstrate that GANN can achieve considerable
benefits in semi-supervised node classification and outperform state-of-the-art
competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Causality for High Dimensional Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Jambulapati, Hilaf Hasson, Youngsuk Park, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining causal relationship between high dimensional observations are
among the most important tasks in scientific discoveries. In this paper, we
revisited the \emph{linear trace method}, a technique proposed
in~\citep{janzing2009telling,zscheischler2011testing} to infer the causal
direction between two random variables of high dimensions. We strengthen the
existing results significantly by providing an improved tail analysis in
addition to extending the results to nonlinear trace functionals with sharper
confidence bounds under certain distributional assumptions. We obtain our
results by interpreting the trace estimator in the causal regime as a function
over random orthogonal matrices, where the concentration of Lipschitz functions
over such space could be applied. We additionally propose a novel
ridge-regularized variant of the estimator in \cite{zscheischler2011testing},
and give provable bounds relating the ridge-estimated terms to their
ground-truth counterparts. We support our theoretical results with encouraging
experiments on synthetic datasets, more prominently, under high-dimension low
sample size regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBSCAN of Multi-Slice Clustering for three-order Tensor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dina Faneva Andriantsiory, Joseph Ben Geloun, Mustapha Lebbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several methods for triclustering three-dimensional data require the cluster
size or the number of clusters in each dimension to be specified. To address
this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal
slices that lie in a low dimensional subspace for a rank-one tensor dataset in
order to find a cluster based on the threshold similarity. We propose an
extension algorithm called MSC-DBSCAN to extract the different clusters of
slices that lie in the different subspaces from the data if the dataset is a
sum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC
algorithm and can find the same solution for rank-one tensor data as MSC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic4cast at NeurIPS 2022 -- Predict Dynamics along Graph Edges from
  Sparse Node Data: Whole City Traffic and ETA from Stationary Vehicle
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Neun, Christian Eichenberger, Henry Martin, Markus Spanring, Rahul Siripurapu, Daniel Springer, Leyan Deng, Chenwang Wu, Defu Lian, Min Zhou, Martin Lumiste, Andrei Ilie, Xinhua Wu, Cheng Lyu, Qing-Long Lu, Vishal Mahajan, Yichao Lu, Jiezhang Li, Junjun Li, Yue-Jiao Gong, Florian Grötschla, Joël Mathys, Ye Wei, He Haitao, Hui Fang, Kevin Malm, Fei Tang, Michael Kopp, David Kreil, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global trends of urbanization and increased personal mobility force us to
rethink the way we live and use urban space. The Traffic4cast competition
series tackles this problem in a data-driven way, advancing the latest methods
in machine learning for modeling complex spatial systems over time. In this
edition, our dynamic road graph data combine information from road maps,
$10^{12}$ probe data points, and stationary vehicle detectors in three cities
over the span of two years. While stationary vehicle detectors are the most
accurate way to capture traffic volume, they are only available in few
locations. Traffic4cast 2022 explores models that have the ability to
generalize loosely related temporal vertex data on just a few nodes to predict
dynamic future traffic states on the edges of the entire road graph. In the
core challenge, participants are invited to predict the likelihoods of three
congestion classes derived from the speed levels in the GPS data for the entire
road graph in three cities 15 min into the future. We only provide vehicle
count data from spatially sparse stationary vehicle detectors in these three
cities as model input for this task. The data are aggregated in 15 min time
bins for one hour prior to the prediction time. For the extended challenge,
participants are tasked to predict the average travel times on super-segments
15 min into the future - super-segments are longer sequences of road segments
in the graph. The competition results provide an important advance in the
prediction of complex city-wide traffic states just from publicly available
sparse vehicle data and without the need for large amounts of real-time
floating vehicle data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print under review, submitted to Proceedings of Machine Learning
  Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiway clustering of 3-order tensor via affinity matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dina Faneva Andriantsiory, Joseph Ben Geloun, Mustapha Lebbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method of multiway clustering for 3-order tensors via
affinity matrix (MCAM). Based on a notion of similarity between the tensor
slices and the spread of information of each slice, our model builds an
affinity/similarity matrix on which we apply advanced clustering methods. The
combination of all clusters of the three modes delivers the desired multiway
clustering. Finally, MCAM achieves competitive results compared with other
known algorithms on synthetics and real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForDigitStress: A multi-modal stress dataset employing a digital job
  interview scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Heimerl, Pooja Prajod, Silvan Mertes, Tobias Baur, Matthias Kraus, Ailin Liu, Helen Risack, Nicolas Rohleder, Elisabeth André, Linda Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-modal stress dataset that uses digital job interviews to
induce stress. The dataset provides multi-modal data of 40 participants
including audio, video (motion capturing, facial recognition, eye tracking) as
well as physiological information (photoplethysmography, electrodermal
activity). In addition to that, the dataset contains time-continuous
annotations for stress and occurred emotions (e.g. shame, anger, anxiety,
surprise). In order to establish a baseline, five different machine learning
classifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest,
Long-Short-Term Memory Network) have been trained and evaluated on the proposed
dataset for a binary stress classification task. The best-performing classifier
achieved an accuracy of 88.3% and an F1-score of 87.5%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can neural networks do arithmetic? A <span class="highlight-title">survey</span> on the elementary numerical
  skills of state-of-the-art deep learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Testolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating learning models that can exhibit sophisticated reasoning skills is
one of the greatest challenges in deep learning research, and mathematics is
rapidly becoming one of the target domains for assessing scientific progress in
this direction. In the past few years there has been an explosion of neural
network architectures, data sets, and benchmarks specifically designed to
tackle mathematical problems, reporting notable success in disparate fields
such as automated theorem proving, numerical integration, and discovery of new
conjectures or matrix multiplication algorithms. However, despite these
impressive achievements it is still unclear whether deep learning models
possess an elementary understanding of quantities and symbolic numbers. In this
survey we critically examine the recent literature, concluding that even
state-of-the-art architectures often fall short when probed with relatively
simple tasks designed to test basic numerical and arithmetic knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme
  Conversion <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungjun Kim, Changjin Han, Gyuhyeon Nam, Gyeongsu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework
that first transforms input sequences into character embeddings, obtains
linguistic information using language models, and then predicts the phonemes
based on global context about the entire input sequence. However, linguistic
knowledge alone is often inadequate. Language models frequently encode overly
general structures of a sentence and fail to cover specific cases needed to use
phonetic knowledge. Also, a handcrafted post-processing system is needed to
address the problems relevant to the tone of the characters. However, the
system exhibits inconsistency in the segmentation of word boundaries which
consequently degrades the performance of the G2P system. To address these
issues, we propose the Reinforcer that provides strong inductive bias for
language models by emphasizing the phonological information between neighboring
characters to help disambiguate pronunciations. Experimental results show that
the Reinforcer boosts the cutting-edge architectures by a large margin. We also
combine the Reinforcer with a large-scale pre-trained model and demonstrate the
validity of using neighboring context in knowledge transfer scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisCoHead: Audio-and-Video-Driven Talking Head Generation by
  Disentangled Control of Head Pose and Facial Expressions <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geumbyeol Hwang, Sunwon Hong, Seunghyun Lee, Sungwoo Park, Gyeongsu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For realistic talking head generation, creating natural head motion while
maintaining accurate lip synchronization is essential. To fulfill this
challenging task, we propose DisCoHead, a novel method to disentangle and
control head pose and facial expressions without supervision. DisCoHead uses a
single geometric transformation as a bottleneck to isolate and extract head
motion from a head-driving video. Either an affine or a thin-plate spline
transformation can be used and both work well as geometric bottlenecks. We
enhance the efficiency of DisCoHead by integrating a dense motion estimator and
the encoder of a generator which are originally separate modules. Taking a step
further, we also propose a neural mix approach where dense motion is estimated
and applied implicitly by the encoder. After applying the disentangled head
motion to a source identity, DisCoHead controls the mouth region according to
speech audio, and it blinks eyes and moves eyebrows following a separate
driving video of the eye region, via the weight modulation of convolutional
neural networks. The experiments using multiple datasets show that DisCoHead
successfully generates realistic audio-and-video-driven talking heads and
outperforms state-of-the-art methods. Project page:
https://deepbrainai-research.github.io/discohead/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Policy Learning for Offline-to-Online Reinforcement Learning <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional reinforcement learning (RL) needs an environment to collect
fresh data, which is impractical when online interactions are costly. Offline
RL provides an alternative solution by directly learning from the previously
collected dataset. However, it will yield unsatisfactory performance if the
quality of the offline datasets is poor. In this paper, we consider an
offline-to-online setting where the agent is first learned from the offline
dataset and then trained online, and propose a framework called Adaptive Policy
Learning for effectively taking advantage of offline and online data.
Specifically, we explicitly consider the difference between the online and
offline data and apply an adaptive update scheme accordingly, that is, a
pessimistic update strategy for the offline dataset and an optimistic/greedy
update scheme for the online dataset. Such a simple and effective method
provides a way to mix the offline and online RL and achieve the best of both
worlds. We further provide two detailed algorithms for implementing the
framework through embedding value or policy-based RL algorithms into it.
Finally, we conduct extensive experiments on popular continuous control tasks,
and results show that our algorithm can learn the expert policy with high
sample efficiency even when the quality of offline dataset is poor, e.g.,
random dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPTN: Fast Pure <span class="highlight-title">Transformer</span> Network for Traffic Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Zhang, Junjie Tang, Juncheng Jin, Zehui Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic flow forecasting is challenging due to the intricate spatio-temporal
correlations in traffic flow data. Existing Transformer-based methods usually
treat traffic flow forecasting as multivariate time series (MTS) forecasting.
However, too many sensors can cause a vector with a dimension greater than 800,
which is difficult to process without information loss. In addition, these
methods design complex mechanisms to capture spatial dependencies in MTS,
resulting in slow forecasting speed. To solve the abovementioned problems, we
propose a Fast Pure Transformer Network (FPTN) in this paper. First, the
traffic flow data are divided into sequences along the sensor dimension instead
of the time dimension. Then, to adequately represent complex spatio-temporal
correlations, Three types of embeddings are proposed for projecting these
vectors into a suitable vector space. After that, to capture the complex
spatio-temporal correlations simultaneously in these vectors, we utilize
Transformer encoder and stack it with several layers. Extensive experiments are
conducted with 4 real-world datasets and 13 baselines, which demonstrate that
FPTN outperforms the state-of-the-art on two metrics. Meanwhile, the
computational time of FPTN spent is less than a quarter of other
state-of-the-art Transformer-based models spent, and the requirements for
computing resources are significantly reduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature representations useful for predicting image memorability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumi Harada, Hiroyuki Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting image memorability has attracted interest in various fields.
Consequently, prediction accuracy with convolutional neural network (CNN)
models has been approaching the empirical upper bound estimated based on human
consistency. However, identifying which feature representations embedded in CNN
models are responsible for such high prediction accuracy of memorability
remains an open question. To tackle this problem, this study sought to identify
memorability-related feature representations in CNN models using brain
similarity. Specifically, memorability prediction accuracy and brain similarity
were examined and assessed by Brain-Score across 16,860 layers in 64 CNN models
pretrained for object recognition. A clear tendency was shown in this
comprehensive analysis that layers with high memorability prediction accuracy
had higher brain similarity with the inferior temporal (IT) cortex, which is
the highest stage in the ventral visual pathway. Furthermore, fine-tuning the
64 CNN models revealed that brain similarity with the IT cortex at the
penultimate layer was positively correlated with memorability prediction
accuracy. This analysis also showed that the best fine-tuned model provided
accuracy comparable to the state-of-the-art CNN models developed specifically
for memorability prediction. Overall, this study's results indicated that the
CNN models' great success in predicting memorability relies on feature
representation acquisition similar to the IT cortex. This study advanced our
understanding of feature representations and its use for predicting image
memorability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems
  Using Optimal Transport <span class="chip">NeurIPS 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mukul Bhutani, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting how distributions over discrete variables vary over time is a
common task in time series forecasting. But whereas most approaches focus on
merely predicting the distribution at subsequent time steps, a crucial piece of
information in many settings is to determine how this probability mass flows
between the different elements over time. We propose a new approach to
predicting such mass flow over time using optimal transport. Specifically, we
propose a generic approach to predicting transport matrices in end-to-end deep
learning systems, replacing the standard softmax operation with Sinkhorn
iterations. We apply our approach to the task of predicting how communities
will evolve over time in social network settings, and show that the approach
improves substantially over alternative prediction methods. We specifically
highlight results on the task of predicting faction evolution in Ukrainian
parliamentary voting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A prior version of the work appeared in the Optimal Transport
  Workshop at NeurIPS 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koos Classification of Vestibular Schwannoma via Image Translation-Based
  Unsupervised Cross-Modality Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Yang, Lisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koos grading scale is a classification system for vestibular schwannoma
(VS) used to characterize the tumor and its effects on adjacent brain
structures. The Koos classification captures many of the characteristics of
treatment deci-sions and is often used to determine treatment plans. Although
both contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2)
scanning can be used for Koos Classification, hrT2 scanning is gaining interest
because of its higher safety and cost-effectiveness. However, in the absence of
annotations for hrT2 scans, deep learning methods often inevitably suffer from
performance deg-radation due to unsupervised learning. If ceT1 scans and their
annotations can be used for unsupervised learning of hrT2 scans, the
performance of Koos classifi-cation using unlabeled hrT2 scans will be greatly
improved. In this regard, we propose an unsupervised cross-modality domain
adaptation method based on im-age translation by transforming annotated ceT1
scans into hrT2 modality and us-ing their annotations to achieve supervised
learning of hrT2 modality. Then, the VS and 7 adjacent brain structures related
to Koos classification in hrT2 scans were segmented. Finally, handcrafted
features are extracted from the segmenta-tion results, and Koos grade is
classified using a random forest classifier. The proposed method received rank
1 on the Koos classification task of the Cross-Modality Domain Adaptation
(crossMoDA 2022) challenge, with Macro-Averaged Mean Absolute Error (MA-MAE) of
0.2148 for the validation set and 0.26 for the test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph
  Neural Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Cao, Jiaxuan You, Jiaju Liu, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AutoML has demonstrated remarkable success in finding an effective neural
architecture for a given machine learning task defined by a specific dataset
and an evaluation metric. However, most present AutoML techniques consider each
task independently from scratch, which requires exploring many architectures,
leading to high computational cost. Here we propose AutoTransfer, an AutoML
solution that improves search efficiency by transferring the prior
architectural design knowledge to the novel task of interest. Our key
innovation includes a task-model bank that captures the model performance over
a diverse set of GNN architectures and tasks, and a computationally efficient
task embedding that can accurately measure the similarity among different
tasks. Based on the task-model bank and the task embeddings, we estimate the
design priors of desirable models of the novel task, by aggregating a
similarity-weighted sum of the top-K design distributions on tasks that are
similar to the task of interest. The computed design priors can be used with
any AutoML search algorithm. We evaluate AutoTransfer on six datasets in the
graph machine learning domain. Experiments demonstrate that (i) our proposed
task embedding can be computed efficiently, and that tasks with similar
embeddings have similar best-performing architectures; (ii) AutoTransfer
significantly improves search efficiency with the transferred design priors,
reducing the number of explored architectures by an order of magnitude.
Finally, we release GNN-Bank-101, a large-scale dataset of detailed GNN
training information of 120,000 task-model combinations to facilitate and
inspire future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Multi-Task Learning: Modeling Relations between Data and
  Tasks <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Cao, Jiaxuan You, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key assumption in multi-task learning is that at the inference time the
multi-task model only has access to a given data point but not to the data
point's labels from other tasks. This presents an opportunity to extend
multi-task learning to utilize data point's labels from other auxiliary tasks,
and this way improves performance on the new task. Here we introduce a novel
relational multi-task learning setting where we leverage data point labels from
auxiliary tasks to make more accurate predictions on the new task. We develop
MetaLink, where our key innovation is to build a knowledge graph that connects
data points and tasks and thus allows us to leverage labels from auxiliary
tasks. The knowledge graph consists of two types of nodes: (1) data nodes,
where node features are data embeddings computed by the neural network, and (2)
task nodes, with the last layer's weights for each task as node features. The
edges in this knowledge graph capture data-task relationships, and the edge
label captures the label of a data point on a particular task. Under MetaLink,
we reformulate the new task as a link label prediction problem between a data
node and a task node. The MetaLink framework provides flexibility to model
knowledge transfer from auxiliary task labels to the task of interest. We
evaluate MetaLink on 6 benchmark datasets in both biochemical and vision
domains. Experiments demonstrate that MetaLink can successfully utilize the
relations among different tasks, outperforming the state-of-the-art methods
under the proposed relational multi-task learning setting, with up to 27%
improvement in ROC AUC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2022 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Normalization for Robust Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bilal Faye, Mohamed-Djallel Dilmi, Hanane Azzag, Mustapha Lebbah, Fangchen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalization is a pre-processing step that converts the data into a more
usable representation. As part of the deep neural networks (DNNs), the batch
normalization (BN) technique uses normalization to address the problem of
internal covariate shift. It can be packaged as general modules, which have
been extensively integrated into various DNNs, to stabilize and accelerate
training, presumably leading to improved generalization. However, the effect of
BN is dependent on the mini-batch size and it does not take into account any
groups or clusters that may exist in the dataset when estimating population
statistics. This study proposes a new normalization technique, called context
normalization, for image data. This approach adjusts the scaling of features
based on the characteristics of each sample, which improves the model's
convergence speed and performance by adapting the data values to the context of
the target task. The effectiveness of context normalization is demonstrated on
various datasets, and its performance is compared to other standard
normalization techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advances and Applications of Machine Learning in Experimental
  Solid Mechanics: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxun Jin, Enrui Zhang, Horacio D. Espinosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many decades, experimental solid mechanics has played a crucial role in
characterizing and understanding the mechanical properties of natural and novel
materials. Recent advances in machine learning (ML) provide new opportunities
for the field, including experimental design, data analysis, uncertainty
quantification, and inverse problems. As the number of papers published in
recent years in this emerging field is exploding, it is timely to conduct a
comprehensive and up-to-date review of recent ML applications in experimental
solid mechanics. Here, we first provide an overview of common ML algorithms and
terminologies that are pertinent to this review, with emphasis placed on
physics-informed and physics-based ML methods. Then, we provide thorough
coverage of recent ML applications in traditional and emerging areas of
experimental mechanics, including fracture mechanics, biomechanics, nano- and
micro-mechanics, architected materials, and 2D material. Finally, we highlight
some current challenges of applying ML to multi-modality and multi-fidelity
experimental datasets and propose several future research directions. This
review aims to provide valuable insights into the use of ML methods as well as
a variety of examples for researchers in solid mechanics to integrate into
their experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>76 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering with Simplicial Complexes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thummaluru Siddartha Reddy, Sundeep Prabhakar Chepuri, Pierre Borgnat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a new clustering algorithm to group nodes in
networks based on second-order simplices (aka filled triangles) to leverage
higher-order network interactions. We define a simplicial conductance function,
which on minimizing, yields an optimal partition with a higher density of
filled triangles within the set while the density of filled triangles is
smaller across the sets. To this end, we propose a simplicial adjacency
operator that captures the relation between the nodes through second-order
simplices. This allows us to extend the well-known Cheeger inequality to
cluster a simplicial complex. Then, leveraging the Cheeger inequality, we
propose the simplicial spectral clustering algorithm. We report results from
numerical experiments on synthetic and real-world network data to demonstrate
the efficacy of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best arm identification in rare events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirban Bhattacharjee, Sushant Vijayan, Sandeep K Juneja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the best arm identification problem in the stochastic multi-armed
bandit framework where each arm has a tiny probability of realizing large
rewards while with overwhelming probability the reward is zero. A key
application of this framework is in online advertising where click rates of
advertisements could be a fraction of a single percent and final conversion to
sales, while highly profitable, may again be a small fraction of the click
rates. Lately, algorithms for BAI problems have been developed that minimise
sample complexity while providing statistical guarantees on the correct arm
selection. As we observe, these algorithms can be computationally prohibitive.
We exploit the fact that the reward process for each arm is well approximated
by a Compound Poisson process to arrive at algorithms that are faster, with a
small increase in sample complexity. We analyze the problem in an asymptotic
regime as rarity of reward occurrence reduces to zero, and reward amounts
increase to infinity. This helps illustrate the benefits of the proposed
algorithm. It also sheds light on the underlying structure of the optimal BAI
algorithms in the rare event setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via
  Language-Based Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souradip Chakraborty, Kasun Weerakoon, Prithvi Poddar, Pratap Tokekar, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning-based policies for continuous control robotic
navigation tasks often fail to adapt to changes in the environment during
real-time deployment, which may result in catastrophic failures. To address
this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest
help and \textbf{MOVE} on), which uses language-based feedback to adjust
trained policies to real-time changes in the environment. In this work, we
enable the trained policy to decide \emph{when to ask for feedback} and
\emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates
epistemic uncertainty to determine the optimal time to request feedback from
humans and uses language-based feedback for real-time adaptation. We perform
extensive synthetic and real-world evaluations to demonstrate the benefits of
our proposed approach in several test-time dynamic navigation scenarios. Our
approach enable robots to learn from human feedback and adapt to previously
unseen adversarial situations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Implicit Geometry of Cross-Entropy Parameterizations for
  Label-Imbalanced Data <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Behnia, Ganesh Ramachandra Kini, Vala Vakilian, Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various logit-adjusted parameterizations of the cross-entropy (CE) loss have
been proposed as alternatives to weighted CE for training large models on
label-imbalanced data far beyond the zero train error regime. The driving force
behind those designs has been the theory of implicit bias, which for
linear(ized) models, explains why they successfully induce bias on the
optimization path towards solutions that favor minorities. Aiming to extend
this theory to non-linear models, we investigate the implicit geometry of
classifiers and embeddings that are learned by different CE parameterizations.
Our main result characterizes the global minimizers of a non-convex
cost-sensitive SVM classifier for the unconstrained features model, which
serves as an abstraction of deep nets. We derive closed-form formulas for the
angles and norms of classifiers and embeddings as a function of the number of
classes, the imbalance and the minority ratios, and the loss hyperparameters.
Using these, we show that logit-adjusted parameterizations can be appropriately
tuned to learn symmetric geometries irrespective of the imbalance ratio. We
complement our analysis with experiments and an empirical study of convergence
accuracy in deep-nets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short version of this accepted at AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC)
  Countries using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leila Ismail, Huned Materwala, Alain Hennebelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  COVID-19 has infected more than 68 million people worldwide since it was
first detected about a year ago. Machine learning time series models have been
implemented to forecast COVID-19 infections. In this paper, we develop time
series models for the Gulf Cooperation Council (GCC) countries using the public
COVID-19 dataset from Johns Hopkins. The dataset set includes the one-year
cumulative COVID-19 cases between 22/01/2020 to 22/01/2021. We developed
different models for the countries under study based on the spatial
distribution of the infection data. Our experimental results show that the
developed models can forecast COVID-19 infections with high precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Proceedings of the 13th International Conference on Computer
  Modeling and Simulation, ICCMS 2021, Autoregressive integrated moving
  average, ARIMA, Coronavirus, COVID-19, Damped Trend, Holt Linear Trend,
  Machine learning, Pandemic, Time series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Contrastive Knowledge Transfer Framework for Model Compression and
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqi Zhao, Yitao Chen, Ming Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Transfer (KT) achieves competitive performance and is widely used
for image classification tasks in model compression and transfer learning.
Existing KT works transfer the information from a large model ("teacher") to
train a small model ("student") by minimizing the difference of their
conditionally independent output distributions. However, these works overlook
the high-dimension structural knowledge from the intermediate representations
of the teacher, which leads to limited effectiveness, and they are motivated by
various heuristic intuitions, which makes it difficult to generalize. This
paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which
enables the transfer of sufficient structural knowledge from the teacher to the
student by optimizing multiple contrastive objectives across the intermediate
representations between them. Also, CKTF provides a generalized agreement to
existing KT techniques and increases their performance significantly by
deriving them as specific cases of CKTF. The extensive evaluation shows that
CKTF consistently outperforms the existing KT works by 0.04% to 11.59% in model
compression and by 0.4% to 4.75% in transfer learning on various models and
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdPE: Adversarial Positional Embeddings for <span class="highlight-title">Pretrain</span>ing Vision
  <span class="highlight-title">Transformer</span>s via MAE+ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Ying Wang, Ziwei Xuan, Guo-Jun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning of vision transformers seeks to pretrain an encoder via
pretext tasks without labels. Among them is the Masked Image Modeling (MIM)
aligned with pretraining of language transformers by predicting masked patches
as a pretext task. A criterion in unsupervised pretraining is the pretext task
needs to be sufficiently hard to prevent the transformer encoder from learning
trivial low-level features not generalizable well to downstream tasks. For this
purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It
distorts the local visual structures by perturbing the position encodings so
that the learned transformer cannot simply use the locally correlated patches
to predict the missing ones. We hypothesize that it forces the transformer
encoder to learn more discriminative features in a global context with stronger
generalizability to downstream tasks. We will consider both absolute and
relative positional encodings, where adversarial positions can be imposed both
in the embedding mode and the coordinate mode. We will also present a new MAE+
baseline that brings the performance of the MIM pretraining to a new level with
the AdPE. The experiments demonstrate that our approach can improve the
fine-tuning accuracy of MAE by $0.8\%$ and $0.4\%$ over 1600 epochs of
pretraining ViT-B and ViT-L on Imagenet1K. For the transfer learning task, it
outperforms the MAE with the ViT-B backbone by $2.6\%$ in mIoU on ADE20K, and
by $3.2\%$ in AP$^{bbox}$ and $1.6\%$ in AP$^{mask}$ on COCO, respectively.
These results are obtained with the AdPE being a pure MIM approach that does
not use any extra models or external datasets for pretraining. The code is
available at https://github.com/maple-research-lab/AdPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Regularized Discrete Optimal Transport with Group-Sparse
  Regularizers <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasutoshi Ida, Sekitoshi Kanai, Kazuki Adachi, Atsutoshi Kumagai, Yasuhiro Fujiwara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized discrete optimal transport (OT) is a powerful tool to measure the
distance between two discrete distributions that have been constructed from
data samples on two different domains. While it has a wide range of
applications in machine learning, in some cases the sampled data from only one
of the domains will have class labels such as unsupervised domain adaptation.
In this kind of problem setting, a group-sparse regularizer is frequently
leveraged as a regularization term to handle class labels. In particular, it
can preserve the label structure on the data samples by corresponding the data
samples with the same class label to one group-sparse regularization term. As a
result, we can measure the distance while utilizing label information by
solving the regularized optimization problem with gradient-based algorithms.
However, the gradient computation is expensive when the number of classes or
data samples is large because the number of regularization terms and their
respective sizes also turn out to be large. This paper proposes fast discrete
OT with group-sparse regularizers. Our method is based on two ideas. The first
is to safely skip the computations of the gradients that must be zero. The
second is to efficiently extract the gradients that are expected to be nonzero.
Our method is guaranteed to return the same value of the objective function as
that of the original method. Experiments show that our method is up to 8.6
times faster than the original method without degrading accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of the paper accepted by the 37th AAAI
  Conference on Artificial Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential three-way decisions with a single hidden layer feedforward
  neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youxi Wu, Shuhui Cheng, Yan Li, Rongjie Lv, Fan Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The three-way decisions strategy has been employed to construct network
topology in a single hidden layer feedforward neural network (SFNN). However,
this model has a general performance, and does not consider the process costs,
since it has fixed threshold parameters. Inspired by the sequential three-way
decisions (STWD), this paper proposes STWD with an SFNN (STWD-SFNN) to enhance
the performance of networks on structured datasets. STWD-SFNN adopts
multi-granularity levels to dynamically learn the number of hidden layer nodes
from coarse to fine, and set the sequential threshold parameters. Specifically,
at the coarse granular level, STWD-SFNN handles easy-to-classify instances by
applying strict threshold conditions, and with the increasing number of hidden
layer nodes at the fine granular level, STWD-SFNN focuses more on disposing of
the difficult-to-classify instances by applying loose threshold conditions,
thereby realizing the classification of instances. Moreover, STWD-SFNN
considers and reports the process cost produced from each granular level. The
experimental results verify that STWD-SFNN has a more compact network on
structured datasets than other SFNN models, and has better generalization
performance than the competitive models. All models and datasets can be
downloaded from https://github.com/wuc567/Machine-learning/tree/main/STWD-SFNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teacher-Student Knowledge Distillation for Radar Perception on Embedded
  Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Shaw, Kanishka Tyagi, Shan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many radar signal processing methodologies are being developed for critical
road safety perception tasks. Unfortunately, these signal processing algorithms
are often poorly suited to run on embedded hardware accelerators used in
automobiles. Conversely, end-to-end machine learning (ML) approaches better
exploit the performance gains brought by specialized accelerators. In this
paper, we propose a teacher-student knowledge distillation approach for
low-level radar perception tasks. We utilize a hybrid model for stationary
object detection as a teacher to train an end-to-end ML student model. The
student can efficiently harness embedded compute for real-time deployment. We
demonstrate that the proposed student model runs at speeds 100x faster than the
teacher model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted at ASILOMAR,2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensitive Region-based Metamorphic Testing Framework using Explainable
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuma Torikoshi, Yasuharu Nishi, Juichi Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) is one of the most popular research topics in machine
learning and DL-driven image recognition systems have developed rapidly. Recent
research has used metamorphic testing (MT) to detect misclassified images. Most
of them discuss metamorphic relations (MR), with little discussion on which
regions should be transformed. We focus on the fact that there are sensitive
regions where even a small transformation can easily change the prediction
results and propose an MT framework that efficiently tests for regions prone to
misclassification by transforming the sensitive regions. Our evaluation showed
that the sensitive regions can be specified by Explainable AI (XAI) and our
framework effectively detects faults.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VANI: Very-lightweight Accent-controllable TTS for Native and Non-native
  speakers with Identity Preservation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Badlani, Akshit Arora, Subhankar Ghosh, Rafael Valle, Kevin J. Shih, João Felipe Santos, Boris Ginsburg, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VANI, a very lightweight multi-lingual accent controllable
speech synthesis system. Our model builds upon disentanglement strategies
proposed in RADMMM and supports explicit control of accent, language, speaker
and fine-grained $F_0$ and energy features for speech synthesis. We utilize the
Indic languages dataset, released for LIMMITS 2023 as part of ICASSP Signal
Processing Grand Challenge, to synthesize speech in 3 different languages. Our
model supports transferring the language of a speaker while retaining their
voice and the native accent of the target language. We utilize the
large-parameter RADMMM model for Track $1$ and lightweight VANI model for Track
$2$ and $3$ of the competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presentation accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Computer Vision Applications for Spatial AI Object
  Recognition in Orange County, California 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostas Alexandridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide an integrated and systematic automation approach to spatial object
recognition and positional detection using AI machine learning and computer
vision algorithms for Orange County, California. We describe a comprehensive
methodology for multi-sensor, high-resolution field data acquisition, along
with post-field processing and pre-analysis processing tasks. We developed a
series of algorithmic formulations and workflows that integrate convolutional
deep neural network learning with detected object positioning estimation in
360{\deg} equirectancular photosphere imagery. We provide examples of
application processing more than 800 thousand cardinal directions in
photosphere images across two areas in Orange County, and present detection
results for stop-sign and fire hydrant object recognition. We discuss the
efficiency and effectiveness of our approach, along with broader inferences
related to the performance and implications of this approach for future
technological innovations, including automation of spatial data and public
asset inventories, and near real-time AI field data systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 15 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong Learning for Anomaly Detection: New Challenges, Perspectives,
  and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Faber, Roberto Corizzo, Bartlomiej Sniezynski, Nathalie Japkowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is of paramount importance in many real-world domains,
characterized by evolving behavior. Lifelong learning represents an emerging
trend, answering the need for machine learning models that continuously adapt
to new challenges in dynamic environments while retaining past knowledge.
However, limited efforts are dedicated to building foundations for lifelong
anomaly detection, which provides intrinsically different challenges compared
to the more widely explored classification setting. In this paper, we face this
issue by exploring, motivating, and discussing lifelong anomaly detection,
trying to build foundations for its wider adoption. First, we explain why
lifelong anomaly detection is relevant, defining challenges and opportunities
to design anomaly detection methods that deal with lifelong learning
complexities. Second, we characterize learning settings and a scenario
generation procedure that enables researchers to experiment with lifelong
anomaly detection using existing datasets. Third, we perform experiments with
popular anomaly detection methods on proposed lifelong scenarios, emphasizing
the gap in performance that could be gained with the adoption of lifelong
learning. Overall, we conclude that the adoption of lifelong anomaly detection
is important to design more robust models that provide a comprehensive view of
the environment, as well as simultaneous adaptation and knowledge retention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging Decision <span class="highlight-title">Transformer</span>s: Weight Averaging for Forming Multi-Task
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Lawson, Ahmed H. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown the promise of creating generalist, transformer-based,
policies for language, vision, and sequential decision-making problems. To
create such models, we generally require centralized training objectives, data,
and compute. It is of interest if we can more flexibly create generalist
policies, by merging together multiple, task-specific, individually trained
policies. In this work, we take a preliminary step in this direction through
merging, or averaging, subsets of Decision Transformers in weight space trained
on different MuJoCo locomotion problems, forming multi-task models without
centralized training. We also propose that when merging policies, we can obtain
better results if all policies start from common, pre-trained initializations,
while also co-training on shared auxiliary tasks during problem-specific
finetuning. In general, we believe research in this direction can help
democratize and distribute the process of which forms generally capable agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Adversarial Learning and its applicability to Automated
  Software Testing: a systematic <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Vitorino, Tiago Dias, Tiago Fonseca, Eva Maia, Isabel Praça
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Every novel technology adds hidden vulnerabilities ready to be exploited by a
growing number of cyber-attacks. Automated software testing can be a promising
solution to quickly analyze thousands of lines of code by generating and
slightly modifying function-specific testing data to encounter a multitude of
vulnerabilities and attack vectors. This process draws similarities to the
constrained adversarial examples generated by adversarial learning methods, so
there could be significant benefits to the integration of these methods in
automated testing tools. Therefore, this systematic review is focused on the
current state-of-the-art of constrained data generation methods applied for
adversarial learning and software testing, aiming to guide researchers and
developers to enhance testing tools with adversarial learning methods and
improve the resilience and robustness of their digital systems. The found
constrained data generation applications for adversarial machine learning were
systematized, and the advantages and limitations of approaches specific for
software testing were thoroughly analyzed, identifying research gaps and
opportunities to improve testing tools with adversarial attack methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 5 tables, 2 figures, Information and Software Technology
  journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WDiscOOD: Out-of-Distribution Detection via Whitened Linear
  Discriminative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiye Chen, Yunzhi Lin, Ruinian Xu, Patricio A. Vela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are susceptible to generating overconfident yet
erroneous predictions when presented with data beyond known concepts. This
challenge underscores the importance of detecting out-of-distribution (OOD)
samples in the open world. In this work, we propose a novel feature-space OOD
detection score that jointly reasons with both class-specific and
class-agnostic information. Specifically, our approach utilizes Whitened Linear
Discriminative Analysis to project features into two subspaces - the
discriminative and residual subspaces - in which the ID classes are maximally
separated and closely clustered, respectively. The OOD score is then determined
by combining the deviation from the input data to the ID distribution in both
subspaces. The efficacy of our method, named WDiscOOD, is verified on the
large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety
of distribution shifts. WDiscOOD demonstrates superior performance on deep
classifiers with diverse backbone architectures, including CNN and vision
transformer. Furthermore, we also show that our method can more effectively
detect novel concepts in representation space trained with contrastive
objectives, including supervised contrastive loss and multi-modality
contrastive loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial
  Wedge Pressure from Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasun C. Tripathi, Mohammod N. I. Suvon, Lawrence Schobs, Shuo Zhou, Samer Alabed, Andrew J. Swift, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart failure is a serious and life-threatening condition that can lead to
elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure
(PAWP) is an important surrogate marker indicating high pressure in the left
ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an
invasive procedure. A non-invasive method is useful in quickly identifying
high-risk patients from a large population. In this work, we develop a tensor
learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic
Resonance Imaging (MRI). This pipeline extracts spatial and temporal features
from high-dimensional scans. For quality control, we incorporate an epistemic
uncertainty-based binning strategy to identify poor-quality training samples.
To improve the performance, we learn complementary information by integrating
features from multimodal data: cardiac MRI with short-axis and four-chamber
views, and Electronic Health Records. The experimental analysis on a large
cohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation
indicates that the proposed pipeline has a diagnostic value and can produce
promising performance with significant improvement over the baseline in
clinical practice (i.e., $\Delta$AUC $=0.10$, $\Delta$Accuracy $=0.06$, and
$\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical
utility of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Synthesis of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Whitehouse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks adapt very well to distributed and continuous
representations, but struggle to generalize from small amounts of data.
Symbolic systems commonly achieve data efficient generalization by exploiting
modularity to benefit from local and discrete features of a representation.
These features allow symbolic programs to be improved one module at a time and
to experience combinatorial growth in the values they can successfully process.
However, it is difficult to design a component that can be used to form
symbolic abstractions and which is adequately overparametrized to learn
arbitrary high-dimensional transformations. I present Graph-based Symbolically
Synthesized Neural Networks (G-SSNNs), a class of neural modules that operate
on representations modified with synthesized symbolic programs to include a
fixed set of local and discrete features. I demonstrate that the choice of
injected features within a G-SSNN module modulates the data efficiency and
generalization of baseline neural models, creating predictable patterns of both
heightened and curtailed generalization. By training G-SSNNs, we also derive
information about desirable semantics of symbolic programs without manual
engineering. This information is compact and amenable to abstraction, but can
also be flexibly recontextualized for other high-dimensional settings. In
future work, I will investigate data efficient generalization and the
transferability of learned symbolic representations in more complex G-SSNN
designs based on more complex classes of symbolic programs. Experimental code
and data are available at
https://github.com/shlomenu/symbolically_synthesized_networks .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure. Minor formula correction and minor textual
  revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expectation Distance-based Distributional Clustering for
  Noise-Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08871v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08871v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahmat Adesunkanmi, Ratnesh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a clustering technique that reduces the susceptibility to
data noise by learning and clustering the data-distribution and then assigning
the data to the cluster of its distribution. In the process, it reduces the
impact of noise on clustering results. This method involves introducing a new
distance among distributions, namely the expectation distance (denoted, ED),
that goes beyond the state-of-art distribution distance of optimal mass
transport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends
only on the marginal distributions while the former also employs the
information about the joint distributions. Using the ED, the paper extends the
classical $K$-means and $K$-medoids clustering to those over data-distributions
(rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also
presents the closed-form expressions of the $W_2$ and ED distance measures. The
implementation results of the proposed ED and the $W_2$ distance measures to
cluster real-world weather data as well as stock data are also presented, which
involves efficiently extracting and using the underlying data distributions --
Gaussians for weather data versus lognormals for stock data. The results show
striking performance improvement over classical clustering of raw-data, with
higher accuracy realized for ED. Also, not only does the distribution-based
clustering offer higher accuracy, but it also lowers the computation time due
to reduced time-complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingvar Ziemann, Henrik Sandberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TWe establish regret lower bounds for adaptively controlling an unknown
linear Gaussian system with quadratic costs. We combine ideas from experiment
design, estimation theory and a perturbation bound of certain information
matrices to derive regret lower bounds exhibiting scaling on the order of
magnitude $\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the
role of control-theoretic parameters and we are able to show that systems that
are hard to control are also hard to learn to control; when instantiated to
state feedback systems we recover the dimensional dependency of earlier work
but with improved scaling with system-theoretic constants such as system costs
and Gramians. Furthermore, we extend our results to a class of partially
observed systems and demonstrate that systems with poor observability structure
also are hard to learn to control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Inference with Gaussian Mixture by Entropy Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Furuya, Hiroyuki Kusumoto, Koichi Taniguchi, Naoya Kanno, Kazuma Suetake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational inference is a technique for approximating intractable posterior
distributions in order to quantify the uncertainty of machine learning.
Although the unimodal Gaussian distribution is usually chosen as a parametric
distribution, it hardly approximates the multimodality. In this paper, we
employ the Gaussian mixture distribution as a parametric distribution. A main
difficulty of variational inference with the Gaussian mixture is how to
approximate the entropy of the Gaussian mixture. We approximate the entropy of
the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which
can be analytically calculated. In addition, we theoretically analyze the
approximation error between the true entropy and approximated one in order to
reveal when our approximation works well. Specifically, the approximation error
is controlled by the ratios of the distances between the means to the sum of
the variances of the Gaussian mixture. Furthermore, it converges to zero when
the ratios go to infinity. This situation seems to be more likely to occur in
higher dimensional parametric spaces because of the curse of dimensionality.
Therefore, our result guarantees that our approximation works well, for
example, in neural networks that assume a large number of weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Complexity and Optimal Algorithms for Non-linear Ridge
  Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nived Rajaraman, Yanjun Han, Jiantao Jiao, Kannan Ramchandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the sequential decision-making problem where the mean outcome is
a non-linear function of the chosen action. Compared with the linear model, two
curious phenomena arise in non-linear models: first, in addition to the
"learning phase" with a standard parametric rate for estimation or regret,
there is an "burn-in period" with a fixed cost determined by the non-linear
function; second, achieving the smallest burn-in cost requires new exploration
algorithms. For a special family of non-linear functions named ridge functions
in the literature, we derive upper and lower bounds on the optimal burn-in
cost, and in addition, on the entire learning trajectory during the burn-in
period via differential equations. In particular, a two-stage algorithm that
first finds a good initial action and then treats the problem as locally linear
is statistically optimal. In contrast, several classical algorithms, such as
UCB and algorithms relying on regression oracles, are provably suboptimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Title change; add a new lower bound for linear bandits in Theorem 13</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Deep Learning Model Calibration for Classification
  Problems in Mechanics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Mohammadzadeh, Peerasait Prachaseree, Emma Lejeune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a growing interest in applying machine learning
methods to problems in engineering mechanics. In particular, there has been
significant interest in applying deep learning techniques to predicting the
mechanical behavior of heterogeneous materials and structures. Researchers have
shown that deep learning methods are able to effectively predict mechanical
behavior with low error for systems ranging from engineered composites, to
geometrically complex metamaterials, to heterogeneous biological tissue.
However, there has been comparatively little attention paid to deep learning
model calibration, i.e., the match between predicted probabilities of outcomes
and the true probabilities of outcomes. In this work, we perform a
comprehensive investigation into ML model calibration across seven open access
engineering mechanics datasets that cover three distinct types of mechanical
problems. Specifically, we evaluate both model and model calibration error for
multiple machine learning methods, and investigate the influence of ensemble
averaging and post hoc model calibration via temperature scaling. Overall, we
find that ensemble averaging of deep neural networks is both an effective and
consistent tool for improving model calibration, while temperature scaling has
comparatively limited benefits. Looking forward, we anticipate that this
investigation will lay the foundation for future work in developing mechanics
specific approaches to deep learning model calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">survey</span> on online active learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Cacciarelli, Murat Kulahci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online active learning is a paradigm in machine learning that aims to select
the most informative data points to label from a data stream. The problem of
minimizing the cost associated with collecting labeled observations has gained
a lot of attention in recent years, particularly in real-world applications
where data is only available in an unlabeled form. Annotating each observation
can be time-consuming and costly, making it difficult to obtain large amounts
of labeled data. To overcome this issue, many active learning strategies have
been proposed in the last decades, aiming to select the most informative
observations for labeling in order to improve the performance of machine
learning models. These approaches can be broadly divided into two categories:
static pool-based and stream-based active learning. Pool-based active learning
involves selecting a subset of observations from a closed pool of unlabeled
data, and it has been the focus of many surveys and literature reviews.
However, the growing availability of data streams has led to an increase in the
number of approaches that focus on online active learning, which involves
continuously selecting and labeling observations as they arrive in a stream.
This work aims to provide an overview of the most recently proposed approaches
for selecting the most informative observations from data streams in the
context of online active learning. We review the various techniques that have
been proposed and discuss their strengths and limitations, as well as the
challenges and opportunities that exist in this area of research. Our review
aims to provide a comprehensive and up-to-date overview of the field and to
highlight directions for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Images Are Counterfactual Samples for Robust Fine-tuning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are challenged by the distribution shift between the
training data and test data. Recently, the large models pre-trained on diverse
data demonstrate unprecedented robustness to various distribution shifts.
However, fine-tuning on these models can lead to a trade-off between
in-distribution (ID) performance and out-of-distribution (OOD) robustness.
Existing methods for tackling this trade-off do not explicitly address the OOD
robustness problem. In this paper, based on causal analysis on the
aforementioned problems, we propose a novel fine-tuning method, which use
masked images as counterfactual samples that help improving the robustness of
the fine-tuning model. Specifically, we mask either the semantics-related or
semantics-unrelated patches of the images based on class activation map to
break the spurious correlation, and refill the masked patches with patches from
other images. The resulting counterfactual samples are used in feature-based
distillation with the pre-trained model. Extensive experiments verify that
regularizing the fine-tuning with the proposed masked images can achieve a
better trade-off between ID and OOD performance, surpassing previous methods on
the OOD performance. Our code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023 (v2: improve the clarity)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training set cleansing of backdoor poisoning by <span class="highlight-title">self-supervised</span>
  representation learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Wang, S. Karami, O. Dia, H. Ritter, E. Emamjomeh-Zadeh, J. Chen, Z. Xiang, D. J. Miller, G. Kesidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A backdoor or Trojan attack is an important type of data poisoning attack
against deep neural network (DNN) classifiers, wherein the training dataset is
poisoned with a small number of samples that each possess the backdoor pattern
(usually a pattern that is either imperceptible or innocuous) and which are
mislabeled to the attacker's target class. When trained on a backdoor-poisoned
dataset, a DNN behaves normally on most benign test samples but makes incorrect
predictions to the target class when the test sample has the backdoor pattern
incorporated (i.e., contains a backdoor trigger). Here we focus on image
classification tasks and show that supervised training may build stronger
association between the backdoor pattern and the associated target class than
that between normal features and the true class of origin. By contrast,
self-supervised representation learning ignores the labels of samples and
learns a feature embedding based on images' semantic content. %We thus propose
to use unsupervised representation learning to avoid emphasising
backdoor-poisoned training samples and learn a similar feature embedding for
samples of the same class. Using a feature embedding found by self-supervised
representation learning, a data cleansing method, which combines sample
filtering and re-labeling, is developed. Experiments on CIFAR-10 benchmark
datasets show that our method achieves state-of-the-art performance in
mitigating backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicted Embedding Power Regression for Large-Scale Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Yang, William Gebhardt, Alexander G. Ororbia, Travis Desell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) inputs can compromise the performance and safety of
real world machine learning systems. While many methods exist for OOD detection
and work well on small scale datasets with lower resolution and few classes,
few methods have been developed for large-scale OOD detection. Existing
large-scale methods generally depend on maximum classification probability,
such as the state-of-the-art grouped softmax method. In this work, we develop a
novel approach that calculates the probability of the predicted class label
based on label distributions learned during the training process. Our method
performs better than current state-of-the-art methods with only a negligible
increase in compute cost. We evaluate our method against contemporary methods
across $14$ datasets and achieve a statistically significant improvement with
respect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking
  Neural Networks with Learnable Neuronal Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumar Kosta, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras have recently shown great potential for high-speed motion
estimation owing to their ability to capture temporally rich information
asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired
event-driven processing can efficiently handle such asynchronous data, while
neuron models such as the leaky-integrate and fire (LIF) can keep track of the
quintessential timing information contained in the inputs. SNNs achieve this by
maintaining a dynamic state in the neuron memory, retaining important
information while forgetting redundant data over time. Thus, we posit that SNNs
would allow for better performance on sequential regression tasks compared to
similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult
to train due to vanishing spikes at later layers. To that effect, we propose an
adaptive fully-spiking framework with learnable neuronal dynamics to alleviate
the spike vanishing problem. We utilize surrogate gradient-based
backpropagation through time (BPTT) to train our deep SNNs from scratch. We
validate our approach for the task of optical flow estimation on the
Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset.
Our experiments on these datasets show an average reduction of 13% in average
endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several
down-scaled models and observe that our SNN models consistently outperform
similarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the
importance of SNNs for smaller models and their suitability at the edge. In
terms of efficiency, our SNNs offer substantial savings in network parameters
(48.3x) and computational energy (10.2x) while attaining ~10% lower EPE
compared to the state-of-the-art ANN implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian <span class="highlight-title">Prompt</span> Learning for Image-Language Model Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi da Costa, Cees G. M. Snoek, Georgios Tzimiropoulos, Brais Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational image-language models have generated considerable interest due
to their efficient adaptation to downstream tasks by prompt learning. Prompt
learning treats part of the language model input as trainable while freezing
the rest, and optimizes an Empirical Risk Minimization objective. However,
Empirical Risk Minimization is known to suffer from distributional shifts which
hurt generalizability to prompts unseen during training. By leveraging the
regularization ability of Bayesian methods, we frame prompt learning from the
Bayesian perspective and formulate it as a variational inference problem. Our
approach regularizes the prompt space, reduces overfitting to the seen prompts
and improves the prompt generalization on unseen prompts. Our framework is
implemented by modeling the input prompt space in a probabilistic manner, as an
a priori distribution which makes our proposal compatible with prompt learning
approaches that are unconditional or conditional on the image. We demonstrate
empirically on 15 benchmarks that Bayesian prompt learning provides an
appropriate coverage of the prompt space, prevents learning spurious features,
and exploits transferable invariant features. This results in better
generalization of unseen prompts, even across different datasets and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision Making for Human-in-the-loop Robotic Agents via
  Uncertainty-Aware Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singi, Zhanpeng He, Alvin Pan, Sandip Patel, Gunnar A. Sigurdsson, Robinson Piramuthu, Shuran Song, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly
autonomously in solving a task, but can request help from an external expert
when needed. However, knowing when to request such assistance is critical: too
few requests can lead to the robot making mistakes, but too many requests can
overload the expert. In this paper, we present a Reinforcement Learning based
approach to this problem, where a semi-autonomous agent asks for external
assistance when it has low confidence in the eventual success of the task. The
confidence level is computed by estimating the variance of the return from the
current state. We show that this estimate can be iteratively improved during
training using a Bellman-like recursion. On discrete navigation problems with
both fully- and partially-observable state information, we show that our method
makes effective use of a limited budget of expert calls at run-time, despite
having no access to the expert at training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization of generative model for neuronal ensemble inference
  method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Kimura, Koujin Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various brain functions that are necessary to maintain life activities
materialize through the interaction of countless neurons. Therefore, it is
important to analyze the structure of functional neuronal network. To elucidate
the mechanism of brain function, many studies are being actively conducted on
the structure of functional neuronal ensemble and hub, including all areas of
neuroscience. In addition, recent study suggests that the existence of
functional neuronal ensembles and hubs contributes to the efficiency of
information processing. For these reasons, there is a demand for methods to
infer functional neuronal ensembles from neuronal activity data, and methods
based on Bayesian inference have been proposed. However, there is a problem in
modeling the activity in Bayesian inference. The features of each neuron's
activity have non-stationarity depending on physiological experimental
conditions. As a result, the assumption of stationarity in Bayesian inference
model impedes inference, which leads to destabilization of inference results
and degradation of inference accuracy. In this study, we extend the range of
the variable for expressing the neuronal state, and generalize the likelihood
of the model for extended variables. By comparing with the previous study, our
model can express the neuronal state in larger space. This generalization
without restriction of the binary input enables us to perform soft clustering
and apply the method to non-stationary neuroactivity data. In addition, for the
effectiveness of the method, we apply the developed method to multiple
synthetic fluorescence data generated from the electrical potential data in
leaky integrated-and-fire model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Softmax Information for Selective Classification with
  Out-of-Distribution Data <span class="chip">ACCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a task that is receiving an
increasing amount of research attention in the domain of deep learning for
computer vision. However, the performance of detection methods is generally
evaluated on the task in isolation, rather than also considering potential
downstream tasks in tandem. In this work, we examine selective classification
in the presence of OOD data (SCOD). That is to say, the motivation for
detecting OOD samples is to reject them so their impact on the quality of
predictions is reduced. We show under this task specification, that existing
post-hoc methods perform quite differently compared to when evaluated only on
OOD detection. This is because it is no longer an issue to conflate
in-distribution (ID) data with OOD data if the ID data is going to be
misclassified. However, the conflation within ID data of correct and incorrect
predictions becomes undesirable. We also propose a novel method for SCOD,
Softmax Information Retaining Combination (SIRC), that augments softmax-based
confidence scores with feature-agnostic information such that their ability to
identify OOD samples is improved without sacrificing separation between correct
and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale
datasets and convolutional neural network architectures show that SIRC is able
to consistently match or outperform the baseline for SCOD, whilst existing OOD
detection methods fail to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2022 (Best Paper Award)
  https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NIERT: Accurate Numerical Interpolation through Unifying Scattered Data
  Representations using <span class="highlight-title">Transformer</span> Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhe Ding, Boyang Xia, Milong Ren, Dongbo Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpolation for scattered data is a classical problem in numerical
analysis, with a long history of theoretical and practical contributions.
Recent advances have utilized deep neural networks to construct interpolators,
exhibiting excellent and generalizable performance. However, they still fall
short in two aspects: \textbf{1) inadequate representation learning}, resulting
from separate embeddings of observed and target points in popular
encoder-decoder frameworks and \textbf{2) limited generalization power}, caused
by overlooking prior interpolation knowledge shared across different domains.
To overcome these limitations, we present a \textbf{N}umerical
\textbf{I}nterpolation approach using \textbf{E}ncoder \textbf{R}epresentation
of \textbf{T}ransformers (called \textbf{NIERT}). On one hand, NIERT utilizes
an encoder-only framework rather than the encoder-decoder structure. This way,
NIERT can embed observed and target points into a unified encoder
representation space, thus effectively exploiting the correlations among them
and obtaining more precise representations. On the other hand, we propose to
pre-train NIERT on large-scale synthetic mathematical functions to acquire
prior interpolation knowledge, and transfer it to multiple interpolation
domains with consistent performance gain. On both synthetic and real-world
datasets, NIERT outperforms the existing approaches by a large margin, i.e.,
4.3$\sim$14.3$\times$ lower MAE on TFRD subsets, and 1.7/1.8/8.7$\times$ lower
MSE on Mathit/PhysioNet/PTV datasets. The source code of NIERT is available at
https://github.com/DingShizhe/NIERT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Entropy as an Inductive Bias for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Peruzzo, Enver Sangineto, Yahui Liu, Marco De Nadai, Wei Bi, Bruno Lepri, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on Vision Transformers (VTs) showed that introducing a local
inductive bias in the VT architecture helps reducing the number of samples
necessary for training. However, the architecture modifications lead to a loss
of generality of the Transformer backbone, partially contradicting the push
towards the development of uniform architectures, shared, e.g., by both the
Computer Vision and the Natural Language Processing areas. In this work, we
propose a different and complementary direction, in which a local bias is
introduced using an auxiliary self-supervised task, performed jointly with
standard supervised training. Specifically, we exploit the observation that the
attention maps of VTs, when trained with self-supervision, can contain a
semantic segmentation structure which does not spontaneously emerge when
training is supervised. Thus, we explicitly encourage the emergence of this
spatial clustering as a form of training regularization. In more detail, we
exploit the assumption that, in a given image, objects usually correspond to
few connected regions, and we propose a spatial formulation of the information
entropy to quantify this object-based inductive bias. By minimizing the
proposed spatial entropy, we include an additional self-supervised signal
during training. Using extensive experiments, we show that the proposed
regularization leads to equivalent or better results than other VT proposals
which include a local bias by changing the basic Transformer architecture, and
it can drastically boost the VT final accuracy when using small-medium training
sets. The code is available at https://github.com/helia95/SAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth image-to-image translations with latent space interpolations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00841v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00841v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Marco De Nadai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-domain image-to-image (I2I) translations can transform a source image
according to the style of a target domain. One important, desired
characteristic of these transformations, is their graduality, which corresponds
to a smooth change between the source and the target image when their
respective latent-space representations are linearly interpolated. However,
state-of-the-art methods usually perform poorly when evaluated using
inter-domain interpolations, often producing abrupt changes in the appearance
or non-realistic intermediate images. In this paper, we argue that one of the
main reasons behind this problem is the lack of sufficient inter-domain
training data and we propose two different regularization methods to alleviate
this issue: a new shrinkage loss, which compacts the latent space, and a Mixup
data-augmentation strategy, which flattens the style representations between
domains. We also propose a new metric to quantitatively evaluate the degree of
the interpolation smoothness, an aspect which is not sufficiently covered by
the existing I2I translation metrics. Using both our proposed metric and
standard evaluation protocols, we show that our regularization techniques can
improve the state-of-the-art multi-domain I2I translations by a large margin.
Our code will be made publicly available upon the acceptance of this article.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Inverted Bottleneck Convolution for DARTS Search Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Ahmadian, Louis S. P. Liu, Yue Fei, Konstantinos N. Plataniotis, Mahdi S. Hosseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable Architecture Search (DARTS) has attracted considerable
attention as a gradient-based neural architecture search method. Since the
introduction of DARTS, there has been little work done on adapting the action
space based on state-of-art architecture design principles for CNNs. In this
work, we aim to address this gap by incrementally augmenting the DARTS search
space with micro-design changes inspired by ConvNeXt and studying the trade-off
between accuracy, evaluation layer count, and computational cost. We introduce
the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the
computational footprint of the inverted bottleneck block proposed in ConvNeXt.
Our proposed architecture is much less sensitive to evaluation layer count and
outperforms a DARTS network with similar size significantly, at layer counts as
small as 2. Furthermore, with less layers, not only does it achieve higher
accuracy with lower computational footprint (measured in GMACs) and parameter
count, GradCAM comparisons show that our network can better detect distinctive
features of target objects compared to DARTS. Code is available from
https://github.com/mahdihosseini/PIBConv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoNIC Challenge: Pushing the Frontiers of Nuclear Detection,
  Segmentation, Classification and Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Graham, Quoc Dang Vu, Mostafa Jahanifar, Martin Weigert, Uwe Schmidt, Wenhua Zhang, Jun Zhang, Sen Yang, Jinxi Xiang, Xiyue Wang, Josef Lorenz Rumberger, Elias Baumann, Peter Hirsch, Lihao Liu, Chenyang Hong, Angelica I. Aviles-Rivero, Ayushi Jain, Heeyoung Ahn, Yiyu Hong, Hussam Azzuni, Min Xu, Mohammad Yaqub, Marie-Claire Blache, Benoît Piégu, Bertrand Vernay, Tim Scherr, Moritz Böhland, Katharina Löffler, Jiachen Li, Weiqin Ying, Chixin Wang, Dagmar Kainmueller, Carola-Bibiane Schönlieb, Shuolin Liu, Dhairya Talsania, Yughender Meda, Prakash Mishra, Muhammad Ridzuan, Oliver Neumann, Marcel P. Schilling, Markus Reischl, Ralf Mikut, Banban Huang, Hsiang-Chin Chien, Ching-Ping Wang, Chia-Yen Lee, Hong-Kun Lin, Zaiyi Liu, Xipeng Pan, Chu Han, Jijun Cheng, Muhammad Dawood, Srijay Deshpande, Raja Muhammad Saad Bashir, Adam Shephard, Pedro Costa, João D. Nunes, Aurélio Campilho, Jaime S. Cardoso, Hrishikesh P S, Densen Puthussery, Devika R G, Jiji C V, Ye Zhang, Zijie Fang, Zhifan Lin, Yongbing Zhang, Chunhui Lin, Liukun Zhang, Lijian Mao, Min Wu, Vi Thi-Tuong Vo, Soo-Hyung Kim, Taebum Lee, Satoshi Kondo, Satoshi Kasai, Pranay Dumbhare, Vedant Phuse, Yash Dubey, Ankush Jamthikar, Trinh Thi Le Vuong, Jin Tae Kwak, Dorsa Ziaei, Hyun Jung, Tianyi Miao, David Snead, Shan E Ahmed Raza, Fayyaz Minhas, Nasir M. Rajpoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclear detection, segmentation and morphometric profiling are essential in
helping us further understand the relationship between histology and patient
outcome. To drive innovation in this area, we setup a community-wide challenge
using the largest available dataset of its kind to assess nuclear segmentation
and cellular composition. Our challenge, named CoNIC, stimulated the
development of reproducible algorithms for cellular recognition with real-time
result inspection on public leaderboards. We conducted an extensive
post-challenge analysis based on the top-performing models using 1,658
whole-slide images of colon tissue. With around 700 million detected nuclei per
model, associated features were used for dysplasia grading and survival
analysis, where we demonstrated that the challenge's improvement over the
previous state-of-the-art led to significant boosts in downstream performance.
Our findings also suggest that eosinophils and neutrophils play an important
role in the tumour microevironment. We release challenge models and WSI-level
results to foster the development of further methods for biomarker discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-driven machine learning models coupling PyTorch and Firedrake <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nacime Bouziani, David A. Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial differential equations (PDEs) are central to describing and modelling
complex physical systems that arise in many disciplines across science and
engineering. However, in many realistic applications PDE modelling provides an
incomplete description of the physics of interest. PDE-based machine learning
techniques are designed to address this limitation. In this approach, the PDE
is used as an inductive bias enabling the coupled model to rely on fundamental
physical laws while requiring less training data. The deployment of
high-performance simulations coupling PDEs and machine learning to complex
problems necessitates the composition of capabilities provided by machine
learning and PDE-based frameworks. We present a simple yet effective coupling
between the machine learning framework PyTorch and the PDE system Firedrake
that provides researchers, engineers and domain specialists with a high
productive way of specifying coupled models while only requiring trivial
changes to existing code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ICLR 2023 Workshop on Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble forecasts in reproducing kernel Hilbert space family: dynamical
  systems in Wonderland 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Dufée, Bérenger Hug, Etienne Memin, Gilles Tissot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A methodological framework for ensemble-based estimation and simulation of
high dimensional dynamical systems such as the oceanic or atmospheric flows is
proposed. To that end, the dynamical system is embedded in a family of
reproducing kernel Hilbert spaces with kernel functions driven by the dynamics.
This family is nicknamed Wonderland for its appealing properties. In Wonderland
the Koopman and Perron-Frobenius operators are unitary and uniformly
continuous. This property warrants they can be expressed in exponential series
of diagonalizable bounded infinitesimal generators. Access to Lyapunov
exponents and to exact ensemble based expressions of the tangent linear
dynamics are directly available as well. Wonderland enables us the devise of
strikingly simple ensemble data assimilation methods for trajectory
reconstructions in terms of constant-in-time linear combinations of trajectory
samples. Such an embarrassingly simple strategy is made possible through a
fully justified superposition principle ensuing from several fundamental
theorems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction
  <span class="highlight-title">Pretrain</span>ing and Conditional Molecule Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Song Song, Liangren Zhang, Bo Huang, Zhenming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical reactions are the fundamental building blocks of drug design and
organic chemistry research. In recent years, there has been a growing need for
a large-scale deep-learning framework that can efficiently capture the basic
rules of chemical reactions. In this paper, we have proposed a unified
framework that addresses both the reaction representation learning and molecule
generation tasks, which allows for a more holistic approach. Inspired by the
organic chemistry mechanism, we develop a novel pretraining framework that
enables us to incorporate inductive biases into the model. Our framework
achieves state-of-the-art results on challenging downstream tasks. By
possessing chemical knowledge, this framework can be applied to reaction-based
generative models, overcoming the limitations of current molecule generation
models that rely on a small number of reaction templates. In the extensive
experiments, our model generates synthesizable drug-like structures of high
quality. Overall, our work presents a significant step toward a large-scale
deep-learning framework for a variety of reaction-based applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving physics-informed neural networks with meta-learned
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Bihlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the error achievable using physics-informed neural networks for
solving systems of differential equations can be substantially reduced when
these networks are trained using meta-learned optimization methods rather than
to using fixed, hand-crafted optimizers as traditionally done. We choose a
learnable optimization method based on a shallow multi-layer perceptron that is
meta-trained for specific classes of differential equations. We illustrate
meta-trained optimizers for several equations of practical relevance in
mathematical physics, including the linear advection equation, Poisson's
equation, the Korteweg--de Vries equation and Burgers' equation. We also
illustrate that meta-learned optimizers exhibit transfer learning abilities, in
that a meta-trained optimizer on one differential equation can also be
successfully deployed on another differential equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Efficient Adversarial Training Guided by Gradient Magnitude 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.03076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.03076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Wang, Yanghao Zhang, Yanbin Zheng, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is an effective but time-consuming way to train robust
deep neural networks that can withstand strong adversarial attacks. As a
response to its inefficiency, we propose Dynamic Efficient Adversarial Training
(DEAT), which gradually increases the adversarial iteration during training. We
demonstrate that the gradient's magnitude correlates with the curvature of the
trained model's loss landscape, allowing it to reflect the effect of
adversarial training. Therefore, based on the magnitude of the gradient, we
propose a general acceleration strategy, M+ acceleration, which enables an
automatic and highly effective method of adjusting the training procedure. M+
acceleration is computationally efficient and easy to implement. It is suited
for DEAT and compatible with the majority of existing adversarial training
techniques. Extensive experiments have been done on CIFAR-10 and ImageNet
datasets with various training environments. The results show that the proposed
M+ acceleration significantly improves the training efficiency of existing
adversarial training methods while achieving similar robustness performance.
This demonstrates that the strategy is highly adaptive and offers a valuable
solution for automatic adversarial training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CycleSense: Detecting Near Miss Incidents in Bicycle Traffic from Mobile
  Motion Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet-Serdar Karakaya, Thomas Ritter, Felix Biessmann, David Bermbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cities worldwide, cars cause health and traffic problems whichcould be
partly mitigated through an increased modal share of bicycles. Many people,
however, avoid cycling due to a lack of perceived safety. For city planners,
addressing this is hard as they lack insights intowhere cyclists feel safe and
where they do not. To gain such insights,we have in previous work proposed the
crowdsourcing platform SimRa,which allows cyclists to record their rides and
report near miss incidentsvia a smartphone app. In this paper, we present
CycleSense, a combination of signal pro-cessing and Machine Learning
techniques, which partially automatesthe detection of near miss incidents, thus
making the reporting of nearmiss incidents easier. Using the SimRa data set, we
evaluate CycleSenseby comparing it to a baseline method used by SimRa and show
that itsignificantly improves incident detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Broad Ensemble Learning System for Drifting Stream Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Bakhshi, Pouya Ghahramanian, Hamed Bonab, Fazli Can
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a data stream environment, classification models must handle concept drift
efficiently and effectively. Ensemble methods are widely used for this purpose;
however, the ones available in the literature either use a large data chunk to
update the model or learn the data one by one. In the former, the model may
miss the changes in the data distribution, and in the latter, the model may
suffer from inefficiency and instability. To address these issues, we introduce
a novel ensemble approach based on the Broad Learning System (BLS), where mini
chunks are used at each update. BLS is an effective lightweight neural
architecture recently developed for incremental learning. Although it is fast,
it requires huge data chunks for effective updates, and is unable to handle
dynamic changes observed in data streams. Our proposed approach named Broad
Ensemble Learning System (BELS) uses a novel updating method that significantly
improves best-in-class model accuracy. It employs an ensemble of output layers
to address the limitations of BLS and handle drifts. Our model tracks the
changes in the accuracy of the ensemble components and react to these changes.
We present the mathematical derivation of BELS, perform comprehensive
experiments with 20 datasets that demonstrate the adaptability of our model to
various drift types, and provide hyperparameter and ablation analysis of our
proposed model. Our experiments show that the proposed approach outperforms
nine state-of-the-art baselines and supplies an overall improvement of 13.28%
in terms of average prequential accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TriNet: stabilizing <span class="highlight-title">self-supervised</span> learning from complete or slow
  collapse on ASR <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixin Cao, Jun Wang, Ben Yang, Dan Su, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) models confront challenges of abrupt
informational collapse or slow dimensional collapse. We propose TriNet, which
introduces a novel triple-branch architecture for preventing collapse and
stabilizing the pre-training. TriNet learns the SSL latent embedding space and
incorporates it to a higher level space for predicting pseudo target vectors
generated by a frozen teacher. Our experimental results show that the proposed
method notably stabilizes and accelerates pre-training and achieves a relative
word error rate reduction (WERR) of 6.06% compared to the state-of-the-art
(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code
at https://github.com/tencent-ailab/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CECT: Controllable Ensemble CNN and <span class="highlight-title">Transformer</span> for COVID-19 Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoshan Liu, Lei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most computer vision models are developed based on either convolutional
neural network (CNN) or transformer, while the former (latter) method captures
local (global) features. To relieve model performance limitations due to the
lack of global (local) features, we develop a novel classification network CECT
by controllable ensemble CNN and transformer. CECT is composed of a
convolutional encoder block, a transposed-convolutional decoder block, and a
transformer classification block. Different from conventional CNN- or
transformer-based methods, our CECT can capture features at both multi-local
and global scales. Besides, the contribution of local features at different
scales can be controlled with the proposed ensemble coefficients. We evaluate
CECT on two public COVID-19 datasets and it outperforms existing
state-of-the-art methods on all evaluation metrics. With remarkable feature
capture ability, we believe CECT can be extended to other medical image
classification scenarios as a diagnosis assistant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Hamdi, Bernard Ghanem, Matthias Nießner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel
view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels
for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage
machine learning and adoption of SRFs as a 3D representation, we present SPARF,
a large-scale ShapeNet-based synthetic dataset for novel view synthesis
consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at
high resolution (400 X 400 pixels). The dataset is orders of magnitude larger
than existing synthetic datasets for novel view synthesis and includes more
than one million 3D-optimized radiance fields with multiple voxel resolutions.
Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate
sparse voxel radiance fields from only few views. This is done by using the
densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs
partial SRFs from few/one images and a specialized SRF loss to learn to
generate high-quality sparse voxel radiance fields that can be rendered from
novel views. Our approach achieves state-of-the-art results in the task of
unconstrained novel view synthesis based on few views on ShapeNet as compared
to recent baselines. The SPARF dataset will be made public with the code and
models on the project website https://abdullahamdi.com/sparf/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale
  Confirmatory Item Factor Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.09500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.09500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Urban, Daniel J. Bauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate novel parameter estimation and goodness-of-fit (GOF)
assessment methods for large-scale confirmatory item factor analysis (IFA) with
many respondents, items, and latent factors. For parameter estimation, we
extend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to
the confirmatory setting by showing how to handle constraints on loadings and
factor correlations. For GOF assessment, we explore simulation-based tests and
indices that extend the classifier two-sample test (C2ST), a method that tests
whether a deep neural network can distinguish between observed data and
synthetic data sampled from a fitted IFA model. Proposed extensions include a
test of approximate fit wherein the user specifies what percentage of observed
and synthetic data should be distinguishable as well as a relative fit index
(RFI) that is similar in spirit to the RFIs used in structural equation
modeling. Via simulation studies, we show that: (1) the confirmatory extension
of Urban and Bauer's (2021) algorithm obtains comparable estimates to a
state-of-the-art estimation procedure in less time; (2) C2ST-based GOF tests
control the empirical type I error rate and detect when the latent
dimensionality is misspecified; and (3) the sampling distribution of the
C2ST-based RFI depends on the sample size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Class Learning using Variational Autoencoders with
  Similarity Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.01303v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.01303v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huo, Terence L. van Zyl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting in neural networks during incremental learning
remains a challenging problem. Previous research investigated catastrophic
forgetting in fully connected networks, with some earlier work exploring
activation functions and learning algorithms. Applications of neural networks
have been extended to include similarity learning. Understanding how similarity
learning loss functions would be affected by catastrophic forgetting is of
significant interest. Our research investigates catastrophic forgetting for
four well-known similarity-based loss functions during incremental class
learning. The loss functions are Angular, Contrastive, Center, and Triplet
loss. Our results show that the catastrophic forgetting rate differs across
loss functions on multiple datasets. The Angular loss was least affected,
followed by Contrastive, Triplet loss, and Center loss with good mining
techniques. We implemented three existing incremental learning techniques,
iCaRL, EWC, and EBLL. We further proposed a novel technique using Variational
Autoencoders (VAEs) to generate representation as exemplars passed through the
network's intermediate layers. Our method outperformed three existing
state-of-the-art techniques. We show that one does not require stored images
(exemplars) for incremental learning with similarity learning. The generated
representations from VAEs help preserve regions of the embedding space used by
prior knowledge so that new knowledge does not ``overwrite'' it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-One Laws of Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13060v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13060v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Adam-Day, Theodor Mihai Iliant, İsmail İlkan Ceylan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are de facto standard deep learning
architectures for machine learning on graphs. This has led to a large body of
work analyzing the capabilities and limitations of these models, particularly
pertaining to their representation and extrapolation capacity. We offer a novel
theoretical perspective on the representation and extrapolation capacity of
GNNs, by answering the question: how do GNNs behave as the number of graph
nodes become very large? Under mild assumptions, we show that when we draw
graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability
that such graphs are mapped to a particular output by a class of GNN
classifiers tends to either zero or to one. This class includes the popular
graph convolutional network architecture. The result establishes 'zero-one
laws' for these GNNs, and analogously to other convergence laws, entails
theoretical limitations on their capacity. We empirically verify our results,
observing that the theoretical asymptotic limits are evident already on
relatively small graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + references + 9 pages appendices, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Concept Knowledge Graph for User Next Intent Prediction at Alipay <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00503v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00503v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacheng He, Qianghuai Jia, Lin Yuan, Ruopeng Li, Yixin Ou, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper illustrates the technologies of user next intent prediction with a
concept knowledge graph. The system has been deployed on the Web at Alipay,
serving more than 100 million daily active users. To explicitly characterize
user intent, we propose AlipayKG, which is an offline concept knowledge graph
in the Life-Service domain modeling the historical behaviors of users, the rich
content interacted by users and the relations between them. We further
introduce a Transformer-based model which integrates expert rules from the
knowledge graph to infer the online user's next intent. Experimental results
demonstrate that the proposed system can effectively enhance the performance of
the downstream tasks while retaining explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hardware Acceleration of Neural Graphics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Husnain Mubarik, Ramakrishna Kanungo, Tobias Zirr, Rakesh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rendering and inverse-rendering algorithms that drive conventional computer
graphics have recently been superseded by neural representations (NR). NRs have
recently been used to learn the geometric and the material properties of the
scenes and use the information to synthesize photorealistic imagery, thereby
promising a replacement for traditional rendering algorithms with scalable
quality and predictable performance. In this work we ask the question: Does
neural graphics (NG) need hardware support? We studied representative NG
applications showing that, if we want to render 4k res. at 60FPS there is a gap
of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,
there is an even larger gap of 2-4 OOM between the desired performance and the
required system power. We identify that the input encoding and the MLP kernels
are the performance bottlenecks, consuming 72%,60% and 59% of application time
for multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,
respectively. We propose a NG processing cluster, a scalable and flexible
hardware architecture that directly accelerates the input encoding and MLP
kernels through dedicated engines and supports a wide range of NG applications.
We also accelerate the rest of the kernels by fusing them together in Vulkan,
which leads to 9.94X kernel-level performance improvement compared to un-fused
implementation of the pre-processing and the post-processing kernels. Our
results show that, NGPC gives up to 58X end-to-end application-level
performance improvement, for multi res. hashgrid encoding on average across the
four NG applications, the performance benefits are 12X,20X,33X and 39X for the
scaling factor of 8,16,32 and 64, respectively. Our results show that with
multi res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS
for NeRF and 8k res. at 120FPS for all our other NG applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Unlearning by Model Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngsik Yoon, Jinhwan Nam, Hyojeong Yun, Jaeho Lee, Dongwoo Kim, Jungseul Ok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a practical scenario of machine unlearning to erase a target
dataset, which causes unexpected behavior from the trained model. The target
dataset is often assumed to be fully identifiable in a standard unlearning
scenario. Such a flawless identification, however, is almost impossible if the
training dataset is inaccessible at the time of unlearning. Unlike previous
approaches requiring a complete set of targets, we consider few-shot unlearning
scenario when only a few samples of target data are available. To this end, we
formulate the few-shot unlearning problem specifying intentions behind the
unlearning request (e.g., purely unlearning, mislabel correction, privacy
protection), and we devise a straightforward framework that (i) retrieves a
proxy of the training data via model inversion fully exploiting information
available in the context of unlearning; (ii) adjusts the proxy according to the
unlearning intention; and (iii) updates the model with the adjusted proxy. We
demonstrate that our method using only a subset of target data can outperform
the state-of-the-art unlearning methods even with a complete indication of
target data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-dimensional analysis of double descent for linear regression with
  random projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider linear regression problems with a varying number of random
projections, where we provably exhibit a double descent curve for a fixed
prediction problem, with a high-dimensional analysis based on random matrix
theory. We first consider the ridge regression estimator and review earlier
results using classical notions from non-parametric statistics, namely degrees
of freedom, also known as effective dimensionality. We then compute asymptotic
equivalents of the generalization performance (in terms of squared bias and
variance) of the minimum norm least-squares fit with random projections,
providing simple expressions for the double descent phenomenon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Discrimination to Generation: Knowledge Graph Completion with
  Generative <span class="highlight-title">Transformer</span> <span class="chip">WWW 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02113v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02113v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui Chen, Feiyu Xiong, Mosha Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2022 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pretrain</span>ed Language Models are Symbolic Mathematics Solvers too! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimia Noorbakhsh, Modar Sulaiman, Mahdi Sharifi, Kallol Roy, Pooyan Jamshidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving symbolic mathematics has always been of in the arena of human
ingenuity that needs compositional reasoning and recurrence. However, recent
studies have shown that large-scale language models such as transformers are
universal and surprisingly can be trained as a sequence-to-sequence task to
solve complex mathematical equations. These large transformer models need
humongous amounts of training data to generalize to unseen symbolic mathematics
problems. In this paper, we present a sample efficient way of solving the
symbolic tasks by first pretraining the transformer model with language
translation and then fine-tuning the pretrained transformer model to solve the
downstream task of symbolic mathematics. We achieve comparable accuracy on the
integration task with our pretrained model while using around $1.5$ orders of
magnitude less number of training samples with respect to the state-of-the-art
deep learning for symbolic mathematics. The test accuracy on differential
equation tasks is considerably lower comparing with integration as they need
higher order recursions that are not present in language translations. We
propose the generalizability of our pretrained language model from Anna
Karenina Principle (AKP). We pretrain our model with different pairs of
language translations. Our results show language bias in solving symbolic
mathematics tasks. Finally, we study the robustness of the fine-tuned model on
symbolic math tasks against distribution shift, and our approach generalizes
better in distribution shift scenarios for the function integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relphormer: Relational Graph <span class="highlight-title">Transformer</span> for Knowledge Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10852v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10852v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Ningyu Zhang, Qiang Chen, Feiyu Xiong, Wei Guo, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Hypergraphs From Signals With Dual Smoothness Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Tang, Siheng Chen, Xiaowen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraph structure learning, which aims to learn the hypergraph structures
from the observed signals to capture the intrinsic high-order relationships
among the entities, becomes crucial when a hypergraph topology is not readily
available in the datasets. There are two challenges that lie at the heart of
this problem: 1) how to handle the huge search space of potential hyperedges,
and 2) how to define meaningful criteria to measure the relationship between
the signals observed on nodes and the hypergraph structure. In this paper, for
the first challenge, we adopt the assumption that the ideal hypergraph
structure can be derived from a learnable graph structure that captures the
pairwise relations within signals. Further, we propose a hypergraph structure
learning framework HGSL with a novel dual smoothness prior that reveals a
mapping between the observed node signals and the hypergraph structure, whereby
each hyperedge corresponds to a subgraph with both node signal smoothness and
edge signal smoothness in the learnable graph structure. Finally, we conduct
extensive experiments to evaluate HGSL on both synthetic and real world
datasets. Experiments show that HGSL can efficiently infer meaningful
hypergraph topologies from observed signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thought Flow Nets: From Single Predictions to Trains of Model Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.12220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.12220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Schuff, Heike Adel, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When humans solve complex problems, they typically create a sequence of ideas
(involving an intuitive decision, reflection, error correction, etc.) in order
to reach a conclusive decision. Contrary to this, today's models are mostly
trained to map an input to one single and fixed output. In this paper, we
investigate how we can give models the opportunity of a second, third and
$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the
concept of a thought flow which creates a sequence of predictions. We present a
self-correction mechanism that is trained to estimate the model's correctness
and performs iterative prediction updates based on the correctness prediction's
gradient. We introduce our method at the example of question answering and
conduct extensive experiments that demonstrate (i) our method's ability to
correct its own predictions and (ii) its potential to notably improve model
performances. In addition, we conduct a qualitative analysis of thought flow
correction patterns and explore how thought flow predictions affect human users
within a crowdsourcing study. We find that (iii) thought flows enable improved
user performance and are perceived as more natural, correct, and intelligent as
single and/or top-3 predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WuYun: Exploring hierarchical skeleton-guided melody generation using
  knowledge-enhanced deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejun Zhang, Xinda Wu, Tieyao Zhang, Zhijie Huang, Xu Tan, Qihao Liang, Songruoyao Wu, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning has revolutionized music generation, existing methods
for structured melody generation follow an end-to-end left-to-right
note-by-note generative paradigm and treat each note equally. Here, we present
WuYun, a knowledge-enhanced deep learning architecture for improving the
structure of generated melodies, which first generates the most structurally
important notes to construct a melodic skeleton and subsequently infills it
with dynamically decorative notes into a full-fledged melody. Specifically, we
use music domain knowledge to extract melodic skeletons and employ sequence
learning to reconstruct them, which serve as additional knowledge to provide
auxiliary guidance for the melody generation process. We demonstrate that WuYun
can generate melodies with better long-term structure and musicality and
outperforms other state-of-the-art methods by 0.51 on average on all subjective
evaluation metrics. Our study provides a multidisciplinary lens to design
melodic hierarchical structures and bridge the gap between data-driven and
knowledge-based approaches for numerous music generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Probabilistic Logic Programming in Discrete-Continuous Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, Angelika Kimmig, Luc De Raedt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic
background knowledge in the form of logic. It has been shown to aid learning in
the limited data regime and to facilitate inference on out-of-distribution
data. Probabilistic NeSy focuses on integrating neural networks with both logic
and probability theory, which additionally allows learning under uncertainty. A
major limitation of current probabilistic NeSy systems, such as DeepProbLog, is
their restriction to finite probability distributions, i.e., discrete random
variables. In contrast, deep probabilistic programming (DPP) excels in
modelling and optimising continuous probability distributions. Hence, we
introduce DeepSeaProbLog, a neural probabilistic logic programming language
that incorporates DPP techniques into NeSy. Doing so results in the support of
inference and learning of both discrete and continuous probability
distributions under logical constraints. Our main contributions are 1) the
semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a
proven asymptotically unbiased learning algorithm, and 3) a series of
experiments that illustrate the versatility of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Representations of Bi-level Knowledge Graphs for Reasoning
  beyond Link Prediction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanyoung Chung, Joyce Jiyoung Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs represent known facts using triplets. While existing
knowledge graph embedding methods only consider the connections between
entities, we propose considering the relationships between triplets. For
example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is
(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,
Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a
prerequisite for $T_2$. In this paper, we define a higher-level triplet to
represent a relationship between triplets, e.g., $\langle T_1$,
PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation.
We define a bi-level knowledge graph that consists of the base-level and the
higher-level triplets. We also propose a data augmentation strategy based on
the random walks on the bi-level knowledge graph to augment plausible triplets.
Our model called BiVE learns embeddings by taking into account the structures
of the base-level and the higher-level triplets, with additional consideration
of the augmented triplets. We propose two new tasks: triplet prediction and
conditional link prediction. Given a triplet $T_1$ and a higher-level relation,
the triplet prediction predicts a triplet that is likely to be connected to
$T_1$ by the higher-level relation, e.g., $\langle T_1$, PrerequisiteFor,
?$\rangle$. The conditional link prediction predicts a missing entity in a
triplet conditioned on another triplet, e.g., $\langle T_1$, PrerequisiteFor,
(Avatar, Wins, ?)$\rangle$. Experimental results show that BiVE significantly
outperforms all other methods in the two new tasks and the typical base-level
link prediction in real-world bi-level knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial
  Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image
  Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Schrader, Tobias Alt, Joachim Weickert, Michael Ertel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Euler's elastica constitute an appealing variational image inpainting model.
It minimises an energy that involves the total variation as well as the level
line curvature. These components are transparent and make it attractive for
shape completion tasks. However, its gradient flow is a singular, anisotropic,
and nonlinear PDE of fourth order, which is numerically challenging: It is
difficult to find efficient algorithms that offer sharp edges and good rotation
invariance. As a remedy, we design the first neural algorithm that simulates
inpainting with Euler's Elastica. We use the deep energy concept which employs
the variational energy as neural network loss. Furthermore, we pair it with a
deep image prior where the network architecture itself acts as a prior. This
yields better inpaintings by steering the optimisation trajectory closer to the
desired solution. Our results are qualitatively on par with state-of-the-art
algorithms on elastica-based shape completion. They combine good rotation
invariance with sharp edges. Moreover, we benefit from the high efficiency and
effortless parallelisation within a neural framework. Our neural elastica
approach only requires 3x3 central difference stencils. It is thus much simpler
than other well-performing algorithms for elastica inpainting. Last but not
least, it is unsupervised as it requires no ground truth training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 10th European Workshop on Visual Information
  Processing, Lisbon, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Label Information Bottleneck for Label Enhancement <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghai Zheng, Jihua Zhu, Haoyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we focus on the challenging problem of Label Enhancement (LE),
which aims to exactly recover label distributions from logical labels, and
present a novel Label Information Bottleneck (LIB) method for LE. For the
recovery process of label distributions, the label irrelevant information
contained in the dataset may lead to unsatisfactory recovery performance. To
address this limitation, we make efforts to excavate the essential label
relevant information to improve the recovery performance. Our method formulates
the LE problem as the following two joint processes: 1) learning the
representation with the essential label relevant information, 2) recovering
label distributions based on the learned representation. The label relevant
information can be excavated based on the "bottleneck" formed by the learned
representation. Significantly, both the label relevant information about the
label assignments and the label relevant information about the label gaps can
be explored in our method. Evaluation experiments conducted on several
benchmark label distribution learning datasets verify the effectiveness and
competitiveness of LIB. Our source codes are available
https://github.com/qinghai-zheng/LIBLE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023, our source codes are available at
  https://github.com/qinghai-zheng/LIBLE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Water Level Forecaster with Spatiotemporal Causal
  Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00515v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00515v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghcul Hong, Yunjin Choi, Jong-June Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting the water level of the Han river is important to control traffic
and avoid natural disasters. There are many variables related to the Han river
and they are intricately connected. In this work, we propose a novel
transformer that exploits the causal relationship based on the prior knowledge
among the variables and forecasts the water level at the Jamsu bridge in the
Han river. Our proposed model considers both spatial and temporal causation by
formalizing the causal structure as a multilayer network and using masking
methods. Due to this approach, we can have interpretability that consistent
with prior knowledge. In real data analysis, we use the Han river dataset from
2016 to 2021 and compare the proposed model with deep learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Diffusion</span> Model Predicts 3D Shapes from 2D Microscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik J. E. Waibel, Ernst Röell, Bastian Rieck, Raja Giryes, Carsten Marr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a special type of generative model, capable of
synthesising new data from a learnt distribution. We introduce DISPR, a
diffusion-based model for solving the inverse problem of three-dimensional (3D)
cell shape prediction from two-dimensional (2D) single cell microscopy images.
Using the 2D microscopy image as a prior, DISPR is conditioned to predict
realistic 3D shape reconstructions. To showcase the applicability of DISPR as a
data augmentation tool in a feature-based single cell classification task, we
extract morphological features from the red blood cells grouped into six highly
imbalanced classes. Adding features from the DISPR predictions to the three
minority classes improved the macro F1 score from $F1_\text{macro} = 55.2 \pm
4.6\%$ to $F1_\text{macro} = 72.2 \pm 4.9\%$. We thus demonstrate that
diffusion models can be successfully applied to inverse biomedical problems,
and that they learn to reconstruct 3D shapes with realistic morphological
features from 2D microscopy images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't PANIC: Prototypical Additive Neural Network for Interpretable
  Classification of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Nuno Wolf, Sebastian Pölsterl, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) has a complex and multifactorial etiology, which
requires integrating information about neuroanatomy, genetics, and
cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep
learning approaches combined image and tabular information to improve
diagnostic performance. However, the black-box nature of such neural networks
is still a barrier for clinical applications, in which understanding the
decision of a heterogeneous model is integral. We propose PANIC, a prototypical
additive neural network for interpretable AD classification that integrates 3D
image and tabular data. It is interpretable by design and, thus, avoids the
need for post-hoc explanations that try to approximate the decision of a
network. Our results demonstrate that PANIC achieves state-of-the-art
performance in AD classification, while directly providing local and global
explanations. Finally, we show that PANIC extracts biologically meaningful
signatures of AD, and satisfies a set of desirable desiderata for trustworthy
machine learning. Our implementation is available at
https://github.com/ai-med/PANIC .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in proceedings of Information Processing In Medical
  Imaging 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Feature Selection with Neuron Evolution in Sparse Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Atashgahi, Xuhao Zhang, Neil Kichler, Shiwei Liu, Lu Yin, Mykola Pechenizkiy, Raymond Veldhuis, Decebal Constantin Mocanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature selection that selects an informative subset of variables from data
not only enhances the model interpretability and performance but also
alleviates the resource demands. Recently, there has been growing attention on
feature selection using neural networks. However, existing methods usually
suffer from high computational costs when applied to high-dimensional datasets.
In this paper, inspired by evolution processes, we propose a novel
resource-efficient supervised feature selection method using sparse neural
networks, named \enquote{NeuroFS}. By gradually pruning the uninformative
features from the input layer of a sparse neural network trained from scratch,
NeuroFS derives an informative subset of features efficiently. By performing
several experiments on $11$ low and high-dimensional real-world benchmarks of
different types, we demonstrate that NeuroFS achieves the highest ranking-based
score among the considered state-of-the-art supervised feature selection
models. The code is available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards better traffic volume estimation: Tackling both underdetermined
  and non-equilibrium problems via a correlation-adaptive graph convolution
  network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Nie, Guoyang Qin, Yunpeng Wang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic volume is an indispensable ingredient to provide fine-grained
information for traffic management and control. However, due to limited
deployment of traffic sensors, obtaining full-scale volume information is far
from easy. Existing works on this topic primarily focus on improving the
overall estimation accuracy of a particular method and ignore the underlying
challenges of volume estimation, thereby having inferior performances on some
critical tasks. This paper studies two key problems with regard to traffic
volume estimation: (1) underdetermined traffic flows caused by undetected
movements, and (2) non-equilibrium traffic flows arise from congestion
propagation. Here we demonstrate a graph-based deep learning method that can
offer a data-driven, model-free and correlation adaptive approach to tackle the
above issues and perform accurate network-wide traffic volume estimation.
Particularly, in order to quantify the dynamic and nonlinear relationships
between traffic speed and volume for the estimation of underdetermined flows, a
speed patternadaptive adjacent matrix based on graph attention is developed and
integrated into the graph convolution process, to capture non-local
correlations between sensors. To measure the impacts of non-equilibrium flows,
a temporal masked and clipped attention combined with a gated temporal
convolution layer is customized to capture time-asynchronous correlations
between upstream and downstream sensors. We then evaluate our model on a
real-world highway traffic volume dataset and compare it with several benchmark
models. It is demonstrated that the proposed model achieves high estimation
accuracy even under 20% sensor coverage rate and outperforms other baselines
significantly, especially on underdetermined and non-equilibrium flow
locations. Furthermore, comprehensive quantitative model analysis are also
carried out to justify the model designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Nash Equilibrium Approximator Learnable? <span class="chip">AAMAS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.07472v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.07472v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Duan, Wenhan Huang, Dinghuai Zhang, Yali Du, Jun Wang, Yaodong Yang, Xiaotie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the learnability of the function approximator
that approximates Nash equilibrium (NE) for games generated from a
distribution. First, we offer a generalization bound using the Probably
Approximately Correct (PAC) learning model. The bound describes the gap between
the expected loss and empirical loss of the NE approximator. Afterward, we
prove the agnostic PAC learnability of the Nash approximator. In addition to
theoretical analysis, we demonstrate an application of NE approximator in
experiments. The trained NE approximator can be used to warm-start and
accelerate classical NE solvers. Together, our results show the practicability
of approximating NE through function approximation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAMAS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Component Segmentation of Engineering Drawings Using Graph Convolutional
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentai Zhang, Joe Joseph, Yue Yin, Liuyue Xie, Tomotake Furuhata, Soji Yamakawa, Kenji Shimada, Levent Burak Kara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a data-driven framework to automate the vectorization and machine
interpretation of 2D engineering part drawings. In industrial settings, most
manufacturing engineers still rely on manual reads to identify the topological
and manufacturing requirements from drawings submitted by designers. The
interpretation process is laborious and time-consuming, which severely inhibits
the efficiency of part quotation and manufacturing tasks. While recent advances
in image-based computer vision methods have demonstrated great potential in
interpreting natural images through semantic segmentation approaches, the
application of such methods in parsing engineering technical drawings into
semantically accurate components remains a significant challenge. The severe
pixel sparsity in engineering drawings also restricts the effective
featurization of image-based data-driven methods. To overcome these challenges,
we propose a deep learning based framework that predicts the semantic type of
each vectorized component. Taking a raster image as input, we vectorize all
components through thinning, stroke tracing, and cubic bezier fitting. Then a
graph of such components is generated based on the connectivity between the
components. Finally, a graph convolutional neural network is trained on this
graph data to identify the semantic type of each component. We test our
framework in the context of semantic segmentation of text, dimension and,
contour components in engineering drawings. Results show that our method yields
the best performance compared to recent image, and graph-based segmentation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted to Computers in Industry</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Safe Reinforcement Learning via Action Projection using
  Reachability Analysis and Polynomial Zonotopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Kochdumper, Hanna Krasowski, Xiao Wang, Stanley Bak, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While reinforcement learning produces very promising results for many
applications, its main disadvantage is the lack of safety guarantees, which
prevents its use in safety-critical systems. In this work, we address this
issue by a safety shield for nonlinear continuous systems that solve
reach-avoid tasks. Our safety shield prevents applying potentially unsafe
actions from a reinforcement learning agent by projecting the proposed action
to the closest safe action. This approach is called action projection and is
implemented via mixed-integer optimization. The safety constraints for action
projection are obtained by applying parameterized reachability analysis using
polynomial zonotopes, which enables to accurately capture the nonlinear effects
of the actions on the system. In contrast to other state-of-the-art approaches
for action projection, our safety shield can efficiently handle input
constraints and dynamic obstacles, eases incorporation of the spatial robot
dimensions into the safety constraints, guarantees robust safety despite
process noise and measurement errors, and is well suited for high-dimensional
systems, as we demonstrate on several challenging benchmark systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combinatorial Pure Exploration of Causal Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07883v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07883v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuoya Xiong, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combinatorial pure exploration of causal bandits is the following online
learning task: given a causal graph with unknown causal inference
distributions, in each round we choose a subset of variables to intervene or do
no intervention, and observe the random outcomes of all random variables, with
the goal that using as few rounds as possible, we can output an intervention
that gives the best (or almost best) expected outcome on the reward variable
$Y$ with probability at least $1-\delta$, where $\delta$ is a given confidence
level. We provide the first gap-dependent and fully adaptive pure exploration
algorithms on two types of causal models -- the binary generalized linear model
(BGLM) and general graphs. For BGLM, our algorithm is the first to be designed
specifically for this setting and achieves polynomial sample complexity, while
all existing algorithms for general graphs have either sample complexity
exponential to the graph size or some unreasonable assumptions. For general
graphs, our algorithm provides a significant improvement on sample complexity,
and it nearly matches the lower bound we prove. Our algorithms achieve such
improvement by a novel integration of prior causal bandit algorithms and prior
adaptive pure exploration algorithms, the former of which utilize the rich
observational feedback in causal bandits but are not adaptive to reward gaps,
while the latter of which have the issue in reverse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Liu, Jie Song, Yihe Zhou, Na Yu, Kaixuan Chen, Zunlei Feng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep cooperative multi-agent reinforcement learning has demonstrated its
remarkable success over a wide spectrum of complex control tasks. However,
recent advances in multi-agent learning mainly focus on value decomposition
while leaving entity interactions still intertwined, which easily leads to
over-fitting on noisy interactions between entities. In this work, we introduce
a novel interactiOn Pattern disenTangling (OPT) method, to disentangle not only
the joint value function into agent-wise value functions for decentralized
execution, but also the entity interactions into interaction prototypes, each
of which represents an underlying interaction pattern within a subgroup of the
entities. OPT facilitates filtering the noisy interactions between irrelevant
entities and thus significantly improves generalizability as well as
interpretability. Specifically, OPT introduces a sparse disagreement mechanism
to encourage sparsity and diversity among discovered interaction prototypes.
Then the model selectively restructures these prototypes into a compact
interaction pattern by an aggregator with learnable weights. To alleviate the
training instability issue caused by partial observability, we propose to
maximize the mutual information between the aggregation weights and the history
behaviors of each agent. Experiments on both single-task and multi-task
benchmarks demonstrate that the proposed method yields results superior to the
state-of-the-art counterparts. Our code is available at
https://github.com/liushunyu/OPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time
  Series Forecasting <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, Yanjie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distribution shift in Time Series Forecasting (TSF), indicating series
distribution changes over time, largely hinders the performance of TSF models.
Existing works towards distribution shift in time series are mostly limited in
the quantification of distribution and, more importantly, overlook the
potential shift between lookback and horizon windows. To address above
challenges, we systematically summarize the distribution shift in TSF into two
categories. Regarding lookback windows as input-space and horizon windows as
output-space, there exist (i) intra-space shift, that the distribution within
the input-space keeps shifted over time, and (ii) inter-space shift, that the
distribution is shifted between input-space and output-space. Then we
introduce, Dish-TS, a general neural paradigm for alleviating distribution
shift in TSF. Specifically, for better distribution estimation, we propose the
coefficient net (CONET), which can be any neural architectures, to map input
sequences into learnable distribution coefficients. To relieve intra-space and
inter-space shift, we organize Dish-TS as a Dual-CONET framework to separately
learn the distribution of input- and output-space, which naturally captures the
distribution difference of two spaces. In addition, we introduce a more
effective training strategy for intractable CONET learning. Finally, we conduct
extensive experiments on several datasets coupled with different
state-of-the-art forecasting models. Experimental results show Dish-TS
consistently boosts them with a more than 20% average improvement. Code is
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dataset Distillation Using Parameter Pruning <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14609v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14609v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many fields, the acquisition of advanced models depends on large datasets,
making data storage and model training expensive. As a solution, dataset
distillation can synthesize a small dataset that preserves most information of
the original large dataset. The recently proposed dataset distillation method
by matching network parameters has been proven effective for several datasets.
However, the dimensions of network parameters are typically large. Furthermore,
some parameters are difficult to match during the distillation process,
degrading distillation performance. Based on this observation, this study
proposes a novel dataset distillation method based on parameter pruning that
solves the problem. The proposed method can synthesize more robust distilled
datasets and improve distillation performance by pruning difficult-to-match
parameters during the distillation process. Experimental results on three
datasets show that the proposed method outperforms other state-of-the-art
dataset distillation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted as a journal paper at IEEE SPL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music Mixing Style Transfer: A Contrastive Learning Approach to
  Disentangle Audio Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Stefan Uhlich, Kyogu Lee, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an end-to-end music mixing style transfer system that converts the
mixing style of an input multitrack to that of a reference song. This is
achieved with an encoder pre-trained with a contrastive objective to extract
only audio effects related information from a reference music recording. All
our models are trained in a self-supervised manner from an already-processed
wet multitrack dataset with an effective data preprocessing method that
alleviates the data scarcity of obtaining unprocessed dry data. We analyze the
proposed encoder for the disentanglement capability of audio effects and also
validate its performance for mixing style transfer through both objective and
subjective evaluations. From the results, we show the proposed system not only
converts the mixing style of multitrack audio close to a reference but is also
robust with mixture-wise style transfer upon using a music source separation
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and
  Its Application to Tumour Segmentation for Breast Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10325v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10325v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongquan Yang, Fengling Li, Yani Wei, Jie Chen, Ning Chen, Hong Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of the combination of
machine learning and logical reasoning, including data-driven logical
reasoning, knowledge driven machine learning and abductive learning, in
inventing advanced artificial intelligence technologies. One-step abductive
multi-target learning (OSAMTL), an approach inspired by abductive learning, via
simply combining machine learning and logical reasoning in a one-step balanced
way, has as well shown its effectiveness in handling complex noisy labels of a
single noisy sample in medical histopathology whole slide image analysis
(MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy
samples (DiNS) are provided for a learning task. In this paper, giving
definition of DiNS, we propose one-step abductive multi-target learning with
DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels
of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in
MHWSIA, we show that OSAMTL-DiNS is able to enable various state-of-the-art
approaches for learning from noisy labels to achieve more rational predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting
  Epidemics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10696v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10696v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhurima Panja, Tanujit Chakraborty, Uttam Kumar, Nan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infectious diseases remain among the top contributors to human illness and
death worldwide, among which many diseases produce epidemic waves of infection.
The unavailability of specific drugs and ready-to-use vaccines to prevent most
of these epidemics makes the situation worse. These force public health
officials and policymakers to rely on early warning systems generated by
reliable and accurate forecasts of epidemics. Accurate forecasts of epidemics
can assist stakeholders in tailoring countermeasures, such as vaccination
campaigns, staff scheduling, and resource allocation, to the situation at hand,
which could translate to reductions in the impact of a disease. Unfortunately,
most of these past epidemics exhibit nonlinear and non-stationary
characteristics due to their spreading fluctuations based on seasonal-dependent
variability and the nature of these epidemics. We analyse a wide variety of
epidemic time series datasets using a maximal overlap discrete wavelet
transform (MODWT) based autoregressive neural network and call it EWNet model.
MODWT techniques effectively characterize non-stationary behavior and seasonal
dependencies in the epidemic time series and improve the nonlinear forecasting
scheme of the autoregressive neural network in the proposed ensemble wavelet
network framework. From a nonlinear time series viewpoint, we explore the
asymptotic stationarity of the proposed EWNet model to show the asymptotic
behavior of the associated Markov Chain. We also theoretically investigate the
effect of learning stability and the choice of hidden neurons in the proposal.
From a practical perspective, we compare our proposed EWNet framework with
several statistical, machine learning, and deep learning models. Experimental
results show that the proposed EWNet is highly competitive compared to the
state-of-the-art epidemic forecasting methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegViz: A federated-learning based framework for multi-or<span class="highlight-title">gan</span>
  segmentation on heterogeneous data sets with partial annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adway U. Kanhere, Pranav Kulkarni, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation is one of the most primary tasks in deep learning for medical
imaging, owing to its multiple downstream clinical applications. However,
generating manual annotations for medical images is time-consuming, requires
high skill, and is an expensive effort, especially for 3D images. One potential
solution is to aggregate knowledge from partially annotated datasets from
multiple groups to collaboratively train global models using Federated
Learning. To this end, we propose SegViz, a federated learning-based framework
to train a segmentation model from distributed non-i.i.d datasets with partial
annotations. The performance of SegViz was compared against training individual
models separately on each dataset as well as centrally aggregating all the
datasets in one place and training a single model. The SegViz framework using
FedBN as the aggregation strategy demonstrated excellent performance on the
external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for
segmentation of liver, spleen, pancreas, and kidneys, respectively,
significantly ($p<0.05$) better (except spleen) than the dice scores of 0.87,
0.83, 0.42, and 0.48 for the baseline models. In contrast, the central
aggregation model significantly ($p<0.05$) performed poorly on the test dataset
with dice scores of 0.65, 0, 0.55, and 0.68. Our results demonstrate the
potential of the SegViz framework to train multi-task models from distributed
datasets with partial labels. All our implementations are open-source and
available at https://anonymous.4open.science/r/SegViz-B746
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An unbiased scene graph generation (SGG) algorithm referred to as Skew
Class-balanced Re-weighting (SCR) is proposed for considering the unbiased
predicate prediction caused by the long-tailed distribution. The prior works
focus mainly on alleviating the deteriorating performances of the minority
predicate predictions, showing drastic dropping recall scores, i.e., losing the
majority predicate performances. It has not yet correctly analyzed the
trade-off between majority and minority predicate performances in the limited
SGG datasets. In this paper, to alleviate the issue, the Skew Class-balanced
Re-weighting (SCR) loss function is considered for the unbiased SGG models.
Leveraged by the skewness of biased predicate predictions, the SCR estimates
the target predicate weight coefficient and then re-weights more to the biased
predicates for better trading-off between the majority predicates and the
minority ones. Extensive experiments conducted on the standard Visual Genome
dataset and Open Image V4 \& V6 show the performances and generality of the SCR
with the traditional SGG models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Robust Spiking Neural Networks on Neuromorphic Data with
  Spatiotemporal Fragments <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11659v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11659v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic vision sensors (event cameras) are inherently suitable for
spiking neural networks (SNNs) and provide novel neuromorphic vision data for
this biomimetic model. Due to the spatiotemporal characteristics, novel data
augmentations are required to process the unconventional visual signals of
these cameras. In this paper, we propose a novel Event SpatioTemporal Fragments
(ESTF) augmentation method. It preserves the continuity of neuromorphic data by
drifting or inverting fragments of the spatiotemporal event stream to simulate
the disturbance of brightness variations, leading to more robust spiking neural
networks. Extensive experiments are performed on prevailing neuromorphic
datasets. It turns out that ESTF provides substantial improvements over pure
geometric transformations and outperforms other event data augmentation
methods. It is worth noting that the SNNs with ESTF achieve the
state-of-the-art accuracy of 83.9\% on the CIFAR10-DVS dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Stronger Spiking Neural Networks with Biomimetic Adaptive
  Internal Association Neurons <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Shen, Yihao Luo, Xiang Cao, Liangqi Zhang, Juyu Xiao, Tianjiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the third generation of neural networks, spiking neural networks (SNNs)
are dedicated to exploring more insightful neural mechanisms to achieve
near-biological intelligence. Intuitively, biomimetic mechanisms are crucial to
understanding and improving SNNs. For example, the associative long-term
potentiation (ALTP) phenomenon suggests that in addition to learning mechanisms
between neurons, there are associative effects within neurons. However, most
existing methods only focus on the former and lack exploration of the internal
association effects. In this paper, we propose a novel Adaptive Internal
Association~(AIA) neuron model to establish previously ignored influences
within neurons. Consistent with the ALTP phenomenon, the AIA neuron model is
adaptive to input stimuli, and internal associative learning occurs only when
both dendrites are stimulated at the same time. In addition, we employ weighted
weights to measure internal associations and introduce intermediate caches to
reduce the volatility of associations. Extensive experiments on prevailing
neuromorphic datasets show that the proposed method can potentiate or depress
the firing of spikes more specifically, resulting in better performance with
fewer spikes. It is worth noting that without adding any parameters at
inference, the AIA model achieves state-of-the-art performance on
DVS-CIFAR10~(83.9\%) and N-CARS~(95.64\%) datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Causes of Arctic Amplification via Deep Learning based
  Time-series Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahara Ali, Omar Faruque, Jianwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The warming of the Arctic, also known as Arctic amplification, is led by
several atmospheric and oceanic drivers, however, the details of its underlying
thermodynamic causes are still unknown. Inferring the causal effects of
atmospheric processes on sea ice melt using fixed treatment effect strategies
leads to unrealistic counterfactual estimations. Such models are also prone to
bias due to time-varying confoundedness. In order to tackle these challenges,
we propose TCINet - time-series causal inference model to infer causation under
continuous treatment using recurrent neural networks. Through experiments on
synthetic and observational data, we show how our research can substantially
improve the ability to quantify the leading causes of Arctic sea ice melt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, Diana Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation aims to segment an image into semantic
regions according to text descriptions, which may not have been seen during
training. Recent two-stage methods first generate class-agnostic mask proposals
and then leverage pre-trained vision-language models, e.g., CLIP, to classify
masked regions. We identify the performance bottleneck of this paradigm to be
the pre-trained CLIP model, since it does not perform well on masked images. To
address this, we propose to finetune CLIP on a collection of masked image
regions and their corresponding text descriptions. We collect training data by
mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to
match masked image regions to nouns in the image captions. Compared with the
more precise and manually annotated segmentation labels with fixed classes
(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain
CLIP's generalization ability. Along with finetuning the entire model, we
utilize the "blank" areas in masked images using a method we dub mask prompt
tuning. Experiments demonstrate mask prompt tuning brings significant
improvement without modifying any weights of CLIP, and it can further improve a
fully finetuned model. In particular, when trained on COCO and evaluated on
ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the
previous state-of-the-art. For the first time, open-vocabulary generalist
models match the performance of supervised specialist models in 2017 without
dataset-specific adaptations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://jeff-liangf.github.io/projects/ovseg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Compression for Noisy Storage Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.07725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.07725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Kristy Choi, Xin Zheng, Tsachy Weissman, Stefano Ermon, H. -S. Philip Wong, Armin Alaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compression and efficient storage of neural network (NN) parameters is
critical for applications that run on resource-constrained devices. Despite the
significant progress in NN model compression, there has been considerably less
investigation in the actual \textit{physical} storage of NN parameters.
Conventionally, model compression and physical storage are decoupled, as
digital storage media with error-correcting codes (ECCs) provide robust
error-free storage. However, this decoupled approach is inefficient as it
ignores the overparameterization present in most NNs and forces the memory
device to allocate the same amount of resources to every bit of information
regardless of its importance. In this work, we investigate analog memory
devices as an alternative to digital media -- one that naturally provides a way
to add more protection for significant bits unlike its counterpart, but is
noisy and may compromise the stored model's performance if used naively. We
develop a variety of robust coding strategies for NN weight storage on analog
devices, and propose an approach to jointly optimize model compression and
memory resource allocation. We then demonstrate the efficacy of our approach on
models trained on MNIST, CIFAR-10 and ImageNet datasets for existing
compression techniques. Compared to conventional error-free digital storage,
our method reduces the memory footprint by up to one order of magnitude,
without significantly compromising the stored model's accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the ACM Transactions on Embedded Computing Systems
  (TECS), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeMAE: <span class="highlight-title">Self-Supervised</span> Representations of Time Series with Decoupled
  Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the expressive capacity of deep learning-based time series models
with self-supervised pre-training has become ever-increasingly prevalent in
time series classification. Even though numerous efforts have been devoted to
developing self-supervised models for time series data, we argue that the
current methods are not sufficient to learn optimal time series representations
due to solely unidirectional encoding over sparse point-wise input units. In
this work, we propose TimeMAE, a novel self-supervised paradigm for learning
transferrable time series representations based on transformer networks. The
distinct characteristics of the TimeMAE lie in processing each time series into
a sequence of non-overlapping sub-series via window-slicing partitioning,
followed by random masking strategies over the semantic units of localized
sub-series. Such a simple yet effective setting can help us achieve the goal of
killing three birds with one stone, i.e., (1) learning enriched contextual
representations of time series with a bidirectional encoding scheme; (2)
increasing the information density of basic semantic units; (3) efficiently
encoding representations of time series using transformer networks.
Nevertheless, it is a non-trivial to perform reconstructing task over such a
novel formulated modeling paradigm. To solve the discrepancy issue incurred by
newly injected masked embeddings, we design a decoupled autoencoder
architecture, which learns the representations of visible (unmasked) positions
and masked ones with two different encoder modules, respectively. Furthermore,
we construct two types of informative targets to accomplish the corresponding
pretext tasks. One is to create a tokenizer module that assigns a codeword to
each masked region, allowing the masked codeword classification (MCC) task to
be completed effectively...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TRANSACTIONS ON KNOWLEDGE AND DATA
  ENGINEERING(TKDE), under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Liu, Yihe Zhou, Jie Song, Tongya Zheng, Kaixuan Chen, Tongtian Zhu, Zunlei Feng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Value Decomposition (VD) aims to deduce the contributions of agents for
decentralized policies in the presence of only global rewards, and has recently
emerged as a powerful credit assignment paradigm for tackling cooperative
Multi-Agent Reinforcement Learning (MARL) problems. One of the main challenges
in VD is to promote diverse behaviors among agents, while existing methods
directly encourage the diversity of learned agent networks with various
strategies. However, we argue that these dedicated designs for agent networks
are still limited by the indistinguishable VD network, leading to homogeneous
agent behaviors and thus downgrading the cooperation capability. In this paper,
we propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly
boosting the credit-level distinguishability of the VD network to break the
bottleneck of multi-agent diversity. Specifically, our approach leverages
contrastive learning to maximize the mutual information between the temporal
credits and identity representations of different agents, encouraging the full
expressiveness of credit assignment and further the emergence of
individualities. The algorithm implementation of the proposed CIA module is
simple yet effective that can be readily incorporated into various VD
architectures. Experiments on the SMAC benchmarks and across different VD
backbones demonstrate that the proposed method yields results superior to the
state-of-the-art counterparts. Our code is available at
https://github.com/liushunyu/CIA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ eDiff-I: Text-to-Image <span class="highlight-title">Diffusion</span> Models with an Ensemble of Expert
  Denoisers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01324v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01324v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale diffusion-based generative models have led to breakthroughs in
text-conditioned high-resolution image synthesis. Starting from random noise,
such text-to-image diffusion models gradually synthesize images in an iterative
fashion while conditioning on text prompts. We find that their synthesis
behavior qualitatively changes throughout this process: Early in sampling,
generation strongly relies on the text prompt to generate text-aligned content,
while later, the text conditioning is almost entirely ignored. This suggests
that sharing model parameters throughout the entire generation process may not
be ideal. Therefore, in contrast to existing works, we propose to train an
ensemble of text-to-image diffusion models specialized for different synthesis
stages. To maintain training efficiency, we initially train a single model,
which is then split into specialized models that are trained for the specific
stages of the iterative generation process. Our ensemble of diffusion models,
called eDiff-I, results in improved text alignment while maintaining the same
inference computation cost and preserving high visual quality, outperforming
previous large-scale text-to-image diffusion models on the standard benchmark.
In addition, we train our model to exploit a variety of embeddings for
conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We
show that these different embeddings lead to different behaviors. Notably, the
CLIP image embedding allows an intuitive way of transferring the style of a
reference image to the target text-to-image output. Lastly, we show a technique
that enables eDiff-I's "paint-with-words" capability. A user can select the
word in the input text and paint it in a canvas to control the output, which is
very handy for crafting the desired image in mind. The project page is
available at https://deepimagination.cc/eDiff-I/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Can Ground Earlier than See: An Effective and Efficient Pipeline for
  Temporal Sentence Grounding in Compressed Videos <span class="chip">CVPR-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fang, Daizong Liu, Pan Zhou, Guoshun Nan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a
target moment semantically according to a sentence query. Although previous
respectable works have made decent success, they only focus on high-level
visual features extracted from the consecutive decoded frames and fail to
handle the compressed videos for query modelling, suffering from insufficient
representation capability and significant computational complexity during
training and testing. In this paper, we pose a new setting, compressed-domain
TSG, which directly utilizes compressed videos rather than fully-decompressed
frames as the visual input. To handle the raw video bit-stream input, we
propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF)
framework, which extracts and aggregates three kinds of low-level visual
features (I-frame, motion vector and residual features) for effective and
efficient grounding. Particularly, instead of encoding the whole decoded frames
like previous works, we capture the appearance representation by only learning
the I-frame feature to reduce delay or latency. Besides, we explore the motion
information not only by learning the motion vector feature, but also by
exploring the relations of neighboring frames via the residual feature. In this
way, a three-branch spatial-temporal attention layer with an adaptive
motion-appearance fusion module is further designed to extract and aggregate
both appearance and motion information for the final grounding. Experiments on
three challenging datasets shows that our TCSF achieves better performance than
other state-of-the-art methods with lower complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuseRoll: Multi-track multi-category music generation based on
  <span class="highlight-title">diffusion</span> model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative models have shown remarkable progress in
music generation. However, most existing methods focus on generating monophonic
or homophonic music, while the generation of polyphonic and multi-track music
with rich attributes is still a challenging task. In this paper, we propose a
novel approach for multi-track, multi-attribute symphonic music generation
using the diffusion model. Specifically, we generate piano-roll representations
with a diffusion model and map them to MIDI format for output. To capture rich
attribute information, we introduce a color coding scheme to encode note
sequences into color and position information that represents pitch,velocity,
and instrument. This scheme enables a seamless mapping between discrete music
sequences and continuous images. We also propose a post-processing method to
optimize the generated scores for better performance. Experimental results show
that our method outperforms state-of-the-art methods in terms of polyphonic
music generation with rich attribute information compared to the figure
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation-Guided Multi-Level Unified Network for Video Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Cheng, Xiangyu Wu, Dong Shen, Hezheng Lin, Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video grounding aims to locate the timestamps best matching the query
description within an untrimmed video. Prevalent methods can be divided into
moment-level and clip-level frameworks. Moment-level approaches directly
predict the probability of each transient moment to be the boundary in a global
perspective, and they usually perform better in coarse grounding. On the other
hand, clip-level ones aggregate the moments in different time windows into
proposals and then deduce the most similar one, leading to its advantage in
fine-grained grounding. In this paper, we propose a multi-level unified
framework to enhance performance by leveraging the merits of both moment-level
and clip-level methods. Moreover, a novel generation-guided paradigm in both
levels is adopted. It introduces a multi-modal generator to produce the
implicit boundary feature and clip feature, later regarded as queries to
calculate the boundary scores by a discriminator. The generation-guided
solution enhances video grounding from a two-unique-modals' match task to a
cross-modal attention task, which steps out of the previous framework and
obtains notable gains. The proposed Generation-guided Multi-level Unified
network (GMU) surpasses previous methods and reaches State-Of-The-Art on
various benchmarks with disparate features, e.g., Charades-STA, ActivityNet
captions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAT: Causal Audio <span class="highlight-title">Transformer</span> for Audio Classification <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Liu, Hanlin Lu, Jianbo Yuan, Xinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention-based Transformers have been increasingly applied to audio
classification because of their global receptive field and ability to handle
long-term dependency. However, the existing frameworks which are mainly
extended from the Vision Transformers are not perfectly compatible with audio
signals. In this paper, we introduce a Causal Audio Transformer (CAT)
consisting of a Multi-Resolution Multi-Feature (MRMF) feature extraction with
an acoustic attention block for more optimized audio modeling. In addition, we
propose a causal module that alleviates over-fitting, helps with knowledge
transfer, and improves interpretability. CAT obtains higher or comparable
state-of-the-art classification performance on ESC50, AudioSet and UrbanSound8K
datasets, and can be easily generalized to other Transformer-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuS-X: Training-Free Name-Only Transfer of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishaal Udandarao, Ankush Gupta, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free "name-only transfer" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DasFormer: Deep Alternating Spectrogram <span class="highlight-title">Transformer</span> for
  Multi/Single-Channel Speech Separation <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Wang, Xiangyu Kong, Xiulian Peng, Mahmood Movassagh, Vinod Prakash, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the task of speech separation, previous study usually treats
multi-channel and single-channel scenarios as two research tracks with
specialized solutions developed respectively. Instead, we propose a simple and
unified architecture - DasFormer (Deep alternating spectrogram transFormer) to
handle both of them in the challenging reverberant environments. Unlike
frame-wise sequence modeling, each TF-bin in the spectrogram is assigned with
an embedding encoding spectral and spatial information. With such input,
DasFormer is then formed by multiple repetition of simple blocks each of which
integrates 1) two multi-head self-attention (MHSA) modules alternately
processing within each frequency bin & temporal frame of the spectrogram 2)
MBConv before each MHSA for modeling local features on the spectrogram.
Experiments show that DasFormer has a powerful ability to model the
time-frequency representation, whose performance far exceeds the current SOTA
models in multi-channel speech separation, and also achieves single-channel
SOTA in the more challenging yet realistic reverberation scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Video Compression with Diverse Contexts <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14402v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14402v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Li, Bin Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any video codecs, the coding efficiency highly relies on whether the
current signal to be encoded can find the relevant contexts from the previous
reconstructed signals. Traditional codec has verified more contexts bring
substantial coding gain, but in a time-consuming manner. However, for the
emerging neural video codec (NVC), its contexts are still limited, leading to
low compression ratio. To boost NVC, this paper proposes increasing the context
diversity in both temporal and spatial dimensions. First, we guide the model to
learn hierarchical quality patterns across frames, which enriches long-term and
yet high-quality temporal contexts. Furthermore, to tap the potential of
optical flow-based coding framework, we introduce a group-based offset
diversity where the cross-group interaction is proposed for better context
mining. In addition, this paper also adopts a quadtree-based partition to
increase spatial context diversity when encoding the latent representation in
parallel. Experiments show that our codec obtains 23.5% bitrate saving over
previous SOTA NVC. Better yet, our codec has surpassed the under-developing
next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in
terms of PSNR. The codes are at https://github.com/microsoft/DCVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023. Codes are at https://github.com/microsoft/DCVC</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-13T00:00:00Z">2023-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Visual Language Maps for Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While interacting in the world is a multi-sensory experience, many robots
continue to predominantly rely on visual perception to map and navigate in
their environments. In this work, we propose Audio-Visual-Language Maps
(AVLMaps), a unified 3D spatial map representation for storing cross-modal
information from audio, visual, and language cues. AVLMaps integrate the
open-vocabulary capabilities of multimodal foundation models pre-trained on
Internet-scale data by fusing their features into a centralized 3D voxel grid.
In the context of navigation, we show that AVLMaps enable robot systems to
index goals in the map based on multimodal queries, e.g., textual descriptions,
images, or audio snippets of landmarks. In particular, the addition of audio
information enables robots to more reliably disambiguate goal locations.
Extensive experiments in simulation show that AVLMaps enable zero-shot
multimodal goal navigation from multimodal prompts and provide 50% better
recall in ambiguous scenarios. These capabilities extend to mobile robots in
the real world - navigating to landmarks referring to visual, audio, and
spatial concepts. Videos and code are available at: https://avlmaps.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://avlmaps.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architext: Language-Driven Generative Architecture Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Galanos, Antonios Liapis, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architectural design is a highly complex practice that involves a wide
diversity of disciplines, technologies, proprietary design software, expertise,
and an almost infinite number of constraints, across a vast array of design
tasks. Enabling intuitive, accessible, and scalable design processes is an
important step towards performance-driven and sustainable design for all. To
that end, we introduce Architext, a novel semantic generation assistive tool.
Architext enables design generation with only natural language prompts, given
to large-scale Language Models, as input. We conduct a thorough quantitative
evaluation of Architext's downstream task performance, focusing on semantic
accuracy and diversity for a number of pre-trained language models ranging from
120 million to 6 billion parameters. Architext models are able to learn the
specific design task, generating valid residential layouts at a near 100\%
rate. Accuracy shows great improvement when scaling the models, with the
largest model (GPT-J) yielding impressive accuracy ranging between 25% to over
80% for different prompt categories. We open source the finetuned Architext
models and our synthetic dataset, hoping to inspire experimentation in this
exciting area of design research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMOM: Adaptive Masking over Masking for Conditional Masked Language
  Model <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yisheng Xiao, Ruiyang Xu, Lijun Wu, Juntao Li, Tao Qin, Yan-Tie Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based autoregressive (AR) methods have achieved appealing
performance for varied sequence-to-sequence generation tasks, e.g., neural
machine translation, summarization, and code generation, but suffer from low
inference efficiency. To speed up the inference stage, many non-autoregressive
(NAR) strategies have been proposed in the past few years. Among them, the
conditional masked language model (CMLM) is one of the most versatile
frameworks, as it can support many different sequence generation scenarios and
achieve very competitive performance on these tasks. In this paper, we further
introduce a simple yet effective adaptive masking over masking strategy to
enhance the refinement capability of the decoder and make the encoder
optimization easier. Experiments on \textbf{3} different tasks (neural machine
translation, summarization, and code generation) with \textbf{15} datasets in
total confirm that our proposed simple method achieves significant performance
improvement over the strong CMLM model. Surprisingly, our proposed model yields
state-of-the-art performance on neural machine translation (\textbf{34.62} BLEU
on WMT16 EN$\to$RO, \textbf{34.82} BLEU on WMT16 RO$\to$EN, and \textbf{34.84}
BLEU on IWSLT De$\to$En) and even better performance than the \textbf{AR}
Transformer on \textbf{7} benchmark datasets with at least \textbf{2.2$\times$}
speedup. Our code is available at GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-tuning Via <span class="highlight-title">Prompt</span>s Makes NLP Models Adversarially Robust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token's hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations, such
as word-level synonym substitutions. In this work, we demonstrate surprising
gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an
alternative method of adapting to downstream tasks. Rather than modifying the
model (by appending an MLP head), MVP instead modifies the input (by appending
a prompt template). Across three classification datasets, MVP improves
performance against adversarial word-level synonym substitutions by an average
of 8% over standard methods and even outperforms adversarial training-based
state-of-art defenses by 3.5%. By combining MVP with adversarial training, we
achieve further improvements in robust accuracy while maintaining clean
accuracy. Finally, we conduct ablations to investigate the mechanism underlying
these gains. Notably, we find that the main causes of vulnerability of MLP can
be attributed to the misalignment between pre-training and fine-tuning tasks,
and the randomly initialized MLP parameters. Code is available at
https://github.com/acmi-lab/mvp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meet in the Middle: A New <span class="highlight-title">Pre-train</span>ing Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Nguyen, Nikos Karampatziakis, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based approaches to Sentiment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olumide Ebenezer Ojo, Hoang Thang Ta, Alexander Gelbukh, Hiram Calvo, Olaronke Oluwayemisi Adebanji, Grigori Sidorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of transfer learning methods is largely responsible for the present
breakthrough in Natural Learning Processing (NLP) tasks across multiple
domains. In order to solve the problem of sentiment detection, we examined the
performance of four different types of well-known state-of-the-art transformer
models for text classification. Models such as Bidirectional Encoder
Representations from Transformers (BERT), Robustly Optimized BERT Pre-training
Approach (RoBERTa), a distilled version of BERT (DistilBERT), and a large
bidirectional neural network architecture (XLNet) were proposed. The
performance of the four models that were used to detect disaster in the text
was compared. All the models performed well enough, indicating that
transformer-based models are suitable for the detection of disaster in text.
The RoBERTa transformer model performs best on the test dataset with a score of
82.6% and is highly recommended for quality predictions. Furthermore, we
discovered that the learning algorithms' performance was influenced by the
pre-processing techniques, the nature of words in the vocabulary, unbalanced
labeling, and the model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publisher: Springer Nature Switzerland AG, Gewerbestrasse 11, 6330
  Cham, Switzerland Published in Book Titled: Recent Developments and the New
  Directions of Research, Foundations, and Applications: Selected Papers of the
  8th World Conference on Soft Computing, February 03-05, 2022, Baku,
  Azerbaijan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of
  Synthetic and Compositional Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision-Language Models with Sparse Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Empirical Evaluation of Existing Word Embedding
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obaidullah Zaland, Muhammad Abulaish, Mohd. Fazil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-based word representations help countless Natural Language Processing
(NLP) tasks capture both semantic and syntactic regularities of the language.
In this paper, we present the characteristics of existing word embedding
approaches and analyze them with regards to many classification tasks. We
categorize the methods into two main groups - Traditional approaches mostly use
matrix factorization to produce word representations, and they are not able to
capture the semantic and syntactic regularities of the language very well.
Neural-Network based approaches, on the other hand, can capture sophisticated
regularities of the language and preserve the word relationships in the
generated word representations. We report experimental results on multiple
classification tasks and highlight the scenarios where one approach performs
better than the rest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Papoulias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new AI task and baseline solution for Inter-Subjective
Reasoning. We define inter-subjective information, to be a mixture of objective
and subjective information possibly shared by different parties. Examples may
include commodities and their objective properties as reported by IR
(Information Retrieval) systems, that need to be cross-referenced with
subjective user reviews from an online forum. For an AI system to successfully
reason about both, it needs to be able to combine symbolic reasoning of
objective facts with the shared consensus found on subjective user reviews. To
this end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as
a baseline solution for this problem. NeuroQL is a neuro-symbolic language that
extends logical unification with neural primitives for extraction and
retrieval. It can function as a target for automatic translation of
inter-subjective questions (posed in natural language) into the neuro-symbolic
code that can answer them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating multiple-choice questions for medical question answering with
  distractors and cue-masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien Sileo, Kanimozhi Uma, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical multiple-choice question answering (MCQA) is particularly difficult.
Questions may describe patient symptoms and ask for the correct diagnosis,
which requires domain knowledge and complex reasoning. Standard language
modeling pretraining alone is not sufficient to achieve the best results.
\citet{jin2020disease} showed that focusing masked language modeling on disease
name prediction when using medical encyclopedic paragraphs as input leads to
considerable MCQA accuracy improvement. In this work, we show that (1)
fine-tuning on generated MCQA dataset outperforms the masked language modeling
based objective and (2) correctly masking the cues to the answers is critical
for good performance. We release new pretraining datasets and achieve
state-of-the-art results on 4 MCQA datasets, notably +5.7\% with base-size
model on MedQA-USMLE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Biases in the Texts using an End-to-End Pipeline Approach <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Syed Raza Bashir,  Sneha, Urooj Qamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of fairness is gaining popularity in academia and industry.
Social media is especially vulnerable to media biases and toxic language and
comments. We propose a fair ML pipeline that takes a text as input and
determines whether it contains biases and toxic content. Then, based on
pre-trained word embeddings, it suggests a set of new words by substituting the
bi-ased words, the idea is to lessen the effects of those biases by replacing
them with alternative words. We compare our approach to existing fairness
models to determine its effectiveness. The results show that our proposed
pipeline can de-tect, identify, and mitigate biases in social media data
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Bias @ ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Human Subject Study of Named Entity Recognition (NER) in
  Conversational Music Recommendation Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena V. Epure, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conducted a human subject study of named entity recognition on a noisy
corpus of conversational music recommendation queries, with many irregular and
novel named entities. We evaluated the human NER linguistic behaviour in these
challenging conditions and compared it with the most common NER systems
nowadays, fine-tuned transformers. Our goal was to learn about the task to
guide the design of better evaluation methods and NER algorithms. The results
showed that NER in our context was quite hard for both human and algorithms
under a strict evaluation schema; humans had higher precision, while the model
higher recall because of entity exposure especially during pre-training; and
entity types had different error patterns (e.g. frequent typing errors for
artists). The released corpus goes beyond predefined frames of interaction and
can support future work in conversational music recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually-rich human affect perception using multimodal scene
  information <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of human affect understanding involves the ability to infer
person specific emotional states from various sources including images, speech,
and language. Affect perception from images has predominantly focused on
expressions extracted from salient face crops. However, emotions perceived by
humans rely on multiple contextual cues including social settings, foreground
interactions, and ambient visual scenes. In this work, we leverage pretrained
vision-language (VLN) models to extract descriptions of foreground context from
images. Further, we propose a multimodal context fusion (MCF) module to combine
foreground cues with the visual scene and person-based contextual information
for emotion prediction. We show the effectiveness of our proposed modular
design on two datasets associated with natural scenes and TV shows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaTroll: Few-shot Detection of State-Sponsored Trolls with <span class="highlight-title">Transformer</span>
  Adapters <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Tian, Xiuzhen Zhang, Jey Han Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-sponsored trolls are the main actors of influence campaigns on social
media and automatic troll detection is important to combat misinformation at
scale. Existing troll detection models are developed based on training data for
known campaigns (e.g.\ the influence campaign by Russia's Internet Research
Agency on the 2016 US Election), and they fall short when dealing with {\em
novel} campaigns with new targets. We propose MetaTroll, a text-based troll
detection model based on the meta-learning framework that enables high
portability and parameter-efficient adaptation to new campaigns using only a
handful of labelled samples for few-shot transfer. We introduce
\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''
campaign-specific knowledge so as to tackle catastrophic forgetting, where a
model ``forgets'' how to detect trolls from older campaigns due to continual
adaptation. Our experiments demonstrate that MetaTroll substantially
outperforms baselines and state-of-the-art few-shot text classification models.
Lastly, we explore simple approaches to extend MetaTroll to multilingual and
multimodal detection. Source code for MetaTroll is available at:
https://github.com/ltian678/metatroll-code.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures, Accepted by the Web Conference 2023 (WWW 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The System Description of dun_oscar team for The ICPR MSR Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Du, Rui Deng, Yingxin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the system submitted by dun_oscar team for the ICPR MSR
Challenge. Three subsystems for task1-task3 are descripted respectively. In
task1, we develop a visual system which includes a OCR model, a text tracker,
and a NLP classifier for distinguishing subtitles and non-subtitles. In task2,
we employ an ASR system which includes an AM with 18 layers and a 4-gram LM.
Semi-supervised learning on unlabeled data is also vital. In task3, we employ
the ASR system to improve the visual system, some false subtitles can be
corrected by a fusion module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing against Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of adversarial attacks, including targeted and
backdoor data poisoning attacks. Despite this vulnerability, robust contrastive
vision-language pretraining against adversarial attacks has remained
unaddressed. In this work, we propose RoCLIP, the first effective method for
robust pretraining {and fine-tuning} multimodal vision-language models. RoCLIP
effectively breaks the association between poisoned image-caption pairs by
considering a pool of random examples, and (1) matching every image with the
text that is most similar to its caption in the pool, and (2) matching every
caption with the image that is most similar to its image in the pool. Our
extensive experiments show that our method renders state-of-the-art targeted
data poisoning and backdoor attacks ineffective during pre-training or
fine-tuning of CLIP. In particular, RoCLIP decreases the poison and backdoor
attack success rates down to 0\% during pre-training and 1\%-4\% during
fine-tuning, and effectively improves the model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transductions and Alignments with RNN Seq2seq Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper studies the capabilities of Recurrent-Neural-Network sequence to
sequence (RNN seq2seq) models in learning four string-to-string transduction
tasks: identity, reversal, total reduplication, and input-specified
reduplication. These transductions are traditionally well studied under finite
state transducers and attributed with varying complexity. We find that RNN
seq2seq models are only able to approximate a mapping that fits the training or
in-distribution data. Attention helps significantly, but does not solve the
out-of-distribution generalization limitation. Task complexity and RNN variants
also play a role in the results. Our results are best understood in terms of
the complexity hierarchy of formal languages as opposed to that of string
transductions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 9 figures; 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Diarization with Non-autoregressive Intermediate Attractors <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Fujita, Tatsuya Komatsu, Robin Scheibler, Yusuke Kida, Tetsuji Ogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end neural diarization (EEND) with encoder-decoder-based attractors
(EDA) is a promising method to handle the whole speaker diarization problem
simultaneously with a single neural network. While the EEND model can produce
all frame-level speaker labels simultaneously, it disregards output label
dependency. In this work, we propose a novel EEND model that introduces the
label dependency between frames. The proposed method generates
non-autoregressive intermediate attractors to produce speaker labels at the
lower layers and conditions the subsequent layers with these labels. While the
proposed model works in a non-autoregressive manner, the speaker labels are
refined by referring to the whole sequence of intermediate labels. The
experiments with the two-speaker CALLHOME dataset show that the intermediate
labels with the proposed non-autoregressive intermediate attractors boost the
diarization performance. The proposed method with the deeper network benefits
more from the intermediate labels, resulting in better performance and training
throughput than EEND-EDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Single Items: Exploring User Preferences in Item Sets with the
  Conversational Playlist Curation Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users in consumption domains, like music, are often able to more efficiently
provide preferences over a set of items (e.g. a playlist or radio) than over
single items (e.g. songs). Unfortunately, this is an underexplored area of
research, with most existing recommendation systems limited to understanding
preferences over single items. Curating an item set exponentiates the search
space that recommender systems must consider (all subsets of items!): this
motivates conversational approaches-where users explicitly state or refine
their preferences and systems elicit preferences in natural language-as an
efficient way to understand user needs. We call this task conversational item
set curation and present a novel data collection methodology that efficiently
collects realistic preferences about item sets in a conversational setting by
observing both item-level and set-level feedback. We apply this methodology to
music recommendation to build the Conversational Playlist Curation Dataset
(CPCD), where we show that it leads raters to express preferences that would
not be otherwise expressed. Finally, we propose a wide range of conversational
retrieval models as baselines for this task and evaluate them on the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLADIS: A General and Large Acronym Disambiguation Benchmark <span class="chip">EACL 23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acronym Disambiguation (AD) is crucial for natural language understanding on
various sources, including biomedical reports, scientific papers, and search
engine queries. However, existing acronym disambiguation benchmarks and tools
are limited to specific domains, and the size of prior benchmarks is rather
small. To accelerate the research on acronym disambiguation, we construct a new
benchmark named GLADIS with three components: (1) a much larger acronym
dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus
with 160 million sentences; (3) three datasets that cover the general,
scientific, and biomedical domains. We then pre-train a language model,
\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,
and show the challenges and values of our new benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper at EACL 23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Vision, Text, and Layout for Universal Document Processing <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Universal Document Processing (UDOP), a foundation Document AI
model which unifies text, image, and layout modalities together with varied
task formats, including document understanding and generation. UDOP leverages
the spatial correlation between textual content and document image to model
image, text, and layout modalities with one uniform representation. With a
novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain
downstream tasks into a prompt-based sequence generation scheme. UDOP is
pretrained on both large-scale unlabeled document corpora using innovative
self-supervised objectives and diverse labeled data. UDOP also learns to
generate document images from text and layout modalities via masked image
reconstruction. To the best of our knowledge, this is the first time in the
field of document AI that one model simultaneously achieves high-quality neural
document editing and content customization. Our method sets the
state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,
across diverse data domains like finance reports, academic papers, and
websites. UDOP ranks first on the leaderboard of the Document Understanding
Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinical BERTScore: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 13 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP), demonstrate that CBERTScore more closely matches what
clinicians prefer, and release the benchmark for the community to further
develop clinically-aware ASR metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdapterSoup: Weight Averaging to Improve Generalization of <span class="highlight-title">Pretrain</span>ed
  Language Models <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, Jesse Dodge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) are trained on massive corpora, but often
need to specialize to specific domains. A parameter-efficient adaptation method
suggests training an adapter for each domain on the task of language modeling.
This leads to good in-domain scores but can be impractical for domain- or
resource-restricted settings. A solution is to use a related-domain adapter for
the novel domain at test time. In this paper, we introduce AdapterSoup, an
approach that performs weight-space averaging of adapters trained on different
domains. Our approach is embarrassingly parallel: first, we train a set of
domain-specific adapters; then, for each novel domain, we determine which
adapters should be averaged at test time. We present extensive experiments
showing that AdapterSoup consistently improves performance to new domains
without extra training. We also explore weight averaging of adapters trained on
the same domain with different hyper-parameters, and show that it preserves the
performance of a PLM on new domains while obtaining strong in-domain results.
We explore various approaches for choosing which adapters to combine, such as
text clustering and semantic similarity. We find that using clustering leads to
the most competitive results on novel domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023; camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution and Obfuscation of Neural Text Authorship: A Data Mining
  Perspective <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM SIGKDD Explorations, Vol. 25, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        BigScience Workshop,  :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Transducer Training: Reduced Memory Consumption with Sample-wise
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Braun, Erik McDermott, Roger Hsiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The neural transducer is an end-to-end model for automatic speech recognition
(ASR). While the model is well-suited for streaming ASR, the training process
remains challenging. During training, the memory requirements may quickly
exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence
lengths. In this work, we analyze the time and space complexity of a typical
transducer training setup. We propose a memory-efficient training method that
computes the transducer loss and gradients sample by sample. We present
optimizations to increase the efficiency and parallelism of the sample-wise
method. In a set of thorough benchmarks, we show that our sample-wise method
significantly reduces memory usage, and performs at competitive speed when
compared to the default batched computation. As a highlight, we manage to
compute the transducer loss and gradients for a batch size of 1024, and audio
length of 40 seconds, using only 6 GB of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accidental Learners: Spoken Language Identification in Multilingual
  <span class="highlight-title">Self-Supervised</span> Models <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Travis M. Bartley, Fei Jia, Krishna C. Puvvada, Samuel Kriman, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we extend previous self-supervised approaches for language
identification by experimenting with Conformer based architecture in a
multilingual pre-training paradigm. We find that pre-trained speech models
optimally encode language discriminatory information in lower layers. Further,
we demonstrate that the embeddings obtained from these layers are significantly
robust to classify unseen languages and different acoustic environments without
additional training. After fine-tuning a pre-trained Conformer model on the
VoxLingua107 dataset, we achieve results similar to current state-of-the-art
systems for language identification. More, our model accomplishes this with 5x
less parameters. We open-source the model through the NVIDIA NeMo toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EDU-level Extractive Summarization with Varying Summary Lengths <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Wu, Ching-Hsun Tseng, Jiayu Shang, Shengzhong Mao, Goran Nenadic, Xiao-Jun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extractive models usually formulate text summarization as extracting fixed
top-$k$ salient sentences from the document as a summary. Few works exploited
extracting finer-grained Elementary Discourse Unit (EDU) with little analysis
and justification for the extractive unit selection. Further, the selection
strategy of the fixed top-$k$ salient sentences fits the summarization need
poorly, as the number of salient sentences in different documents varies and
therefore a common or best $k$ does not exist in reality. To fill these gaps,
this paper first conducts the comparison analysis of oracle summaries based on
EDUs and sentences, which provides evidence from both theoretical and
experimental perspectives to justify and quantify that EDUs make summaries with
higher automatic evaluation scores than sentences. Then, considering this merit
of EDUs, this paper further proposes an EDU-level extractive model with Varying
summary Lengths and develops the corresponding learning algorithm. EDU-VL
learns to encode and predict probabilities of EDUs in the document, generate
multiple candidate summaries with varying lengths based on various $k$ values,
and encode and score candidate summaries, in an end-to-end training manner.
Finally, EDU-VL is experimented on single and multi-document benchmark datasets
and shows improved performances on ROUGE scores in comparison with
state-of-the-art extractive models, and further human evaluation suggests that
EDU-constituent summaries maintain good grammaticality and readability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Wang, Minghui Qiu, Chen Shi, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun Huang, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Reasonability of the Test Set for Simultaneous Machine
  Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengge Liu, Wen Zhang, Xiang Li, Jian Luan, Bin Wang, Yuhang Guo, Shuoying Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous machine translation (SimulMT) models start translation before
the end of the source sentence, making the translation monotonically aligned
with the source sentence. However, the general full-sentence translation test
set is acquired by offline translation of the entire source sentence, which is
not designed for SimulMT evaluation, making us rethink whether this will
underestimate the performance of SimulMT models. In this paper, we manually
annotate a monotonic test set based on the MuST-C English-Chinese test set,
denoted as SiMuST-C. Our human evaluation confirms the acceptability of our
annotated test set. Evaluations on three different SimulMT models verify that
the underestimation problem can be alleviated on our test set. Further
experiments show that finetuning on an automatically extracted monotonic
training set improves SimulMT models by up to 3 BLEU points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 48th IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Machine Translation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and adapt to
corrected translations in domain-specific projects. Machine translation (MT)
has achieved significant progress in the area of domain adaptation. However,
real-time adaptation remains challenging. Large-scale language models (LLMs)
have recently shown interesting capabilities of in-context learning, where they
learn to replicate certain input-output text generation patterns, without
further fine-tuning. By feeding an LLM at inference time with a prompt that
consists of a list of translation pairs, it can then simulate the domain and
style characteristics. This work aims to investigate how we can utilize
in-context learning to improve real-time adaptive MT. Our extensive experiments
show promising results at translation time. For example, GPT-3.5 can adapt to a
set of in-domain sentence pairs and/or terminology while translating a new
sentence. We observe that the translation quality with few-shot in-context
learning can surpass that of strong encoder-decoder MT systems, especially for
high-resource languages. Moreover, we investigate whether we can combine MT
from strong encoder-decoder models with fuzzy matches, which can further
improve translation quality, especially for less supported languages. We
conduct our experiments across five diverse language pairs, namely
English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French
(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image
  Captioning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng Zhang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning is a traditional vision-and-language task that aims to
generate the language description of an image. Recent studies focus on scaling
up the model size and the number of training data, which significantly increase
the cost of model training. Different to these heavy-cost models, we introduce
a lightweight image captioning framework (I-Tuning), which contains a small
number of trainable parameters. We design a novel I-Tuning cross-attention
module to connect the non-trainable pre-trained language decoder GPT2 and
vision encoder CLIP-ViT. Since most parameters are not required to be updated
during training, our framework is lightweight and fast. Experimental results
conducted on three image captioning benchmarks reveal that our framework
achieves comparable or better performance than the large-scale baseline
systems. But our models contain up to 10 times fewer trainable parameters and
require much fewer data for training compared with state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01063v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01063v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon Lee, Kyumin Park, Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alternate Intermediate Conditioning with Syllable-level and
  Character-level Targets for Japanese ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Fujita, Tatsuya Komatsu, Yusuke Kida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition directly maps input speech to
characters. However, the mapping can be problematic when several different
pronunciations should be mapped into one character or when one pronunciation is
shared among many different characters. Japanese ASR suffers the most from such
many-to-one and one-to-many mapping problems due to Japanese kanji characters.
To alleviate the problems, we introduce explicit interaction between characters
and syllables using Self-conditioned connectionist temporal classification
(CTC), in which the upper layers are ``self-conditioned'' on the intermediate
predictions from the lower layers. The proposed method utilizes character-level
and syllable-level intermediate predictions as conditioning features to deal
with mutual dependency between characters and syllables. Experimental results
on Corpus of Spontaneous Japanese show that the proposed method outperformed
the conventional multi-task and Self-conditioned CTC methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Attention Networks Can Process Bounded Hierarchical Languages <span class="chip">ACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.11115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.11115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive performance in NLP, self-attention networks were
recently proved to be limited for processing formal languages with hierarchical
structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested
parentheses of $k$ types. This suggested that natural language can be
approximated well with models that are too weak for formal languages, or that
the role of hierarchy and recursion in natural language might be limited. We
qualify this implication by proving that self-attention networks can process
$\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by
$D$, which arguably better captures the bounded hierarchical structure of
natural language. Specifically, we construct a hard-attention network with
$D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes
$\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and
$O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show
that self-attention networks trained on $\mathsf{Dyck}_{k, D}$ generalize to
longer inputs with near-perfect accuracy, and also verify the theoretical
memory advantage of self-attention networks over recurrent networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2021. 19 pages with extended appendix. Fixed a small typo in the
  formula at the end of page 5 (thank to Gabriel Faria). Code:
  https://github.com/princeton-nlp/dyck-transformer</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unsupervised Learning based Denoising of Cyber Physical System
  Data to Mitigate Security Concerns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mst Shapna Akter, Hossain Shahriar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A dataset, collected under an industrial setting, often contains a
significant portion of noises. In many cases, using trivial filters is not
enough to retrieve useful information i.e., accurate value without the noise.
One such data is time-series sensor readings collected from moving vehicles
containing fuel information. Due to the noisy dynamics and mobile environment,
the sensor readings can be very noisy. Denoising such a dataset is a
prerequisite for any useful application and security issues. Security is a
primitive concern in present vehicular schemes. The server side for retrieving
the fuel information can be easily hacked. Providing the accurate and noise
free fuel information via vehicular networks become crutial. Therefore, it has
led us to develop a system that can remove noise and keep the original value.
The system is also helpful for vehicle industry, fuel station, and power-plant
station that require fuel. In this work, we have only considered the value of
fuel level, and we have come up with a unique solution to filter out the noise
of high magnitudes using several algorithms such as interpolation,
extrapolation, spectral clustering, agglomerative clustering, wavelet analysis,
and median filtering. We have also employed peak detection and peak validation
algorithms to detect fuel refill and consumption in charge-discharge cycles. We
have used the R-squared metric to evaluate our model, and it is 98 percent In
most cases, the difference between detected value and real value remains within
the range of 1L.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization via Nuclear Norm Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generalize to unseen domains is crucial for machine learning
systems deployed in the real world, especially when we only have data from
limited training domains. In this paper, we propose a simple and effective
regularization method based on the nuclear norm of the learned features for
domain generalization. Intuitively, the proposed regularizer mitigates the
impacts of environmental features and encourages learning domain-invariant
features. Theoretically, we provide insights into why nuclear norm
regularization is more effective compared to ERM and alternative regularization
methods. Empirically, we conduct extensive experiments on both synthetic and
real datasets. We show that nuclear norm regularization achieves strong
performance compared to baselines in a wide range of domain generalization
tasks. Moreover, our regularizer is broadly applicable with various methods
such as ERM and SWAD with consistently improved performance, e.g., 1.7% and
0.9% test accuracy improvements respectively on the DomainBed benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Vulnerability Detection in Source Code Using Quantum Natural
  Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mst Shapna Akter, Hossain Shahriar, Zakirul Alam Bhuiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most important challenges in the field of software code audit is
the presence of vulnerabilities in software source code. These flaws are highly
likely ex-ploited and lead to system compromise, data leakage, or denial of
ser-vice. C and C++ open source code are now available in order to create a
large-scale, classical machine-learning and quantum machine-learning system for
function-level vulnerability identification. We assembled a siz-able dataset of
millions of open-source functions that point to poten-tial exploits. We created
an efficient and scalable vulnerability detection method based on a deep neural
network model Long Short Term Memory (LSTM), and quantum machine learning model
Long Short Term Memory (QLSTM), that can learn features extracted from the
source codes. The source code is first converted into a minimal intermediate
representation to remove the pointless components and shorten the de-pendency.
Therefore, We keep the semantic and syntactic information using state of the
art word embedding algorithms such as Glove and fastText. The embedded vectors
are subsequently fed into the classical and quantum convolutional neural
networks to classify the possible vulnerabilities. To measure the performance,
we used evaluation metrics such as F1 score, precision, re-call, accuracy, and
total execution time. We made a comparison between the results derived from the
classical LSTM and quantum LSTM using basic feature representation as well as
semantic and syntactic represen-tation. We found that the QLSTM with semantic
and syntactic features detects significantly accurate vulnerability and runs
faster than its classical counterpart.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Group Recommendation Based on a Probabilistic Semantic
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Dueñas-Lerín, Raúl Lara-Cabrera, Fernando Ortega, Jesús Bobadilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation to groups of users is a challenging subfield of recommendation
systems. Its key concept is how and where to make the aggregation of each set
of user information into an individual entity, such as a ranked recommendation
list, a virtual user, or a multi-hot input vector encoding. This paper proposes
an innovative strategy where aggregation is made in the multi-hot vector that
feeds the neural network model. The aggregation provides a probabilistic
semantic, and the resulting input vectors feed a model that is able to
conveniently generalize the group recommendation from the individual
predictions. Furthermore, using the proposed architecture, group
recommendations can be obtained by simply feedforwarding the pre-trained model
with individual ratings; that is, without the need to obtain datasets
containing group of user information, and without the need of running two
separate trainings (individual and group). This approach also avoids
maintaining two different models to support both individual and group learning.
Experiments have tested the proposed architecture using three representative
collaborative filtering datasets and a series of baselines; results show
suitable accuracy improvements compared to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Single Items: Exploring User Preferences in Item Sets with the
  Conversational Playlist Curation Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users in consumption domains, like music, are often able to more efficiently
provide preferences over a set of items (e.g. a playlist or radio) than over
single items (e.g. songs). Unfortunately, this is an underexplored area of
research, with most existing recommendation systems limited to understanding
preferences over single items. Curating an item set exponentiates the search
space that recommender systems must consider (all subsets of items!): this
motivates conversational approaches-where users explicitly state or refine
their preferences and systems elicit preferences in natural language-as an
efficient way to understand user needs. We call this task conversational item
set curation and present a novel data collection methodology that efficiently
collects realistic preferences about item sets in a conversational setting by
observing both item-level and set-level feedback. We apply this methodology to
music recommendation to build the Conversational Playlist Curation Dataset
(CPCD), where we show that it leads raters to express preferences that would
not be otherwise expressed. Finally, we propose a wide range of conversational
retrieval models as baselines for this task and evaluate them on the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommending on graphs: a comprehensive <span class="highlight-title">review</span> from a data perspective <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lemei Zhang, Peng Liu, Jon Atle Gulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in graph-based learning approaches have demonstrated their
effectiveness in modelling users' preferences and items' characteristics for
Recommender Systems (RSS). Most of the data in RSS can be organized into graphs
where various objects (e.g., users, items, and attributes) are explicitly or
implicitly connected and influence each other via various relations. Such a
graph-based organization brings benefits to exploiting potential properties in
graph learning (e.g., random walk and network embedding) techniques to enrich
the representations of the user and item nodes, which is an essential factor
for successful recommendations. In this paper, we provide a comprehensive
survey of Graph Learning-based Recommender Systems (GLRSs). Specifically, we
start from a data-driven perspective to systematically categorize various
graphs in GLRSs and analyze their characteristics. Then, we discuss the
state-of-the-art frameworks with a focus on the graph learning module and how
they address practical recommendation challenges such as scalability, fairness,
diversity, explainability and so on. Finally, we share some potential research
directions in this rapidly growing area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by UMUAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NASTyLinker: NIL-Aware Scalable <span class="highlight-title">Transformer</span>-based Entity Linker <span class="chip">ESWC'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Heist, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is the task of detecting mentions of entities in text and
disambiguating them to a reference knowledge base. Most prevalent EL approaches
assume that the reference knowledge base is complete. In practice, however, it
is necessary to deal with the case of linking to an entity that is not
contained in the knowledge base (NIL entity). Recent works have shown that,
instead of focusing only on affinities between mentions and entities,
considering inter-mention affinities can be used to represent NIL entities by
producing clusters of mentions. At the same time, inter-mention affinities can
help to substantially improve linking performance for known entities. With
NASTyLinker, we introduce an EL approach that is aware of NIL entities and
produces corresponding mention clusters while maintaining high linking
performance for known entities. The approach clusters mentions and entities
based on dense representations from Transformers and resolves conflicts (if
more than one entity is assigned to a cluster) by computing transitive
mention-entity affinities. We show the effectiveness and scalability of
NASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL
with respect to NIL entities. Further, we apply the presented approach to an
actual EL task, namely to knowledge graph population by linking entities in
Wikipedia listings, and provide an analysis of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of a paper in the research track of the 20th Extended
  Semantic Web Conference (ESWC'23)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiSSNet: Sound Event Detection and Speaker Identification via
  Hierarchical Prototypical Networks for Low-Resource Headphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        N Shashaank, Berker Banar, Mohammad Rasool Izadi, Jeremy Kemmerer, Shuo Zhang,  Chuan-Che,  Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern noise-cancelling headphones have significantly improved users'
auditory experiences by removing unwanted background noise, but they can also
block out sounds that matter to users. Machine learning (ML) models for sound
event detection (SED) and speaker identification (SID) can enable headphones to
selectively pass through important sounds; however, implementing these models
for a user-centric experience presents several unique challenges. First, most
people spend limited time customizing their headphones, so the sound detection
should work reasonably well out of the box. Second, the models should be able
to learn over time the specific sounds that are important to users based on
their implicit and explicit interactions. Finally, such models should have a
small memory footprint to run on low-power headphones with limited on-chip
memory. In this paper, we propose addressing these challenges using HiSSNet
(Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model that
uses a hierarchical prototypical network to detect both general and specific
sounds of interest and characterize both alarm-like and speech sounds. We show
that HiSSNet outperforms an SEID model trained using non-hierarchical
prototypical networks by 6.9 - 8.6 percent. When compared to state-of-the-art
(SOTA) models trained specifically for SED or SID alone, HiSSNet achieves
similar or better performance while reducing the memory footprint required to
support multiple capabilities on-device.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fractional dynamics foster deep learning of COPD stage prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenzhong Yin, Mihai Udrescu, Gaurav Gupta, Mingxi Cheng, Andrei Lihu, Lucretia Udrescu, Paul Bogdan, David M Mannino, Stefan Mihaicuta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic obstructive pulmonary disease (COPD) is one of the leading causes of
death worldwide. Current COPD diagnosis (i.e., spirometry) could be unreliable
because the test depends on an adequate effort from the tester and testee.
Moreover, the early diagnosis of COPD is challenging. We address COPD detection
by constructing two novel physiological signals datasets (4432 records from 54
patients in the WestRo COPD dataset and 13824 medical records from 534 patients
in the WestRo Porti COPD dataset). The authors demonstrate their complex
coupled fractal dynamical characteristics and perform a fractional-order
dynamics deep learning analysis to diagnose COPD. The authors found that the
fractional-order dynamical modeling can extract distinguishing signatures from
the physiological signals across patients with all COPD stages from stage 0
(healthy) to stage 4 (very severe). They use the fractional signatures to
develop and train a deep neural network that predicts COPD stages based on the
input features (such as thorax breathing effort, respiratory rate, or oxygen
saturation). The authors show that the fractional dynamic deep learning model
(FDDLM) achieves a COPD prediction accuracy of 98.66% and can serve as a robust
alternative to spirometry. The FDDLM also has high accuracy when validated on a
dataset with different physiological signals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on Advanced Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Planning using Reinforcement Learning: A Policy Iteration Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saumil Shivdikar, Jagannath Nirmal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the impact of real-time processing being realized in the recent past,
the need for efficient implementations of reinforcement learning algorithms has
been on the rise. Albeit the numerous advantages of Bellman equations utilized
in RL algorithms, they are not without the large search space of design
parameters.
  This research aims to shed light on the design space exploration associated
with reinforcement learning parameters, specifically that of Policy Iteration.
Given the large computational expenses of fine-tuning the parameters of
reinforcement learning algorithms, we propose an auto-tuner-based ordinal
regression approach to accelerate the process of exploring these parameters
and, in return, accelerate convergence towards an optimal policy. Our approach
provides 1.82x peak speedup with an average of 1.48x speedup over the previous
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization via Nuclear Norm Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generalize to unseen domains is crucial for machine learning
systems deployed in the real world, especially when we only have data from
limited training domains. In this paper, we propose a simple and effective
regularization method based on the nuclear norm of the learned features for
domain generalization. Intuitively, the proposed regularizer mitigates the
impacts of environmental features and encourages learning domain-invariant
features. Theoretically, we provide insights into why nuclear norm
regularization is more effective compared to ERM and alternative regularization
methods. Empirically, we conduct extensive experiments on both synthetic and
real datasets. We show that nuclear norm regularization achieves strong
performance compared to baselines in a wide range of domain generalization
tasks. Moreover, our regularizer is broadly applicable with various methods
such as ERM and SWAD with consistently improved performance, e.g., 1.7% and
0.9% test accuracy improvements respectively on the DomainBed benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Vulnerability Detection in Source Code Using Quantum Natural
  Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mst Shapna Akter, Hossain Shahriar, Zakirul Alam Bhuiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most important challenges in the field of software code audit is
the presence of vulnerabilities in software source code. These flaws are highly
likely ex-ploited and lead to system compromise, data leakage, or denial of
ser-vice. C and C++ open source code are now available in order to create a
large-scale, classical machine-learning and quantum machine-learning system for
function-level vulnerability identification. We assembled a siz-able dataset of
millions of open-source functions that point to poten-tial exploits. We created
an efficient and scalable vulnerability detection method based on a deep neural
network model Long Short Term Memory (LSTM), and quantum machine learning model
Long Short Term Memory (QLSTM), that can learn features extracted from the
source codes. The source code is first converted into a minimal intermediate
representation to remove the pointless components and shorten the de-pendency.
Therefore, We keep the semantic and syntactic information using state of the
art word embedding algorithms such as Glove and fastText. The embedded vectors
are subsequently fed into the classical and quantum convolutional neural
networks to classify the possible vulnerabilities. To measure the performance,
we used evaluation metrics such as F1 score, precision, re-call, accuracy, and
total execution time. We made a comparison between the results derived from the
classical LSTM and quantum LSTM using basic feature representation as well as
semantic and syntactic represen-tation. We found that the QLSTM with semantic
and syntactic features detects significantly accurate vulnerability and runs
faster than its classical counterpart.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Visual Language Maps for Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While interacting in the world is a multi-sensory experience, many robots
continue to predominantly rely on visual perception to map and navigate in
their environments. In this work, we propose Audio-Visual-Language Maps
(AVLMaps), a unified 3D spatial map representation for storing cross-modal
information from audio, visual, and language cues. AVLMaps integrate the
open-vocabulary capabilities of multimodal foundation models pre-trained on
Internet-scale data by fusing their features into a centralized 3D voxel grid.
In the context of navigation, we show that AVLMaps enable robot systems to
index goals in the map based on multimodal queries, e.g., textual descriptions,
images, or audio snippets of landmarks. In particular, the addition of audio
information enables robots to more reliably disambiguate goal locations.
Extensive experiments in simulation show that AVLMaps enable zero-shot
multimodal goal navigation from multimodal prompts and provide 50% better
recall in ambiguous scenarios. These capabilities extend to mobile robots in
the real world - navigating to landmarks referring to visual, audio, and
spatial concepts. Videos and code are available at: https://avlmaps.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://avlmaps.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architext: Language-Driven Generative Architecture Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Galanos, Antonios Liapis, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architectural design is a highly complex practice that involves a wide
diversity of disciplines, technologies, proprietary design software, expertise,
and an almost infinite number of constraints, across a vast array of design
tasks. Enabling intuitive, accessible, and scalable design processes is an
important step towards performance-driven and sustainable design for all. To
that end, we introduce Architext, a novel semantic generation assistive tool.
Architext enables design generation with only natural language prompts, given
to large-scale Language Models, as input. We conduct a thorough quantitative
evaluation of Architext's downstream task performance, focusing on semantic
accuracy and diversity for a number of pre-trained language models ranging from
120 million to 6 billion parameters. Architext models are able to learn the
specific design task, generating valid residential layouts at a near 100\%
rate. Accuracy shows great improvement when scaling the models, with the
largest model (GPT-J) yielding impressive accuracy ranging between 25% to over
80% for different prompt categories. We open source the finetuned Architext
models and our synthetic dataset, hoping to inspire experimentation in this
exciting area of design research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonparametric Multi-shape Modeling with Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09127v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09127v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengrui Luo, Justin D. Strait
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling and uncertainty quantification of closed curves is an important
problem in the field of shape analysis, and can have significant ramifications
for subsequent statistical tasks. Many of these tasks involve collections of
closed curves, which often exhibit structural similarities at multiple levels.
Modeling multiple closed curves in a way that efficiently incorporates such
between-curve dependence remains a challenging problem. In this work, we
propose and investigate a multiple-output (a.k.a. multi-output),
multi-dimensional Gaussian process modeling framework. We illustrate the
proposed methodological advances, and demonstrate the utility of meaningful
uncertainty quantification, on several curve and shape-related tasks. This
model-based approach not only addresses the problem of inference on closed
curves (and their shapes) with kernel constructions, but also opens doors to
nonparametric modeling of multi-level dependence for functional objects in
general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Characteristics and Performance of Augmented Reality
  Applications on Head-Mounted Displays: A Study of the Hololens Application
  Store 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pubudu Wijesooriya, Sheikh Muhammad Farjad, Nikolaos Stergiou, Spyridon Mastorakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained
significant traction over the recent years. Nevertheless, it remains unclear
what AR HMD-based applications have been developed over the years and what
their system performance is when they are run on HMDs. In this paper, we aim to
shed light into this direction. Our study focuses on the applications available
on the Microsoft Hololens application store given the wide use of the Hololens
headset. Our study has two major parts: (i) we collect metadata about the
applications available on the Microsoft Hololens application store to
understand their characteristics (e.g., categories, pricing, permissions
requested, hardware and software compatibility); and (ii) we interact with
these applications while running on a Hololens 2 headset and collect data about
systems-related metrics (e.g., memory and storage usage, time spent on CPU and
GPU related operations) to investigate the systems performance of applications.
Our study has resulted in several interesting findings, which we share with the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication by IEEE ICC workshops
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriDet: Temporal Action Detection with Relative Boundary Modeling <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a one-stage framework TriDet for temporal action
detection. Existing methods often suffer from imprecise boundary predictions
due to the ambiguous action boundaries in videos. To alleviate this problem, we
propose a novel Trident-head to model the action boundary via an estimated
relative probability distribution around the boundary. In the feature pyramid
of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer
to mitigate the rank loss problem of self-attention that takes place in the
video features and aggregate information across different temporal
granularities. Benefiting from the Trident-head and the SGP-based feature
pyramid, TriDet achieves state-of-the-art performance on three challenging
benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational
costs, compared to previous methods. For example, TriDet hits an average mAP of
$69.3\%$ on THUMOS14, outperforming the previous best by $2.5\%$, but with only
$74.6\%$ of its latency. The code is released to
https://github.com/sssste/TriDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2023; Temporal Action Detection; Temporal Action Localization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMC-CLIP: Contrastive Language-Image <span class="highlight-title">Pre-train</span>ing using Biomedical
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on large-scale dataset gain a recent surge in CV
and NLP. In contrast, development in biomedical domain lags far behind due to
data scarcity. To address this issue, we build and release PMC-OA, a biomedical
dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess
subset, which is 8 times larger than before. PMC-OA covers diverse modalities
or diseases, with majority of the image-caption samples aligned at
finer-grained level, i.e., subfigure and subcaption. While pretraining a
CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art
results on various downstream tasks, including image-text retrieval on ROCO,
MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text
retrieval, +3.9% accuracy on image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Distortion Invariant Representation for Image Restoration from
  A Causality Perspective <span class="chip">CVPR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Bingchen Li, Xin Jin, Cuiling Lan, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have witnessed the great advancement of Deep neural
networks (DNNs) in image restoration. However, a critical limitation is that
they cannot generalize well to real-world degradations with different degrees
or types. In this paper, we are the first to propose a novel training strategy
for image restoration from the causality perspective, to improve the
generalization ability of DNNs for unknown degradations. Our method, termed
Distortion Invariant representation Learning (DIL), treats each distortion type
and degree as one specific confounder, and learns the distortion-invariant
representation by eliminating the harmful confounding effect of each
degradation. We derive our DIL with the back-door criterion in causality by
modeling the interventions of different distortions from the optimization
perspective. Particularly, we introduce counterfactual distortion augmentation
to simulate the virtual distortion types and degrees as the confounders. Then,
we instantiate the intervention of each distortion with a virtual model
updating based on corresponding distorted images, and eliminate them from the
meta-learning perspective. Extensive experiments demonstrate the effectiveness
of our DIL on the generalization capability for unseen distortion types and
degrees. Our code will be available at
https://github.com/lixinustc/Casual-IRDIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sentence Grounding in Videos: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV), \aka natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate the methods in different categories with their
strengths and weaknesses. Lastly, we discuss issues with the current TSGV
research and share our insights about promising research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Making a Trojan-horse Attack on Text-to-Image Retrieval <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03861v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03861v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Aozhu Chen, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning based image retrieval is reported to be vulnerable to
adversarial attacks, existing works are mainly on image-to-image retrieval with
their attacks performed at the front end via query modification. By contrast,
we present in this paper the first study about a threat that occurs at the back
end of a text-to-image retrieval (T2IR) system. Our study is motivated by the
fact that the image collection indexed by the system will be regularly updated
due to the arrival of new images from various sources such as web crawlers and
advertisers. With malicious images indexed, it is possible for an attacker to
indirectly interfere with the retrieval process, letting users see certain
images that are completely irrelevant w.r.t. their queries. We put this thought
into practice by proposing a novel Trojan-horse attack (THA). In particular, we
construct a set of Trojan-horse images by first embedding word-specific
adversarial information into a QR code and then putting the code on benign
advertising images. A proof-of-concept evaluation, conducted on two popular
T2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed
THA in a white-box mode.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETMA: Efficient <span class="highlight-title">Transformer</span> Based Multilevel Attention framework for
  Multimodal Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashima Yadav, Shivani Gaba, Haneef Khan, Ishan Budhiraja, Akansha Singh, Krishan Kant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this new digital era, social media has created a severe impact on the
lives of people. In recent times, fake news content on social media has become
one of the major challenging problems for society. The dissemination of
fabricated and false news articles includes multimodal data in the form of text
and images. The previous methods have mainly focused on unimodal analysis.
Moreover, for multimodal analysis, researchers fail to keep the unique
characteristics corresponding to each modality. This paper aims to overcome
these limitations by proposing an Efficient Transformer based Multilevel
Attention (ETMA) framework for multimodal fake news detection, which comprises
the following components: visual attention-based encoder, textual
attention-based encoder, and joint attention-based learning. Each component
utilizes the different forms of attention mechanism and uniquely deals with
multimodal data to detect fraudulent content. The efficacy of the proposed
network is validated by conducting several experiments on four real-world fake
news datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,
and Risdal Fake News Dataset using multiple evaluation metrics. The results
show that the proposed method outperforms the baseline methods on all four
datasets. Further, the computation time of the model is also lower than the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Computational Social
  Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-12T00:00:00Z">2023-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUKE-Graph: A <span class="highlight-title">Transformer</span>-based Approach with Gated Relational Graph
  Attention for Cloze-style Reading Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Foolad, Kourosh Kiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating prior knowledge can improve existing pre-training models in
cloze-style machine reading and has become a new trend in recent studies.
Notably, most of the existing models have integrated external knowledge graphs
(KG) and transformer-based models, such as BERT into a unified data structure.
However, selecting the most relevant ambiguous entities in KG and extracting
the best subgraph remains a challenge. In this paper, we propose the
LUKE-Graph, a model that builds a heterogeneous graph based on the intuitive
relationships between entities in a document without using any external KG. We
then use a Relational Graph Attention (RGAT) network to fuse the graph's
reasoning information and the contextual representation encoded by the
pre-trained LUKE model. In this way, we can take advantage of LUKE, to derive
an entity-aware representation; and a graph model - to exploit relation-aware
representation. Moreover, we propose Gated-RGAT by augmenting RGAT with a
gating mechanism that regulates the question information for the graph
convolution operation. This is very similar to human reasoning processing
because they always choose the best entity candidate based on the question
information. Experimental results demonstrate that the LUKE-Graph achieves
state-of-the-art performance on the ReCoRD dataset with commonsense reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for neurocomputing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive
  Machine Translation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengrui Ma, Chenze Shao, Shangtong Gui, Min Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive translation (NAT) reduces the decoding latency but suffers
from performance degradation due to the multi-modality problem. Recently, the
structure of directed acyclic graph has achieved great success in NAT, which
tackles the multi-modality problem by introducing dependency between vertices.
However, training it with negative log-likelihood loss implicitly requires a
strict alignment between reference tokens and vertices, weakening its ability
to handle multiple translation modalities. In this paper, we hold the view that
all paths in the graph are fuzzily aligned with the reference sentence. We do
not require the exact alignment but train the model to maximize a fuzzy
alignment score between the graph and reference, which takes captured
translations in all modalities into account. Extensive experiments on major WMT
benchmarks show that our method substantially improves translation performance
and increases prediction confidence, setting a new state of the art for NAT on
the raw training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MWE as WSD: Solving Multiword Expression Identification with Word Sense
  Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Tanner, Jacob Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in word sense disambiguation (WSD) utilizes encodings of the
sense gloss (definition text), in addition to the input words and context, to
improve performance. In this work we demonstrate that this approach can be
adapted for use in multiword expression (MWE) identification by training a
Bi-encoder model which uses gloss and context information to filter MWE
candidates produced from a simple rule-based extraction pipeline. We achieve
state-of-the-art results in MWE identification on the DiMSUM dataset, and
competitive results on the PARSEME 1.1 English dataset using this method. Our
model also retains most of its ability to perform WSD, demonstrating that a
single model can successfully be applied to both of these tasks. Additionally,
we experiment with applying Poly-encoder models to MWE identification and WSD,
introducing a modified Poly-encoder architecture which outperforms the standard
Poly-encoder on these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Intent Classification accuracy in Noisy Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Nabih Ali, Alessio Brutti, Daniele Falavigna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent classification is a fundamental task in the spoken language
understanding field that has recently gained the attention of the scientific
community, mainly because of the feasibility of approaching it with end-to-end
neural models. In this way, avoiding using intermediate steps, i.e. automatic
speech recognition, is possible, thus the propagation of errors due to
background noise, spontaneous speech, speaking styles of users, etc. Towards
the development of solutions applicable in real scenarios, it is interesting to
investigate how environmental noise and related noise reduction techniques to
address the intent classification task with end-to-end neural models. In this
paper, we experiment with a noisy version of the fluent speech command data
set, combining the intent classifier with a time-domain speech enhancement
solution based on Wave-U-Net and considering different training strategies.
Experimental results reveal that, for this task, the use of speech enhancement
greatly improves the classification accuracy in noisy conditions, in particular
when the classification model is trained on enhanced signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards General Purpose Medical AI: Continual Learning Medical
  Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, Kang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inevitable domain and task discrepancies in real-world scenarios can impair
the generalization performance of the pre-trained deep models for medical data.
Therefore, we audaciously propose that we should build a general-purpose
medical AI system that can be seamlessly adapted to downstream domains/tasks.
Since the domain/task adaption procedures usually involve additional labeling
work for the target data, designing a data-efficient adaption algorithm is
desired to save the cost of transferring the learned knowledge. Our recent work
found that vision-language models (VLMs) are efficient learners with
extraordinary cross-domain ability. Therefore, in this work, we further explore
the possibility of leveraging pre-trained VLMs as medical foundation models for
building general-purpose medical AI, where we thoroughly investigate three
machine-learning paradigms, i.e., domain/task-specialized learning, joint
learning, and continual learning, for training the VLMs and evaluate their
generalization performance on cross-domain and cross-task test sets. To
alleviate the catastrophic forgetting during sequential training, we employ
rehearsal learning and receive a sharp boost in terms of generalization
capability. In a nutshell, our empirical evidence suggests that continual
learning may be a practical and efficient learning paradigm for the medical
foundation model. And we hope researchers can use our empirical evidence as
basement to further explore the path toward medical foundation model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span> Models for Non-autoregressive Text Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing improved generation quality. In this survey, we
review the recent progress in diffusion models for NAR text generation. As the
background, we first present the general definition of diffusion models and the
text diffusion models, and then discuss their merits for NAR generation. As the
core content, we further introduce two mainstream diffusion models in existing
text diffusion works, and review the key designs of the diffusion process.
Moreover, we discuss the utilization of pre-trained language models (PLMs) for
text diffusion models and introduce optimization techniques for text data.
Finally, we discuss several promising directions and conclude this paper. Our
survey aims to provide researchers with a systematic reference of related
research on text diffusion models for NAR generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Heterogeneous Graph for Abstractive Multi-Document
  Summarization <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jianzhong Qi, Jey Han Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Articulation <span class="highlight-title">GAN</span>: Unsupervised modeling of articulatory learning <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gašper Beguš, Alan Zhou, Peter Wu, Gopala K Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative deep neural networks are widely used for speech synthesis, but
most existing models directly generate waveforms or spectral outputs. Humans,
however, produce speech by controlling articulators, which results in the
production of speech sounds through physical properties of sound propagation.
We introduce the Articulatory Generator to the Generative Adversarial Network
paradigm, a new unsupervised generative model of speech production/synthesis.
The Articulatory Generator more closely mimics human speech production by
learning to generate articulatory representations (electromagnetic
articulography or EMA) in a fully unsupervised manner. A separate pre-trained
physical model (ema2wav) then transforms the generated EMA representations to
speech waveforms, which get sent to the Discriminator for evaluation.
Articulatory analysis suggests that the network learns to control articulators
in a similar manner to humans during speech production. Acoustic analysis of
the outputs suggests that the network learns to generate words that are both
present and absent in the training distribution. We additionally discuss
implications of articulatory representations for cognitive models of human
language and speech technology in general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Language Models in Biomedical Domain: A Systematic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.05006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.05006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) have been the de facto paradigm for most
natural language processing (NLP) tasks. This also benefits biomedical domain:
researchers from informatics, medicine, and computer science (CS) communities
propose various PLMs trained on biomedical datasets, e.g., biomedical text,
electronic health records, protein, and DNA sequences for various biomedical
tasks. However, the cross-discipline characteristics of biomedical PLMs hinder
their spreading among communities; some existing works are isolated from each
other without comprehensive comparison and discussions. It expects a survey
that not only systematically reviews recent advances of biomedical PLMs and
their applications but also standardizes terminology and benchmarks. In this
paper, we summarize the recent progress of pre-trained language models in the
biomedical domain and their applications in biomedical downstream tasks.
Particularly, we discuss the motivations and propose a taxonomy of existing
biomedical PLMs. Their applications in biomedical downstream tasks are
exhaustively discussed. At last, we illustrate various limitations and future
trends, which we hope can provide inspiration for the future research of the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An improved version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Correct answers" from the psychology of artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter S. Park, Philipp Schoenegger, Chongyang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have vastly grown in capabilities. One proposed
application of such AI systems is to support data collection in the social and
cognitive sciences, where perfect experimental control is currently unfeasible
and the collection of large, representative datasets is generally expensive. In
this paper, we re-replicate 14 studies from the Many Labs 2 replication project
with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We
collected responses from the default setting of GPT3.5 by inputting each
study's survey as text. Among the eight studies we could analyse, our GPT
sample replicated 37.5% of the original results as well as 37.5% of the Many
Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as
we had planned in our pre-registration. This was because for each of these six
studies, GPT3.5 answered at least one of the survey questions (either a
dependent variable or a condition variable) in an extremely predetermined way:
an unexpected phenomenon we call the "correct answer" effect. Different runs of
GPT3.5 answered nuanced questions probing political orientation, economic
preference, judgement, and moral philosophy with zero or near-zero variation in
responses: with the supposedly "correct answer." For example, our survey
questions found the default setting of GPT3.5 to almost always self-identify as
a maximally strong conservative (99.6%, N=1,030), and to always be morally
deontological in opposing the hypothetical pushing of a large man in front of
an incoming trolley to save the lives of five people (100%, N=1,030). Since AI
models of the future may be trained on much of the same data as GPT3.5,
training data from which GPT3.5 may have learned its supposedly "correct
answers," our results raise concerns that a hypothetical AI-led future may in
certain ways be subject to a diminished diversity of thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages (31-page main text, 21-page SI); nine visualizations (three
  tables and two figures in the main text, four figures in the SI); added
  corrections regarding the previously erroneous survey for Study 4's
  replication of Graham et al. (2009); preregistered OSF database is available
  at https://osf.io/dzp8t/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with
  Variational Information Bottleneck and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingshan Chang, Min Yang, Qingshan Jiang, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have dominated the literature on aspect-based
sentiment analysis (ABSA), yielding state-of-the-art results. However, these
deep models generally suffer from spurious correlation problems between input
features and output labels, which creates significant barriers to robustness
and generalization capability. In this paper, we propose a novel Contrastive
Variational Information Bottleneck framework (called CVIB) to reduce spurious
correlations for ABSA. The proposed CVIB framework is composed of an original
network and a self-pruned network, and these two networks are optimized
simultaneously via contrastive learning. Concretely, we employ the Variational
Information Bottleneck (VIB) principle to learn an informative and compressed
network (self-pruned network) from the original network, which discards the
superfluous patterns or spurious correlations between input features and
prediction labels. Then, self-pruning contrastive learning is devised to pull
together semantically similar positive pairs and push away dissimilar pairs,
where the representations of the anchor learned by the original and self-pruned
networks respectively are regarded as a positive pair while the representations
of two different sentences within a mini-batch are treated as a negative pair.
To verify the effectiveness of our CVIB method, we conduct extensive
experiments on five benchmark ABSA datasets and the experimental results show
that our approach achieves better performance than the strong competitors in
terms of overall prediction performance, robustness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning ASR pathways: A sparse multilingual ASR model <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Yang, Andros Tjandra, Chunxi Liu, David Zhang, Duc Le, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning compresses automatic speech recognition (ASR) models
effectively. However, in multilingual ASR, language-agnostic pruning may lead
to severe performance drops on some languages because language-agnostic pruning
masks may not fit all languages and discard important language-specific
parameters. In this work, we present ASR pathways, a sparse multilingual ASR
model that activates language-specific sub-networks ("pathways"), such that the
parameters for each language are learned explicitly. With the overlapping
sub-networks, the shared parameters can also enable knowledge transfer for
lower-resource languages via joint multilingual training. We propose a novel
algorithm to learn ASR pathways, and evaluate the proposed method on 4
languages with a streaming RNN-T model. Our proposed ASR pathways outperform
both dense models and a language-agnostically pruned model, and provide better
performance on low-resource languages compared to the monolingual sparse
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Emotion Embeddings to Transfer Knowledge Between Emotions,
  Languages, and Annotation Formats <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Gireesh Mahajan, Sabyasachee Baruah, Keith Burghardt, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for emotional inference from text continues to diversify as more and
more disciplines integrate emotions into their theories and applications. These
needs include inferring different emotion types, handling multiple languages,
and different annotation formats. A shared model between different
configurations would enable the sharing of knowledge and a decrease in training
costs, and would simplify the process of deploying emotion recognition models
in novel environments. In this work, we study how we can build a single model
that can transition between these different configurations by leveraging
multilingual models and Demux, a transformer-based model whose input includes
the emotions of interest, enabling us to dynamically change the emotions
predicted by the model. Demux also produces emotion embeddings, and performing
operations on them allows us to transition to clusters of emotions by pooling
the embeddings of each cluster. We show that Demux can simultaneously transfer
knowledge in a zero-shot manner to a new language, to a novel annotation format
and to unseen emotions. Code is available at
https://github.com/gchochla/Demux-MEmo .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP'23, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Label Correlations in a Multi-label Setting: A Case Study in
  Emotion <span class="chip">ICASSP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Gireesh Mahajan, Sabyasachee Baruah, Keith Burghardt, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting emotions expressed in text has become critical to a range of
fields. In this work, we investigate ways to exploit label correlations in
multi-label emotion recognition models to improve emotion detection. First, we
develop two modeling approaches to the problem in order to capture word
associations of the emotion words themselves, by either including the emotions
in the input, or by leveraging Masked Language Modeling (MLM). Second, we
integrate pairwise constraints of emotion representations as regularization
terms alongside the classification loss of the models. We split these terms
into two categories, local and global. The former dynamically change based on
the gold labels, while the latter remain static during training. We demonstrate
state-of-the-art performance across Spanish, English, and Arabic in SemEval
2018 Task 1 E-c using monolingual BERT-based models. On top of better
performance, we also demonstrate improved robustness. Code is available at
https://github.com/gchochla/Demux-MEmo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP'23, 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-MMF: Provider Max-min Fairness Re-ranking in Recommender System <span class="chip">WWW23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, Zhenghua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the issue of recommending fairly from the aspect of
providers, which has become increasingly essential in multistakeholder
recommender systems. Existing studies on provider fairness usually focused on
designing proportion fairness (PF) metrics that first consider systematic
fairness. However, sociological researches show that to make the market more
stable, max-min fairness (MMF) is a better metric. The main reason is that MMF
aims to improve the utility of the worst ones preferentially, guiding the
system to support the providers in weak market positions. When applying MMF to
recommender systems, how to balance user preferences and provider fairness in
an online recommendation scenario is still a challenging problem. In this
paper, we proposed an online re-ranking model named Provider Max-min Fairness
Re-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates
provider fair recommendation as a resource allocation problem, where the
exposure slots are considered the resources to be allocated to providers and
the max-min fairness is used as the regularizer during the process. We show
that the problem can be further represented as a regularized online optimizing
problem and solved efficiently in its dual space. During the online re-ranking
phase, a momentum gradient descent method is designed to conduct the dynamic
re-ranking. Theoretical analysis showed that the regret of P-MMF can be
bounded. Experimental results on four public recommender datasets demonstrated
that P-MMF can outperformed the state-of-the-art baselines. Experimental
results also show that P-MMF can retain small computationally costs on a corpus
with the large number of items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WWW23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoDenoise: Automatic Data Instance Denoising for Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, Wanyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historical user-item interaction datasets are essential in training modern
recommender systems for predicting user preferences. However, the arbitrary
user behaviors in most recommendation scenarios lead to a large volume of noisy
data instances being recorded, which cannot fully represent their true
interests. While a large number of denoising studies are emerging in the
recommender system community, all of them suffer from highly dynamic data
distributions. In this paper, we propose a Deep Reinforcement Learning (DRL)
based framework, AutoDenoise, with an Instance Denoising Policy Network, for
denoising data instances with an instance selection manner in deep recommender
systems. To be specific, AutoDenoise serves as an agent in DRL to adaptively
select noise-free and predictive data instances, which can then be utilized
directly in training representative recommendation models. In addition, we
design an alternate two-phase optimization strategy to train and validate the
AutoDenoise properly. In the searching phase, we aim to train the policy
network with the capacity of instance denoising; in the validation phase, we
find out and evaluate the denoised subset of data instances selected by the
trained policy network, so as to validate its denoising ability. We conduct
extensive experiments to validate the effectiveness of AutoDenoise combined
with multiple representative recommender system models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 5 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileRec: A Large-Scale Dataset for Mobile Apps Recommendation <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. H. Maqbool, Umar Farooq, Adib Mosharrof, A. B. Siddique, Hassan Foroosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in our digital lives, from
recommending products on e-commerce websites to suggesting movies and music on
streaming platforms. Existing recommendation datasets, such as Amazon Product
Reviews and MovieLens, greatly facilitated the research and development of
recommender systems in their respective domains. While the number of mobile
users and applications (aka apps) has increased exponentially over the past
decade, research in mobile app recommender systems has been significantly
constrained, primarily due to the lack of high-quality benchmark datasets, as
opposed to recommendations for products, movies, and news. To facilitate
research for app recommendation systems, we introduce a large-scale dataset,
called MobileRec. We constructed MobileRec from users' activity on the Google
play store. MobileRec contains 19.3 million user interactions (i.e., user
reviews on apps) with over 10K unique apps across 48 categories. MobileRec
records the sequential activity of a total of 0.7 million distinct users. Each
of these users has interacted with no fewer than five distinct apps, which
stands in contrast to previous datasets on mobile apps that recorded only a
single interaction per user. Furthermore, MobileRec presents users' ratings as
well as sentiments on installed apps, and each app contains rich metadata such
as app name, category, description, and overall rating, among others. We
demonstrate that MobileRec can serve as an excellent testbed for app
recommendation through a comparative study of several state-of-the-art
recommendation approaches. The quantitative results can act as a baseline for
other researchers to compare their results against. The MobileRec dataset is
available at https://huggingface.co/datasets/recmeapp/mobilerec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 tables, 4 figures, Under submission at SIGIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Prioritization of App Issues via Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moghis Fereidouni, Adib Mosharrof, Umar Farooq, AB Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile app stores produce a tremendous amount of data in the form of user
reviews, which is a huge source of user requirements and sentiments; such
reviews allow app developers to proactively address issues in their apps.
However, only a small number of reviews capture common issues and sentiments
which creates a need for automatically identifying prominent reviews.
Unfortunately, most existing work in text ranking and popularity prediction
focuses on social contexts where other signals are available, which renders
such works ineffective in the context of app reviews. In this work, we propose
a new framework, PPrior, that enables proactive prioritization of app issues
through identifying prominent reviews (ones predicted to receive a large number
of votes in a given time window). Predicting highly-voted reviews is
challenging given that, unlike social posts, social network features of users
are not available. Moreover, there is an issue of class imbalance, since a
large number of user reviews receive little to no votes. PPrior employs a
pre-trained T5 model and works in three phases. Phase one adapts the
pre-trained T5 model to the user reviews data in a self-supervised fashion. In
phase two, we leverage contrastive training to learn a generic and
task-independent representation of user reviews. Phase three uses radius
neighbors classifier t o m ake t he final predictions. This phase also uses
FAISS index for scalability and efficient search. To conduct extensive
experiments, we acquired a large dataset of over 2.1 million user reviews from
Google Play. Our experimental results demonstrate the effectiveness of the
proposed framework when compared against several state-of-the-art approaches.
Moreover, the accuracy of PPrior in predicting prominent reviews is comparable
to that of experienced app developers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2022 IEEE International Conference on Big Data (Big Data)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Know Your Contextual Search Intent: A <span class="highlight-title">Prompt</span>ing
  Framework for Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelong Mao, Zhicheng Dou, Haonan Chen, Fengran Mo, Hongjin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a prompting framework called LLMCS that leverages
large language models, such as code-davinci-002 of GPT-3, to perform few-shot
conversational query rewriting for conversational search. We explore three
prompting methods to generate multiple query rewrites and hypothetical
responses, and propose aggregating them into an integrated representation that
can robustly represent the user's real contextual search intent. Experimental
results on two conversational search datasets, including CAst-19 and CAsT-20,
show that our approach achieves significant improvements in search
effectiveness over existing baselines and manual rewrites. Notably, LLMCS can
significantly outperform the state-of-the-art baselines by up to +5.9\% and
+32.9\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential
of large language models for conversational search. Our code will be released
at https://github.com/kyriemao/LLMCS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention over Self-attention:Intention-aware Re-ranking with Dynamic
  <span class="highlight-title">Transformer</span> Encoders for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05333v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05333v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Sheng Zang, Rundong Wang, Zhu Sun, J. Senthilnath, Chi Xu, Chee-Keong Kwoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Re-ranking models refine item recommendation lists generated by the prior
global ranking model, which have demonstrated their effectiveness in improving
the recommendation quality. However, most existing re-ranking solutions only
learn from implicit feedback with a shared prediction model, which regrettably
ignore inter-item relationships under diverse user intentions. In this paper,
we propose a novel Intention-aware Re-ranking Model with Dynamic Transformer
Encoder (RAISE), aiming to perform user-specific prediction for each individual
user based on her intentions. Specifically, we first propose to mine latent
user intentions from text reviews with an intention discovering module (IDM).
By differentiating the importance of review information with a co-attention
network, the latent user intention can be explicitly modeled for each user-item
pair. We then introduce a dynamic transformer encoder (DTE) to capture
user-specific inter-item relationships among item candidates by seamlessly
accommodating the learned latent user intentions via IDM. As such, one can not
only achieve more personalized recommendations but also obtain corresponding
explanations by constructing RAISE upon existing recommendation engines.
Empirical study on four public datasets shows the superiority of our proposed
RAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by
Precision@5, MAP@5, and NDCG@5 respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMET: Convolutional Dimension Interaction for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14129v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14129v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Lin, Lei Feng, Xingzhi Guo, Yu Zhang, Rui Yin, Chee Keong Kwoh, Chi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning-based recommendation models play a dominant role
among recommendation techniques. However, most of the existing methods assume
both historical interactions and embedding dimensions are independent of each
other, and thus regrettably ignore the high-order interaction information among
historical interactions and embedding dimensions. In this paper, we propose a
novel representation learning-based model called COMET (COnvolutional diMEnsion
inTeraction), which simultaneously models the high-order interaction patterns
among historical interactions and embedding dimensions. To be specific, COMET
stacks the embeddings of historical interactions horizontally at first, which
results in two "embedding maps". In this way, internal interactions and
dimensional interactions can be exploited by convolutional neural networks
(CNN) with kernels of different sizes simultaneously. A fully-connected
multi-layer perceptron (MLP) is then applied to obtain two interaction vectors.
Lastly, the representations of users and items are enriched by the learnt
interaction vectors, which can further be used to produce the final prediction.
Extensive experiments and ablation studies on various public implicit feedback
datasets clearly demonstrate the effectiveness and rationality of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TIST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dataset for Learning Graph Representations to Predict Customer Returns
  in Fashion Retail <span class="chip">RecSys 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie McGowan, Elizabeth Guest, Ziyang Yan, Cong Zheng, Neha Patel, Mason Cusack, Charlie Donaldson, Sofie de Cnudde, Gabriel Facini, Fabon Dzogang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel dataset collected by ASOS (a major online fashion
retailer) to address the challenge of predicting customer returns in a fashion
retail ecosystem. With the release of this substantial dataset we hope to
motivate further collaboration between research communities and the fashion
industry. We first explore the structure of this dataset with a focus on the
application of Graph Representation Learning in order to exploit the natural
data structure and provide statistical insights into particular features within
the data. In addition to this, we show examples of a return prediction
classification task with a selection of baseline models (i.e. with no
intermediate representation learning step) and a graph representation based
model. We show that in a downstream return prediction classification task, an
F1-score of 0.792 can be found using a Graph Neural Network (GNN), improving
upon other models discussed in this work. Alongside this increased F1-score, we
also present a lower cross-entropy loss by recasting the data into a graph
structure, indicating more robust predictions from a GNN based solution. These
results provide evidence that GNNs could provide more impactful and usable
classifications than other baseline models on the presented dataset and with
this motivation, we hope to encourage further research into graph-based
approaches using the ASOS GraphReturns dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The ASOS GraphReturns dataset can be found at https://osf.io/c793h/.
  Accepted at FashionXRecSys 2022 workshop. Published Version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Graph Learning for Acoustic Event Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Shirian, Mona Ahmadian, Krishna Somandepalli, Tanaya Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous graphs provide a compact, efficient, and scalable way to model
data involving multiple disparate modalities. This makes modeling audiovisual
data using heterogeneous graphs an attractive option. However, graph structure
does not appear naturally in audiovisual data. Graphs for audiovisual data are
constructed manually which is both difficult and sub-optimal. In this work, we
address this problem by (i) proposing a parametric graph construction strategy
for the intra-modal edges, and (ii) learning the crossmodal edges. To this end,
we develop a new model, heterogeneous graph crossmodal network (HGCN) that
learns the crossmodal edges. Our proposed model can adapt to various spatial
and temporal scales owing to its parametric construction, while the learnable
crossmodal edges effectively connect the relevant nodes across modalities.
Experiments on a large benchmark dataset (AudioSet) show that our model is
state-of-the-art (0.53 mean average precision), outperforming transformer-based
models and other graph-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2207.07935</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision, Deduction and Alignment: An Empirical Study on Multi-modal
  Knowledge Graph Alignment <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-11T00:00:00Z">2023-03-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcription free filler word detection with Neural semi-CRFs <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhu, Yujia Yan, Juan-Pablo Caceres, Zhiyao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-linguistic filler words, such as "uh" or "um", are prevalent in
spontaneous speech and serve as indicators for expressing hesitation or
uncertainty. Previous works for detecting certain non-linguistic filler words
are highly dependent on transcriptions from a well-established commercial
automatic speech recognition (ASR) system. However, certain ASR systems are not
universally accessible from many aspects, e.g., budget, target languages, and
computational power. In this work, we investigate filler word detection system
that does not depend on ASR systems. We show that, by using the structured
state space sequence model (S4) and neural semi-Markov conditional random
fields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment
level) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a
qualitative analysis on the detected results to analyze the limitations of our
proposed system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and
  Multilingual Natural Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation (NLG) accepts input data in the form of images,
videos, or text and generates corresponding natural language text as output.
Existing NLG methods mainly adopt a supervised approach and rely heavily on
coupled data-to-text pairs. However, for many targeted scenarios and for
non-English languages, sufficient quantities of labeled data are often not
available. To relax the dependency on labeled data of downstream tasks, we
propose an intuitive and effective zero-shot learning framework, ZeroNLG, which
can deal with multiple NLG tasks, including image-to-text (image captioning),
video-to-text (video captioning), and text-to-text (neural machine
translation), across English, Chinese, German, and French within a unified
framework. ZeroNLG does not require any labeled downstream pairs for training.
During training, ZeroNLG (i) projects different domains (across modalities and
languages) to corresponding coordinates in a shared common latent space; (ii)
bridges different domains by aligning their corresponding coordinates in this
space; and (iii) builds an unsupervised multilingual auto-encoder to learn to
generate text by reconstructing the input text given its coordinate in shared
latent space. Consequently, during inference, based on the data-to-text
pipeline, ZeroNLG can generate target sentences across different languages
given the coordinate of input data in the common space. Within this unified
framework, given visual (imaging or video) data as input, ZeroNLG can perform
zero-shot visual captioning; given textual sentences as input, ZeroNLG can
perform zero-shot machine translation. We present the results of extensive
experiments on twelve NLG tasks, showing that, without using any labeled
downstream pairs for training, ZeroNLG generates high-quality and believable
outputs and significantly outperforms existing zero-shot methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We will release the codes and models at
  https://github.com/yangbang18/ZeroNLG soon. Without any labeled downstream
  pairs for training, the ZeroNLG can deal with multiple NLG tasks, including
  image-to-text, video-to-text, and text-to-text, across English, Chinese,
  German, and French within a unified framework</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parachute: Evaluating Interactive Human-LM Co-writing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Shen, Tongshuang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A surge of advances in language models (LMs) has led to significant interest
in using LMs to build co-writing systems, in which humans and LMs interactively
contribute to a shared writing artifact. However, there is a lack of studies
assessing co-writing systems in interactive settings. We propose a
human-centered evaluation framework, Parachute, for interactive co-writing
systems. Parachute showcases an integrative view of interaction evaluation,
where each evaluation aspect consists of categorized practical metrics.
Furthermore, we present Parachute with a use case to demonstrate how to
evaluate and compare co-writing systems using Parachute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CHI'23 In2Writing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stabilizing <span class="highlight-title">Transformer</span> Training by Preventing Attention Entropy
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, Josh Susskind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training stability is of great importance to Transformers. In this work, we
investigate the training dynamics of Transformers by examining the evolution of
the attention layers. In particular, we track the attention entropy for each
attention head during the course of training, which is a proxy for model
sharpness. We identify a common pattern across different architectures and
tasks, where low attention entropy is accompanied by high training instability,
which can take the form of oscillating loss or divergence. We denote the
pathologically low attention entropy, corresponding to highly concentrated
attention scores, as $\textit{entropy collapse}$. As a remedy, we propose
$\sigma$Reparam, a simple and efficient solution where we reparametrize all
linear layers with spectral normalization and an additional learned scalar. We
demonstrate that the proposed reparameterization successfully prevents entropy
collapse in the attention layers, promoting more stable training. Additionally,
we prove a tight lower bound of the attention entropy, which decreases
exponentially fast with the spectral norm of the attention logits, providing
additional motivation for our approach. We conduct experiments with
$\sigma$Reparam on image classification, image self-supervised learning,
machine translation, automatic speech recognition, and language modeling tasks,
across Transformer architectures. We show that $\sigma$Reparam provides
stability and robustness with respect to the choice of hyperparameters, going
so far as enabling training (a) a Vision Transformer to competitive performance
without warmup, weight decay, layer normalization or adaptive optimizers; (b)
deep architectures in machine translation and (c) speech recognition to
competitive performance without warmup and adaptive optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Analysis of ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongjun Jang, Thomas Lukasiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT, a question-and-answer dialogue system based on a large language
model, has gained huge popularity since its introduction. Its positive aspects
have been reported through many media platforms, and some analyses even showed
that ChatGPT achieved a decent grade in professional exams, including the law,
medical, and finance domains, adding extra support to the claim that AI now can
assist and, even, replace humans in industrial fields. Others, however, doubt
its reliability and trustworthiness. In this paper, we investigate ChatGPT's
trustworthiness regarding logically consistent behaviours. Our findings suggest
that, although ChatGPT seems to achieve an improved language understanding
ability, it still fails to generate logically correct predictions frequently.
Hence, while it is true that ChatGPT is an impressive and promising new
technique, we conclude that its usage in real-world applications without
thorough human inspection requires further consideration, especially for
risk-sensitive areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interactive UI to Support Sensemaking over Collections of Parallel
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joyce Zhou, Elena Glassman, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientists and science journalists, among others, often need to make sense of
a large number of papers and how they compare with each other in scope, focus,
findings, or any other important factors. However, with a large corpus of
papers, it's cognitively demanding to pairwise compare and contrast them all
with each other. Fully automating this review process would be infeasible,
because it often requires domain-specific knowledge, as well as understanding
what the context and motivations for the review are. While there are existing
tools to help with the process of organizing and annotating papers for
literature reviews, at the core they still rely on people to serially read
through papers and manually make sense of relevant information.
  We present AVTALER, which combines peoples' unique skills, contextual
awareness, and knowledge, together with the strength of automation. Given a set
of comparable text excerpts from a paper corpus, it supports users in
sensemaking and contrasting paper attributes by interactively aligning text
excerpts in a table so that comparable details are presented in a shared
column. AVTALER is based on a core alignment algorithm that makes use of modern
NLP tools. Furthermore, AVTALER is a mixed-initiative system: users can
interactively give the system constraints which are integrated into the
alignment construction process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbal behavior without syntactic structures: beyond Skinner and Chomsky 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shimon Edelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does it mean to know language? Since the Chomskian revolution, one
popular answer to this question has been: to possess a generative grammar that
exclusively licenses certain syntactic structures. Decades later, not even an
approximation to such a grammar, for any language, has been formulated; the
idea that grammar is universal and innately specified has proved barren; and
attempts to show how it could be learned from experience invariably come up
short. To move on from this impasse, we must rediscover the extent to which
language is like any other human behavior: dynamic, social, multimodal,
patterned, and purposive, its purpose being to promote desirable actions (or
thoughts) in others and self. Recent psychological, computational,
neurobiological, and evolutionary insights into the shaping and structure of
behavior may then point us toward a new, viable account of language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ms completed on February 4, 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatically Extracting Information in Medical Dialogue: Expert System
  And Attention for Labelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshi Wang, Daniel Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical dialogue information extraction is becoming an increasingly
significant problem in modern medical care. It is difficult to extract key
information from electronic medical records (EMRs) due to their large numbers.
Previously, researchers proposed attention-based models for retrieving features
from EMRs, but their limitations were reflected in their inability to recognize
different categories in medical dialogues. In this paper, we propose a novel
model, Expert System and Attention for Labelling (ESAL). We use mixture of
experts and pre-trained BERT to retrieve the semantics of different categories,
enabling the model to fuse the differences between them. In our experiment,
ESAL was applied to a public dataset and the experimental results indicated
that ESAL significantly improved the performance of Medical Information
Classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the impact of contextual information in hate speech detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Pérez, Franco Luque, Demian Zayat, Martín Kondratzky, Agustín Moro, Pablo Serrati, Joaquín Zajac, Paula Miguel, Natalia Debandi, Agustín Gravano, Viviana Cotik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-view Graph Neural Networks for Knowledge Graph Completion <span class="chip">ESWC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09231v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09231v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinh Tong, Dai Quoc Nguyen, Dinh Phung, Dat Quoc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an effective graph neural network (GNN)-based knowledge graph
embedding model, which we name WGE, to capture entity- and relation-focused
graph structures. Given a knowledge graph, WGE builds a single undirected
entity-focused graph that views entities as nodes. WGE also constructs another
single undirected graph from relation-focused constraints, which views entities
and relations as nodes. WGE then proposes a GNN-based architecture to better
learn vector representations of entities and relations from these two single
entity- and relation-focused graphs. WGE feeds the learned entity and relation
representations into a weighted score function to return the triple scores for
knowledge graph completion. Experimental results show that WGE outperforms
strong baselines on seven benchmark datasets for knowledge graph completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of ESWC 2023; 17 pages; 4 tables; 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Token-level Contrastive Framework for Sign Language Translation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Fu, Peigen Ye, Liang Zhang, Pei Yu, Cong Hu, Yidong Chen, Xiaodong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign Language Translation (SLT) is a promising technology to bridge the
communication gap between the deaf and the hearing people. Recently,
researchers have adopted Neural Machine Translation (NMT) methods, which
usually require large-scale corpus for training, to achieve SLT. However, the
publicly available SLT corpus is very limited, which causes the collapse of the
token representations and the inaccuracy of the generated tokens. To alleviate
this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive
learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation ,
which learns effective token representations by incorporating token-level
contrastive learning into the SLT decoding process. Concretely, ConSLT treats
each token and its counterpart generated by different dropout masks as positive
pairs during decoding, and then randomly samples $K$ tokens in the vocabulary
that are not in the current sentence to construct negative examples. We conduct
comprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both
end-to-end and cascaded settings. The experimental results demonstrate that
ConSLT can achieve better translation quality than the strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Wihan van der Heever, Ranjan Satapathy, Gianmarco Mengaldo, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach for explainability in financial analysis
by utilizing the Pearson correlation coefficient to establish a relationship
between aspect-based sentiment analysis and stock prices. The proposed
methodology involves constructing an aspect list from financial news articles
and analyzing sentiment intensity scores for each aspect. These scores are then
compared to the stock prices for the relevant companies using the Pearson
coefficient to determine any significant correlations. The results indicate
that the proposed approach provides a more detailed and accurate understanding
of the relationship between sentiment analysis and stock prices, which can be
useful for investors and financial analysts in making informed decisions.
Additionally, this methodology offers a transparent and interpretable way to
explain the sentiment analysis results and their impact on stock prices.
Overall, the findings of this paper demonstrate the importance of
explainability in financial analysis and highlight the potential benefits of
utilizing the Pearson coefficient for analyzing aspect-based sentiment analysis
and stock prices. The proposed approach offers a valuable tool for
understanding the complex relationships between financial news sentiment and
stock prices, providing a new perspective on the financial market and aiding in
making informed investment decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Named Entity Detection and Injection for Direct Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Gaido, Yun Tang, Ilia Kulikov, Rongqing Huang, Hongyu Gong, Hirofumi Inaguma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a sentence, certain words are critical for its semantic. Among them, named
entities (NEs) are notoriously challenging for neural models. Despite their
importance, their accurate handling has been neglected in speech-to-text (S2T)
translation research, and recent work has shown that S2T models perform poorly
for locations and notably person names, whose spelling is challenging unless
known in advance. In this work, we explore how to leverage dictionaries of NEs
known to likely appear in a given context to improve S2T model outputs. Our
experiments show that we can reliably detect NEs likely present in an utterance
starting from S2T encoder outputs. Indeed, we demonstrate that the current
detection quality is sufficient to improve NE accuracy in the translation with
a 31% reduction in person name errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmentation with Projection: Towards an Effective and Efficient Data
  Augmentation Paradigm for Distillation <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Yuexin Wu, Frederick Liu, Daogao Liu, Le Hou, Hongkun Yu, Jing Li, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is one of the primary methods of transferring
knowledge from large to small models. However, it requires massive
task-specific data, which may not be plausible in many real-world applications.
Data augmentation methods such as representation interpolation, token
replacement, or augmentation with models are applied to tackle this problem.
However, these data augmentation methods either potentially cause shifts in
decision boundaries (representation interpolation), are not expressive enough
(token replacement), or introduce too much computational overhead (augmentation
with models). To this end, we propose AugPro (Augmentation with Projection), an
effective and efficient data augmentation method for distillation. Our method
builds on top of representation interpolation augmentation methods to maintain
the diversity of expressions and converts the augmented data to tokens to avoid
shifting decision boundaries. It uses simple operations that come with little
computational overhead. The results on multiple GLUE tasks show that our
methods can improve distillation performance by a large margin at a low time
cost. Codes are available at
https://github.com/google-research/google-research/tree/master/augpro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures. Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Computational Power of Nondeterministic Stack RNNs <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01343v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01343v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian DuSell, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recurrent neural networks (RNNs) have a fixed, finite number of
memory cells. In theory (assuming bounded range and precision), this limits
their formal language recognition power to regular languages, and in practice,
RNNs have been shown to be unable to learn many context-free languages (CFLs).
In order to expand the class of languages RNNs recognize, prior work has
augmented RNNs with a nondeterministic stack data structure, putting them on
par with pushdown automata and increasing their language recognition power to
CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic
CFLs), but in this paper, we show that nondeterminism and the neural controller
interact to produce two more unexpected abilities. First, the nondeterministic
stack RNN can recognize not only CFLs, but also many non-context-free
languages. Second, it can recognize languages with much larger alphabet sizes
than one might expect given the size of its stack alphabet. Finally, to
increase the information capacity in the stack and allow it to solve more
complicated tasks with large alphabet sizes, we propose a new version of the
nondeterministic stack that simulates stacks of vectors rather than discrete
symbols. We demonstrate perplexity improvements with this new model on the Penn
Treebank language modeling benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures. Published at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PowerMat: context-aware recommender system without user item rating
  values that solves the cold-start problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems serves as an important technical asset in many modern
companies. With the increasing demand for higher precision of the technology,
more and more research and investment has been allocated to the field. One
important sub-field of recommender systems that has been stagnating is
context-aware recommender systems. Due to the difficulty of collecting input
dataset, the amount of research on context-aware recommender systems is much
less than other sub-fields of recommender systems. In this paper, we propose a
new algorithm named PowerMat to tackle the context-aware recommendation
problem. We build our theory on matrix factorization and Zipf's law, and also
more recent research work such as DotMat. We prove by experiments that our
method achieves superior results to the classic matrix factorization algorithm
and other context-aware recommender systems such as MovieMat+. In addition, by
theoretical analysis, we show that our algorithm solves the cold-start problem
for context-aware recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Betti Number for Point Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topology is the foundation for many industrial applications ranging from CAD
to simulation analysis. Computational topology mostly focuses on structured
data such as mesh, however unstructured dataset such as point set remains a
virgin land for topology scientists. The significance of point-based topology
can never be overemphasized, especially in the area of reverse engineering,
geometric modeling and algorithmic analysis. In this paper, we propose a novel
approach to compute the Betti number for point set data and illustrate its
usefulness in real world examples. To the best of our knowledge, our work is
pioneering and first of its kind in the fields of computational topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Retention-oriented Recommendation with Decision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, Dawei yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving user retention with reinforcement learning~(RL) has attracted
increasing attention due to its significant importance in boosting user
engagement. However, training the RL policy from scratch without hurting users'
experience is unavoidable due to the requirement of trial-and-error searches.
Furthermore, the offline methods, which aim to optimize the policy without
online interactions, suffer from the notorious stability problem in value
estimation or unbounded variance in counterfactual policy evaluation. To this
end, we propose optimizing user retention with Decision Transformer~(DT), which
avoids the offline difficulty by translating the RL as an autoregressive
problem. However, deploying the DT in recommendation is a non-trivial problem
because of the following challenges: (1) deficiency in modeling the numerical
reward value; (2) data discrepancy between the policy learning and
recommendation generation; (3) unreliable offline performance evaluation. In
this work, we, therefore, contribute a series of strategies for tackling the
exposed issues. We first articulate an efficient reward prompt by weighted
aggregation of meta embeddings for informative reward embedding. Then, we endow
a weighted contrastive learning method to solve the discrepancy between
training and inference. Furthermore, we design two robust offline metrics to
measure user retention. Finally, the significant improvement in the benchmark
datasets demonstrates the superiority of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoMLP: Automated MLP for Sequential Recommendations <span class="chip">WWW'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, Ruocheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems aim to predict users' next interested item
given their historical interactions. However, a long-standing issue is how to
distinguish between users' long/short-term interests, which may be
heterogeneous and contribute differently to the next recommendation. Existing
approaches usually set pre-defined short-term interest length by exhaustive
search or empirical experience, which is either highly inefficient or yields
subpar results. The recent advanced transformer-based models can achieve
state-of-the-art performances despite the aforementioned issue, but they have a
quadratic computational complexity to the length of the input sequence. To this
end, this paper proposes a novel sequential recommender system, AutoMLP, aiming
for better modeling users' long/short-term interests from their historical
interactions. In addition, we design an automated and adaptive search algorithm
for preferable short-term interest length via end-to-end optimization. Through
extensive experiments, we show that AutoMLP has competitive performance against
state-of-the-art methods, while maintaining linear computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'23</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multimodal Information based Speech Processing (MISP) 2022
  Challenge: Audio-Visual Diarization and Recognition <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Shilong Wu, Hang Chen, Mao-Kui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Siniscalchi, Odette Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-modal Information based Speech Processing (MISP) challenge aims to
extend the application of signal processing technology in specific scenarios by
promoting the research into wake-up words, speaker diarization, speech
recognition, and other technologies. The MISP2022 challenge has two tracks: 1)
audio-visual speaker diarization (AVSD), aiming to solve ``who spoken when''
using both audio and visual data; 2) a novel audio-visual diarization and
recognition (AVDR) task that focuses on addressing ``who spoken what when''
with audio-visual speaker diarization results. Both tracks focus on the Chinese
language, and use far-field audio and video in real home-tv scenarios: 2-6
people communicating each other with TV noise in the background. This paper
introduces the dataset, track settings, and baselines of the MISP2022
challenge. Our analyses of experiments and examples indicate the good
performance of AVDR baseline system, and the potential difficulties in this
challenge due to, e.g., the far-field video quality, the presence of TV noise
in the background, and the indistinguishable speakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, to be published in ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution (HR) images are usually downscaled to low-resolution (LR)
ones for better display and afterward upscaled back to the original size to
recover details. Recent work in image rescaling formulates downscaling and
upscaling as a unified task and learns a bijective mapping between HR and LR
via invertible networks. However, in real-world applications (e.g., social
media), most images are compressed for transmission. Lossy compression will
lead to irreversible information loss on LR images, hence damaging the inverse
upscaling procedure and degrading the reconstruction accuracy. In this paper,
we propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware
image rescaling. To tackle the distribution shift, we first develop an
end-to-end asymmetric framework with two separate bijective mappings for
high-quality and compressed LR images, respectively. Then, based on empirical
analysis of this framework, we model the distribution of the lost information
(including downscaling and compression) using isotropic Gaussian mixtures and
propose the Enhanced Invertible Block to derive high-quality/compressed LR
images in one forward pass. Besides, we design a set of losses to regularize
the learned LR images and enhance the invertibility. Extensive experiments
demonstrate the consistent improvements of SAIN across various image rescaling
datasets in terms of both quantitative and qualitative evaluation under
standard image compression formats (i.e., JPEG and WebP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023. Code is available at
  https://github.com/yang-jin-hai/SAIN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual
  Fine-Grained Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruize Xu, Ruoxuan Feng, Shi-Xiong Zhang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual learning helps to comprehensively understand the world by fusing
practical information from multiple modalities. However, recent studies show
that the imbalanced optimization of uni-modal encoders in a joint-learning
model is a bottleneck to enhancing the model's performance. We further find
that the up-to-date imbalance-mitigating methods fail on some audio-visual
fine-grained tasks, which have a higher demand for distinguishable feature
distribution. Fueled by the success of cosine loss that builds hyperspherical
feature spaces and achieves lower intra-class angular variability, this paper
proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise $L_2$
normalization to features and weights towards balanced and better multi-modal
fine-grained learning. We demonstrate that our method can alleviate the
imbalanced optimization from the perspective of weight norm and fully exploit
the discriminability of the cosine metric. Extensive experiments prove the
effectiveness of our method and the versatility with advanced multi-modal
fusion strategies and up-to-date imbalance-mitigating methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human
  Pose Estimation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxin Lin, Yunwei Chiu, Peiyuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The graph convolutional networks (GCNs) have been applied to model the
physically connected and non-local relations among human joints for 3D human
pose estimation (HPE). In addition, the purely Transformer-based models
recently show promising results in video-based 3D HPE. However, the
single-frame method still needs to model the physically connected relations
among joints because the feature representations transformed only by global
relations via the Transformer neglect information on the human skeleton. To
deal with this problem, we propose a novel method in which the Transformer
encoder and GCN blocks are alternately stacked, namely AMPose, to combine the
global and physically connected relations among joints towards HPE. In the
AMPose, the Transformer encoder is applied to connect each joint with all the
other joints, while GCNs are applied to capture information on physically
connected relations. The effectiveness of our proposed method is evaluated on
the Human3.6M dataset. Our model also shows better generalization ability by
testing on the MPI-INF-3DHP dataset. Code can be retrieved at
https://github.com/erikervalid/AMPose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2023 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging <span class="highlight-title">Pre-train</span>ed AudioLDM for Text to Sound Generation: A
  Benchmark Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yuan, Haohe Liu, Jinhua Liang, Xubo Liu, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have recently achieved breakthroughs in sound generation
with text prompts. Despite their promising performance, current text-to-sound
generation models face issues on small-scale datasets (e.g., overfitting),
significantly limiting their performance. In this paper, we investigate the use
of pre-trained AudioLDM, the state-of-the-art model for text-to-audio
generation, as the backbone for sound generation. Our study demonstrates the
advantages of using pre-trained models for text-to-sound generation, especially
in data-scarcity scenarios. In addition, experiments show that different
training strategies (e.g., training conditions) may affect the performance of
AudioLDM on datasets of different scales. To facilitate future studies, we also
evaluate various text-to-sound generation systems on several frequently used
datasets under the same evaluation protocols, which allow fair comparisons and
benchmarking of these methods on the common ground.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDMIC: Learning-based Distributed Multi-view Image Coding <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Jiawei Shao, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression plays a critical role in 3D-related
applications. Existing methods adopt a predictive coding architecture, which
requires joint encoding to compress the corresponding disparity as well as
residual information. This demands collaboration among cameras and enforces the
epipolar geometric constraint between different views, which makes it
challenging to deploy these methods in distributed camera systems with randomly
overlapping fields of view. Meanwhile, distributed source coding theory
indicates that efficient data compression of correlated sources can be achieved
by independent encoding and joint decoding, which motivates us to design a
learning-based distributed multi-view image coding (LDMIC) framework. With
independent encoders, LDMIC introduces a simple yet effective joint context
transfer module based on the cross-attention mechanism at the decoder to
effectively capture the global inter-view correlations, which is insensitive to
the geometric relationships between images. Experimental results show that
LDMIC significantly outperforms both traditional and learning-based MIC methods
while enjoying fast encoding speed. Code will be released at
https://github.com/Xinjie-Q/LDMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-10T00:00:00Z">2023-03-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajjwal Bhargava, Pooyan Amini, Shahin Shayandeh, Chinnadhurai Sankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large dialogue models become commonplace in practice, the problems
surrounding high compute requirements for training, inference and larger memory
footprint still persists. In this work, we present AUTODIAL, a multi-task
dialogue model that addresses the challenges of deploying dialogue model.
AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act
prediction, domain prediction, intent prediction, and dialogue state tracking.
Using classification decoders over generative decoders allows AUTODIAL to
significantly reduce memory footprint and achieve faster inference times
compared to existing generative approach namely SimpleTOD. We demonstrate that
AUTODIAL provides 3-6x speedups during inference while having 11x fewer
parameters on three dialogue tasks compared to SimpleTOD. Our results show that
extending current dialogue models to have parallel decoders can be a viable
alternative for deploying them in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Query Focused Summaries without Fine-tuning the
  <span class="highlight-title">Transformer</span>-based <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deen Abdullah, Shamanth Nayak, Gandharv Suri, Yllias Chali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning the Natural Language Processing (NLP) models for each new data
set requires higher computational time associated with increased carbon
footprint and cost. However, fine-tuning helps the pre-trained models adapt to
the latest data sets; what if we avoid the fine-tuning steps and attempt to
generate summaries using just the pre-trained models to reduce computational
time and cost. In this paper, we tried to omit the fine-tuning steps and
investigate whether the Marginal Maximum Relevance (MMR)-based approach can
help the pre-trained models to obtain query-focused summaries directly from a
new data set that was not used to pre-train the models. First, we used topic
modelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to
generate queries for summarization tasks. Then, using MMR, we ranked the
sentences of the documents according to the queries. Next, we passed the ranked
sentences to seven transformer-based pre-trained models to perform the
summarization tasks. Finally, we used the MMR approach again to select the
query relevant sentences from the generated summaries of individual pre-trained
models and constructed the final summary. As indicated by the experimental
results, our MMR-based approach successfully ranked and selected the most
relevant sentences as summaries and showed better performance than the
individual pre-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert
  (MoE) Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S. Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, Benjamin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models have recently gained steam in achieving the
state-of-the-art performance in a wide range of tasks in computer vision and
natural language processing. They effectively expand the model capacity while
incurring a minimal increase in computation cost during training. However,
deploying such models for inference is difficult due to their large model size
and complex communication pattern. In this work, we provide a characterization
of two MoE workloads, namely Language Modeling (LM) and Machine Translation
(MT) and identify their sources of inefficiencies at deployment.
  We propose three optimization techniques to mitigate sources of
inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert
load balancing. We show that dynamic gating improves execution time by
1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT
Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to
1.1$\times$ for MT. We further propose Expert Buffering, a new caching
mechanism that only keeps hot, active experts in GPU memory while buffering the
rest in CPU memory. This reduces static memory allocation by 1.47$\times$. We
finally propose a load balancing methodology that provides additional
robustness to the workload. The code will be open-sourced upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rewarding Chatbots for Real-World Engagement with Millions of Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, William Beauchamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of pretrained large language models has led to the deployment
of a range of social chatbots for chitchat. Although these chatbots demonstrate
language ability and fluency, they are not guaranteed to be engaging and can
struggle to retain users. This work investigates the development of social
chatbots that prioritize user engagement to enhance retention, specifically
examining the use of human feedback to efficiently develop highly engaging
chatbots. The proposed approach uses automatic pseudo-labels collected from
user interactions to train a reward model that can be used to reject
low-scoring sample responses generated by the chatbot model at inference time.
Intuitive evaluation metrics, such as mean conversation length (MCL), are
introduced as proxies to measure the level of engagement of deployed chatbots.
A/B testing on groups of 10,000 new daily chatbot users on the Chai Research
platform shows that this approach increases the MCL by up to 70%, which
translates to a more than 30% increase in user retention for a GPT-J 6B model.
Future work aims to use the reward model to realise a data fly-wheel, where the
latest user conversations can be used to alternately fine-tune the language
model and the reward model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Susceptibility to Influence of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T Mai, Maria Vau, Matthew Caldwell, Augustine Marvor-Parker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two studies tested the hypothesis that a Large Language Model (LLM) can be
used to model psychological change following exposure to influential input. The
first study tested a generic mode of influence - the Illusory Truth Effect
(ITE) - where earlier exposure to a statement (through, for example, rating its
interest) boosts a later truthfulness test rating. Data was collected from 1000
human participants using an online experiment, and 1000 simulated participants
using engineered prompts and LLM completion. 64 ratings per participant were
collected, using all exposure-test combinations of the attributes: truth,
interest, sentiment and importance. The results for human participants
reconfirmed the ITE, and demonstrated an absence of effect for attributes other
than truth, and when the same attribute is used for exposure and test. The same
pattern of effects was found for LLM-simulated participants. The second study
concerns a specific mode of influence - populist framing of news to increase
its persuasion and political mobilization. Data from LLM-simulated participants
was collected and compared to previously published data from a 15-country
experiment on 7286 human participants. Several effects previously demonstrated
from the human study were replicated by the simulated study, including effects
that surprised the authors of the human study by contradicting their
theoretical expectations (anti-immigrant framing of news decreases its
persuasion and mobilization); but some significant relationships found in human
data (modulation of the effectiveness of populist framing according to relative
deprivation of the participant) were not present in the LLM data. Together the
two studies support the view that LLMs have potential to act as models of the
effect of influence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures, 7 tables, 53 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is In-hospital Meta-information Useful for Abstractive Discharge Summary
  Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenichiro Ando, Mamoru Komachi, Takashi Okumura, Hiromasa Horiguchi, Yuji Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the patient's hospitalization, the physician must record daily
observations of the patient and summarize them into a brief document called
"discharge summary" when the patient is discharged. Automated generation of
discharge summary can greatly relieve the physicians' burden, and has been
addressed recently in the research community. Most previous studies of
discharge summary generation using the sequence-to-sequence architecture focus
on only inpatient notes for input. However, electric health records (EHR) also
have rich structured metadata (e.g., hospital, physician, disease, length of
stay, etc.) that might be useful. This paper investigates the effectiveness of
medical meta-information for summarization tasks. We obtain four types of
meta-information from the EHR systems and encode each meta-information into a
sequence-to-sequence model. Using Japanese EHRs, meta-information encoded
models increased ROUGE-1 by up to 4.45 points and BERTScore by 3.77 points over
the vanilla Longformer. Also, we found that the encoded meta-information
improves the precisions of its related terms in the outputs. Our results showed
the benefit of the use of medical meta-information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Knowledge Distillation from RNN-T Models With Noisy Training
  Labels Using Full-Sum Loss <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zeineldeen, Kartik Audhkhasi, Murali Karthick Baskar, Bhuvana Ramabhadran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies knowledge distillation (KD) and addresses its constraints
for recurrent neural network transducer (RNN-T) models. In hard distillation, a
teacher model transcribes large amounts of unlabelled speech to train a student
model. Soft distillation is another popular KD method that distills the output
logits of the teacher model. Due to the nature of RNN-T alignments, applying
soft distillation between RNN-T architectures having different posterior
distributions is challenging. In addition, bad teachers having high
word-error-rate (WER) reduce the efficacy of KD. We investigate how to
effectively distill knowledge from variable quality ASR teachers, which has not
been studied before to the best of our knowledge. We show that a sequence-level
KD, full-sum distillation, outperforms other distillation methods for RNN-T
models, especially for bad teachers. We also propose a variant of full-sum
distillation that distills the sequence discriminative knowledge of the teacher
leading to further improvement in WER. We conduct experiments on public
datasets namely SpeechStew and LibriSpeech, and on in-house production data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Ghost in the Research Shell: Large Language Models and
  Academic Knowledge Creation in Management Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper looks at the role of large language models in academic knowledge
creation based on a scoping review (2018 to January 2023) of how researchers
have previously used the language model GPT to assist in the performance of
academic knowledge creation tasks beyond data analysis. These tasks include
writing, editing, reviewing, dataset creation and curation, which have been
difficult to perform using earlier ML tools. Based on a synthesis of these
papers, this study identifies pathways for a future academic research landscape
that incorporates wider usage of large language models based on the current
modes of adoption in published articles as a Co-Writer, Research Assistant and
Respondent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creation and evaluation of timelines for longitudinal user posts <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Hills, Adam Tsakalidis, Federico Nanni, Ioannis Zachos, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest to work with user generated content in social
media, especially textual posts over time. Currently there is no consistent way
of segmenting user posts into timelines in a meaningful way that improves the
quality and cost of manual annotation. Here we propose a set of methods for
segmenting longitudinal user posts into timelines likely to contain interesting
moments of change in a user's behaviour, based on their online posting
activity. We also propose a novel framework for evaluating timelines and show
its applicability in the context of two different social media datasets.
Finally, we present a discussion of the linguistic content of highly ranked
timelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023 (main, long); camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does ChatGPT resemble humans in language use? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenguang G. Cai, David A. Haslett, Xufeng Duan, Shuqi Wang, Martin J. Pickering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have
shown remarkable capacities in comprehending and producing language. However,
their internal workings remain a black box in cognitive terms, and it is
unclear whether LLMs and chatbots can develop humanlike characteristics in
language use. Cognitive scientists have devised many experiments that probe,
and have made great progress in explaining, how people process language. We
subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000
runs per experiment. In 10 of them, ChatGPT replicated the human pattern of
language use. It associated unfamiliar words with different meanings depending
on their forms, continued to access recently encountered meanings of ambiguous
words, reused recent sentence structures, reinterpreted implausible sentences
that were likely to have been corrupted by noise, glossed over errors, drew
reasonable inferences, associated causality with different discourse entities
according to verb semantics, and accessed different meanings and retrieved
different words depending on the identity of its interlocutor. However, unlike
humans, it did not prefer using shorter words to convey less informative
content and it did not use context to disambiguate syntactic ambiguities. We
discuss how these convergences and divergences may occur in the transformer
architecture. Overall, these experiments demonstrate that LLM-driven chatbots
like ChatGPT are capable of mimicking human language processing to a great
extent, and that they have the potential to provide insights into how people
learn and use language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An algebraic approach to translating Japanese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Boboc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use Lambek's pregroups and the framework of compositional distributional
models of language ("DisCoCat") to study translations from Japanese to English
as pairs of functors. Adding decorations to pregroups we show how to handle
word order changes between languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, multiple diagrams and glosses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">Overview</span> on Language Models: Recent Developments and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Wei, Yun-Cheng Wang, Bin Wang, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner. In contrast,
pre-trained language models (PLMs) cover broader concepts and can be used in
both causal sequential modeling and fine-tuning for downstream applications.
PLMs have their own training paradigms (usually self-supervised) and serve as
foundation models in modern NLP systems. This overview paper provides an
introduction to both CLMs and PLMs from five aspects, i.e., linguistic units,
structures, training methods, evaluation methods, and applications.
Furthermore, we discuss the relationship between CLMs and PLMs and shed light
on the future directions of language modeling in the pre-trained era.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aminul Huq, Weiyi Zhang, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems based on deep neural networks are
weak against adversarial perturbations. We propose mixPGD adversarial training
method to improve the robustness of the model for ASR systems. In standard
adversarial training, adversarial samples are generated by leveraging
supervised or unsupervised methods. We merge the capabilities of both
supervised and unsupervised approaches in our method to generate new
adversarial samples which aid in improving model robustness. Extensive
experiments and comparison across various state-of-the-art defense methods and
adversarial attacks have been performed to show that mixPGD gains 4.1% WER of
better performance than previous best performing models under white-box
adversarial attack setting. We tested our proposed defense method against both
white-box and transfer based black-box attack settings to ensure that our
defense strategy is robust against various types of attacks. Empirical results
on several adversarial attacks validate the effectiveness of our proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Abuse in Financial Transaction Descriptions Using Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Leontjeva, Genevieve Richards, Kaavya Sriskandaraja, Jessica Perchman, Luiz Pizzato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since introducing changes to the New Payments Platform (NPP) to include
longer messages as payment descriptions, it has been identified that people are
now using it for communication, and in some cases, the system was being used as
a targeted form of domestic and family violence. This type of tech-assisted
abuse poses new challenges in terms of identification, actions and approaches
to rectify this behaviour. Commonwealth Bank of Australia's Artificial
Intelligence Labs team (CBA AI Labs) has developed a new system using advances
in deep learning models for natural language processing (NLP) to create a
powerful abuse detector that periodically scores all the transactions, and
identifies cases of high-risk abuse in millions of records. In this paper, we
describe the problem of tech-assisted abuse in the context of banking services,
outline the developed model and its performance, and the operating framework
more broadly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler
  and Multiple Choice Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-and-language understanding has a variety of applications in the
industry, such as video question answering, text-video retrieval and
multi-label classification. Existing video-and-language understanding methods
generally adopt heavy multi-modal encoders and feature fusion modules, which
consume large amounts of GPU memory. Especially, they have difficulty dealing
with dense video frames or long text that are prevalent in industrial
applications. In this paper, we propose MuLTI, a highly accurate and
memory-efficient video-and-language understanding model that achieves efficient
and effective feature fusion through feature sampling and attention modules.
Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we
introduce an attention-based adapter to the encoders, which finetunes the
shallow features to improve the model's performance with low GPU memory
consumption. Finally, to further improve the model's performance, we introduce
a new pretraining task named Multiple Choice Modeling to bridge the task gap
between pretraining and downstream tasks and enhance the model's ability to
align the video and the text. Benefiting from the efficient feature fusion
module, the attention-based adapter and the new pretraining task, MuLTI
achieves state-of-the-art performance on multiple datasets. Implementation and
pretrained models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence
  Reasoning <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Luo, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their similarity-based learning objectives, pretrained sentence
encoders often internalize stereotypical assumptions that reflect the social
biases that exist within their training corpora. In this paper, we describe
several kinds of stereotypes concerning different communities that are present
in popular sentence representation models, including pretrained next sentence
prediction and contrastive sentence representation models. We compare such
models to textual entailment models that learn language logic for a variety of
downstream language understanding tasks. By comparing strong pretrained models
based on text similarity with textual entailment learning, we conclude that the
explicit logic learning with textual entailment can significantly reduce bias
and improve the recognition of social communities, without an explicit
de-biasing process
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-based News Narrative Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Keith Norambuena, Tanushree Mitra, Chris North
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are fundamental to our understanding of the world, providing us
with a natural structure for knowledge representation over time. Computational
narrative extraction is a subfield of artificial intelligence that makes heavy
use of information retrieval and natural language processing techniques.
Despite the importance of computational narrative extraction, relatively little
scholarly work exists on synthesizing previous research and strategizing future
research in the area. In particular, this article focuses on extracting news
narratives from an event-centric perspective. Extracting narratives from news
data has multiple applications in understanding the evolving information
landscape. This survey presents an extensive study of research in the area of
event-based news narrative extraction. In particular, we screened over 900
articles that yielded 54 relevant articles. These articles are synthesized and
organized by representation model, extraction criteria, and evaluation
approaches. Based on the reviewed studies, we identify recent trends, open
challenges, and potential research lines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures, to be published in the journal ACM CSUR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning the Legibility of Visual Text Perturbations <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dev Seth, Rickard Stureborg, Danish Pruthi, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many adversarial attacks in NLP perturb inputs to produce visually similar
strings ('ergo' $\rightarrow$ '$\epsilon$rgo') which are legible to humans but
degrade model performance. Although preserving legibility is a necessary
condition for text perturbation, little work has been done to systematically
characterize it; instead, legibility is typically loosely enforced via
intuitions around the nature and extent of perturbations. Particularly, it is
unclear to what extent can inputs be perturbed while preserving legibility, or
how to quantify the legibility of a perturbed string. In this work, we address
this gap by learning models that predict the legibility of a perturbed string,
and rank candidate perturbations based on their legibility. To do so, we
collect and release LEGIT, a human-annotated dataset comprising the legibility
of visually perturbed text. Using this dataset, we build both text- and
vision-based models which achieve up to $0.91$ F1 score in predicting whether
an input is legible, and an accuracy of $0.86$ in predicting which of two given
perturbations is more legible. Additionally, we discover that legible
perturbations from the LEGIT dataset are more effective at lowering the
performance of NLP models than best-known attack strategies, suggesting that
current models may be vulnerable to a broad range of perturbations beyond what
is captured by existing visual attacks. Data, code, and models are available at
https://github.com/dvsth/learning-legibility-2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures. Accepted at EACL 2023 (main, long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arabic aspect sentiment polarity classification using BERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.13290v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.13290v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed M. Abdelgwad, Taysir Hassan A Soliman, Ahmed I. Taloba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic aspect sentiment polarity classification task. In
particular, we develop a simple but effective BERT-based neural baseline to
handle this task. Our BERT architecture with a simple linear classification
layer surpassed the state-of-the-art works, according to the experimental
results on three different Arabic datasets. Achieving an accuracy of 89.51% on
the Arabic hotel reviews dataset, 73% on the Human annotated book reviews
dataset, and 85.73% on the Arabic news dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPT-3-driven pedagogical agents for training children's curious
  question-asking skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14228v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14228v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to train children's ability to ask curiosity-driven questions,
previous research has explored designing specific exercises relying on
providing semantic and linguistic cues to help formulate such questions. But
despite showing pedagogical efficiency, this method is still limited as it
relies on generating the said cues by hand, which can be a very costly process.
In this context, we propose to leverage advances in the natural language
processing field (NLP) and investigate the efficiency of using a large language
model (LLM) for automating the production of the pedagogical content of a
curious question-asking (QA) training. We study generating the said content
using the "prompt-based" method that consists of explaining the task to the LLM
in natural text. We evaluate the output using human experts annotations and
comparisons with hand-generated content. Results suggested indeed the relevance
and usefulness of this content. We also conduct a field study in primary school
(75 children aged 9-10), where we evaluate children's QA performance when
having this training. We compare 3 types of content : 1) hand-generated content
that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated
content that proposes the same type of cues; 3) GPT-3-generated content that
proposes "open" cues leading to several possible questions. We see a similar QA
performance between the two "closed" trainings (showing the scalability of the
approach using GPT-3), and a better one for participants with the "open"
training. These results suggest the efficiency of using LLMs to support
children in generating more curious questions, using a natural language
prompting approach that affords usability by teachers and other users not
specialists of AI techniques. Furthermore, results also show that open-ended
content may be more suitable for training curious question-asking skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Human-Level <span class="highlight-title">Prompt</span> Engineers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the "program," optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Kind Introduction to Lexical and Grammatical Aspect, with a <span class="highlight-title">Survey</span> of
  Computational Approaches <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annemarie Friedrich, Nianwen Xue, Alexis Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspectual meaning refers to how the internal temporal structure of situations
is presented. This includes whether a situation is described as a state or as
an event, whether the situation is finished or ongoing, and whether it is
viewed as a whole or with a focus on a particular phase. This survey gives an
overview of computational approaches to modeling lexical and grammatical aspect
along with intuitive explanations of the necessary linguistic concepts and
terminology. In particular, we describe the concepts of stativity, telicity,
habituality, perfective and imperfective, as well as influential inventories of
eventuality and situation types. We argue that because aspect is a crucial
component of semantics, especially when it comes to reporting the temporal
structure of situations in a precise way, future NLP approaches need to be able
to handle and evaluate it systematically in order to achieve human-level
language understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL 2023, camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Adaptive Named Entity Recognition by Retrieving Unstructured
  Knowledge <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Naoki Yoshinaga, Kyosuke Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although named entity recognition (NER) helps us to extract domain-specific
entities from text (e.g., artists in the music domain), it is costly to create
a large amount of training data or a structured knowledge base to perform
accurate NER in the target domain. Here, we propose self-adaptive NER, which
retrieves external knowledge from unstructured text to learn the usages of
entities that have not been learned well. To retrieve useful knowledge for NER,
we design an effective two-stage model that retrieves unstructured knowledge
using uncertain entities as queries. Our model predicts the entities in the
input and then finds those of which the prediction is not confident. Then, it
retrieves knowledge by using these uncertain entities as queries and
concatenates the retrieved text to the original input to revise the prediction.
Experiments on CrossNER datasets demonstrated that our model outperforms strong
baselines by 2.35 points in F1 metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL2023 (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case
  Study using Latent Dirichlet Allocation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03029v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03029v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernadeta Griciūtė, Lifeng Han, Hao Li, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic Modelling (TM) is from the research branches of natural language
understanding (NLU) and natural language processing (NLP) that is to facilitate
insightful analysis from large documents and datasets, such as a summarisation
of main topics and the topic changes. This kind of discovery is getting more
popular in real-life applications due to its impact on big data analytics. In
this study, from the social-media and healthcare domain, we apply popular
Latent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish
newspaper articles about Coronavirus. We describe the corpus we created
including 6515 articles, methods applied, and statistics on topic changes over
approximately 1 year and two months period of time from 17th January 2020 to
13th March 2021. We hope this work can be an asset for grounding applications
of topic modelling and can be inspiring for similar case studies in an era with
pandemics, to support socio-economic impact research as well as clinical and
healthcare analytics. Our data and source code are openly available at
https://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation
(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;
BERT-topic
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fillers in Spoken Language Understanding: Computational and
  Psycholinguistic Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanvi Dinkar, Chloé Clavel, Ioana Vasilescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disfluencies (i.e. interruptions in the regular flow of speech), are
ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that
occur the most frequently compared to other kinds of disfluencies. Yet, to the
best of our knowledge, there isn't a resource that brings together the research
perspectives influencing Spoken Language Understanding (SLU) on these speech
events. This aim of this article is to survey a breadth of perspectives in a
holistic way; i.e. from considering underlying (psycho)linguistic theory, to
their annotation and consideration in Automatic Speech Recognition (ASR) and
SLU systems, to lastly, their study from a generation standpoint. This article
aims to present the perspectives in an approachable way to the SLU and
Conversational AI community, and discuss moving forward, what we believe are
the trends and challenges in each area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in TAL Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word-Graph2vec: An efficient word embedding approach on word
  co-occurrence graph using random walk sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Li, Yuanzhe Cai, Jiahong Xue, Xi Zhang, Huacan Chen, Zeyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embedding has become ubiquitous and is widely used in various text
mining and natural language processing (NLP) tasks, such as information
retrieval, semantic analysis, and machine translation, among many others.
Unfortunately, it is prohibitively expensive to train the word embedding in a
relatively large corpus. We propose a graph-based word embedding algorithm,
called Word-Graph2vec, which converts the large corpus into a word
co-occurrence graph, then takes the word sequence samples from this graph by
randomly traveling and trains the word embedding on this sampling corpus in the
end. We posit that because of the stable vocabulary, relative idioms, and fixed
expressions in English, the size and density of the word co-occurrence graph
change slightly with the increase in the training corpus. So that
Word-Graph2vec has stable runtime on the large scale data set, and its
performance advantage becomes more and more obvious with the growth of the
training corpus. Extensive experiments conducted on real-world datasets show
that the proposed algorithm outperforms traditional Skip-Gram by four-five
times in terms of efficiency, while the error generated by the random walk
sampling is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19
  Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Hassannataj Joloudari, Sadiq Hussain, Mohammad Ali Nematollahi, Rouhollah Bagheri, Fatemeh Fazl, Roohallah Alizadehsani, Reza Lashgari, Ashis Talukder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach
  for Speech Emotion Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Ye, Xin-cheng Wen, Yujie Wei, Yong Xu, Kunhong Liu, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) plays a vital role in improving the
interactions between humans and machines by inferring human emotion and
affective states from speech signals. Whereas recent works primarily focus on
mining spatiotemporal information from hand-crafted features, we explore how to
model the temporal patterns of speech emotions from dynamic temporal scales.
Towards that goal, we introduce a novel temporal emotional modeling approach
for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),
which learns multi-scale contextual affective representations from various time
scales. Specifically, TIM-Net first employs temporal-aware blocks to learn
temporal affective representation, then integrates complementary information
from the past and the future to enrich contextual representations, and finally,
fuses multiple time scale features for better adaptation to the emotional
variation. Extensive experimental results on six benchmark SER datasets
demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%
improvements of the average UAR and WAR over the second-best on each corpus.
The source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a
  Context Synergized Hyperbolic Network <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tremendous growth of social media users interacting in online
conversations has also led to significant growth in hate speech. Most of the
prior works focus on detecting explicit hate speech, which is overt and
leverages hateful phrases, with very little work focusing on detecting hate
speech that is implicit or denotes hatred through indirect or coded language.
In this paper, we present CoSyn, a user- and conversational-context synergized
network for detecting implicit hate speech in online conversation trees. CoSyn
first models the user's personal historical and social context using a novel
hyperbolic Fourier attention mechanism and hyperbolic graph convolution
network. Next, we jointly model the user's personal context and the
conversational context using a novel context interaction mechanism in the
hyperbolic space that clearly captures the interplay between the two and makes
independent assessments on the amounts of information to be retrieved from both
contexts. CoSyn performs all operations in the hyperbolic space to account for
the scale-free dynamics of social media. We demonstrate the effectiveness of
CoSyn both qualitatively and quantitatively on an open-source hate speech
dataset with Twitter conversations and show that CoSyn outperforms all our
baselines in detecting implicit hate speech with absolute improvements in the
range of 8.15% - 19.50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReAct: Synergizing Reasoning and Acting in Language Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3 is the ICLR camera ready version with some typos fixed. Project
  site with code: https://react-lm.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Coordination for Quantifying and Maximizing Knowledge
  Transference in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua Yang, Jianxin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been widely applied in online advertising and
recommender systems. To address the negative transfer issue, recent studies
have proposed optimization methods that thoroughly focus on the gradient
alignment of directions or magnitudes. However, since prior study has proven
that both general and specific knowledge exist in the limited shared capacity,
overemphasizing on gradient alignment may crowd out task-specific knowledge,
and vice versa. In this paper, we propose a transference-driven approach CoGrad
that adaptively maximizes knowledge transference via Coordinated Gradient
modification. We explicitly quantify the transference as loss reduction from
one task to another, and then derive an auxiliary gradient from optimizing it.
We perform the optimization by incorporating this gradient into original task
gradients, making the model automatically maximize inter-task transfer and
minimize individual losses. Thus, CoGrad can harmonize between general and
specific knowledge to boost overall performance. Besides, we introduce an
efficient approximation of the Hessian matrix, making CoGrad computationally
efficient and simple to implement. Both offline and online experiments verify
that CoGrad significantly outperforms previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Adversarial Learning for Complementary Item
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koby Bibas, Oren Sar Shalom, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complementary item recommendations are a ubiquitous feature of modern
e-commerce sites. Such recommendations are highly effective when they are based
on collaborative signals like co-purchase statistics. In certain online
marketplaces, however, e.g., on online auction sites, constantly new items are
added to the catalog. In such cases, complementary item recommendations are
often based on item side-information due to a lack of interaction data. In this
work, we propose a novel approach that can leverage both item side-information
and labeled complementary item pairs to generate effective complementary
recommendations for cold items, i.e., for items for which no co-purchase
statistics yet exist. Given that complementary items typically have to be of a
different category than the seed item, we technically maintain a latent space
for each item category. Simultaneously, we learn to project distributed item
representations into these category spaces to determine suitable
recommendations. The main learning process in our architecture utilizes labeled
pairs of complementary items. In addition, we adopt ideas from Cycle Generative
Adversarial Networks (CycleGAN) to leverage available item information even in
case no labeled data exists for a given item and category. Experiments on three
e-commerce datasets show that our method is highly effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pacos: Modeling Users' Interpretable and Context-Dependent Choices in
  Preference Reversals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choice problems refer to selecting the best choices from several items, and
learning users' preferences in choice problems is of great significance in
understanding the decision making mechanisms and providing personalized
services. Existing works typically assume that people evaluate items
independently. In practice, however, users' preferences depend on the market in
which items are placed, which is known as context effects; and the order of
users' preferences for two items may even be reversed, which is referred to
preference reversals. In this work, we identify three factors contributing to
context effects: users' adaptive weights, the inter-item comparison, and
display positions. We propose a context-dependent preference model named Pacos
as a unified framework for addressing three factors simultaneously, and
consider two design methods including an additive method with high
interpretability and an ANN-based method with high accuracy. We study the
conditions for preference reversals to occur and provide an theoretical proof
of the effectiveness of Pacos in addressing preference reversals. Experimental
results show that the proposed method has better performance than prior works
in predicting users' choices, and has great interpretability to help understand
the cause of preference reversals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Event-based News Narrative Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Keith Norambuena, Tanushree Mitra, Chris North
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are fundamental to our understanding of the world, providing us
with a natural structure for knowledge representation over time. Computational
narrative extraction is a subfield of artificial intelligence that makes heavy
use of information retrieval and natural language processing techniques.
Despite the importance of computational narrative extraction, relatively little
scholarly work exists on synthesizing previous research and strategizing future
research in the area. In particular, this article focuses on extracting news
narratives from an event-centric perspective. Extracting narratives from news
data has multiple applications in understanding the evolving information
landscape. This survey presents an extensive study of research in the area of
event-based news narrative extraction. In particular, we screened over 900
articles that yielded 54 relevant articles. These articles are synthesized and
organized by representation model, extraction criteria, and evaluation
approaches. Based on the reviewed studies, we identify recent trends, open
challenges, and potential research lines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures, to be published in the journal ACM CSUR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Task Recommendations with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multi-task Learning (MTL) has yielded immense success in
Recommender System (RS) applications. However, current MTL-based recommendation
models tend to disregard the session-wise patterns of user-item interactions
because they are predominantly constructed based on item-wise datasets.
Moreover, balancing multiple objectives has always been a challenge in this
field, which is typically avoided via linear estimations in existing works. To
address these issues, in this paper, we propose a Reinforcement Learning (RL)
enhanced MTL framework, namely RMTL, to combine the losses of different
recommendation tasks using dynamic weights. To be specific, the RMTL structure
can address the two aforementioned issues by (i) constructing an MTL
environment from session-wise interactions and (ii) training multi-task
actor-critic network structure, which is compatible with most existing
MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL
loss function using the weights generated by critic networks. Experiments on
two real-world public datasets demonstrate the effectiveness of RMTL with a
higher AUC against state-of-the-art MTL-based recommendation models.
Additionally, we evaluate and validate RMTL's compatibility and transferability
across various MTL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achievable Rates and Low-Complexity Encoding of Posterior Matching for
  the BSC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaael Antonini, Rita Gimelshein, Richard Wesel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Horstein, Burnashev, Shayevitz and Feder, Naghshvar et al. and others have
studied sequential transmission of a K-bit message over the binary symmetric
channel (BSC) with full, noiseless feedback using posterior matching. Yang et
al. provide an improved lower bound on the achievable rate using martingale
analysis that relies on the small-enough difference (SED) partitioning
introduced by Naghshvar et al. SED requires a relatively complex encoder and
decoder. To reduce complexity, this paper replaces SED with relaxed constraints
that admit the small enough absolute difference (SEAD) partitioning rule. The
main analytical results show that achievable-rate bounds higher than those
found by Yang et al. are possible even under the new constraints, which are
less restrictive than SED. The new analysis does not use martingale theory for
the confirmation phase and applies a surrogate channel technique to tighten the
results. An initial systematic transmission further increases the achievable
rate bound. The simplified encoder associated with SEAD has a complexity below
order O(K^2) and allows simulations for message sizes of at least 1000 bits.
For example, simulations achieve 99% of of the channel's 0.50-bit capacity with
an average block size of 200 bits for a target codeword error rate of 10^(-3).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper consists of 26 pages and contains 6 figures. An earlier
  version of the algorithm included in this paper was published at the 2020
  IEEE International Symposium on Information Theory (ISIT), (DOI:
  10.1109/ISIT44484.2020.9174232)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QVRF: A Quantization-error-aware Variable Rate Framework for Learned
  Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kedeng Tong, Yaojun Wu, Yue Li, Kai Zhang, Li Zhang, Xin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression has exhibited promising compression performance,
but variable bitrates over a wide range remain a challenge. State-of-the-art
variable rate methods compromise the loss of model performance and require
numerous additional parameters. In this paper, we present a
Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a
univariate quantization regulator a to achieve wide-range variable rates within
a single model. Specifically, QVRF defines a quantization regulator vector
coupled with predefined Lagrange multipliers to control quantization error of
all latent representation for discrete variable rates. Additionally, the
reparameterization method makes QVRF compatible with a round quantizer.
Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods
equipped with QVRF can achieve wide-range continuous variable rates within a
single model without significant performance degradation. Furthermore, QVRF
outperforms contemporary variable-rate methods in rate-distortion performance
with minimal additional parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler
  and Multiple Choice Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-and-language understanding has a variety of applications in the
industry, such as video question answering, text-video retrieval and
multi-label classification. Existing video-and-language understanding methods
generally adopt heavy multi-modal encoders and feature fusion modules, which
consume large amounts of GPU memory. Especially, they have difficulty dealing
with dense video frames or long text that are prevalent in industrial
applications. In this paper, we propose MuLTI, a highly accurate and
memory-efficient video-and-language understanding model that achieves efficient
and effective feature fusion through feature sampling and attention modules.
Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we
introduce an attention-based adapter to the encoders, which finetunes the
shallow features to improve the model's performance with low GPU memory
consumption. Finally, to further improve the model's performance, we introduce
a new pretraining task named Multiple Choice Modeling to bridge the task gap
between pretraining and downstream tasks and enhance the model's ability to
align the video and the text. Benefiting from the efficient feature fusion
module, the attention-based adapter and the new pretraining task, MuLTI
achieves state-of-the-art performance on multiple datasets. Implementation and
pretrained models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Audio-Visual Masked Autoencoder <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first extend the recent Masked Auto-Encoder (MAE) model
from a single modality to audio-visual multi-modalities. Subsequently, we
propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining
contrastive learning and masked data modeling, two major self-supervised
learning frameworks, to learn a joint and coordinated audio-visual
representation. Our experiments show that the contrastive audio-visual
correspondence learning objective not only enables the model to perform
audio-visual retrieval tasks, but also helps the model learn a better joint
representation. As a result, our fully self-supervised pretrained CAV-MAE
achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the
previous best supervised pretrained model on AudioSet in the audio-visual event
classification task. Code and pretrained models are at
https://github.com/yuangongnd/cav-mae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained
  models are at https://github.com/yuangongnd/cav-mae</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-09T00:00:00Z">2023-03-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization analysis of an unfolding network for analysis-based
  Compressed Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicky Kouni, Yannis Panagakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unfolding networks have shown promising results in the Compressed Sensing
(CS) field. Yet, the investigation of their generalization ability is still in
its infancy. In this paper, we perform generalization analysis of a
state-of-the-art ADMM-based unfolding network, which jointly learns a decoder
for CS and a sparsifying redundant analysis operator. To this end, we first
impose a structural constraint on the learnable sparsifier, which parametrizes
the network's hypothesis class. For the latter, we estimate its Rademacher
complexity. With this estimate in hand, we deliver generalization error bounds
for the examined network. Finally, the validity of our theory is assessed and
numerical comparisons to a state-of-the-art unfolding network are made, on
synthetic and real-world datasets. Our experimental results demonstrate that
our proposed framework complies with our theoretical findings and outperforms
the baseline, consistently for all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Robustness of Conversational Recommender Systems by
  Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Montazeralghaem, James Allan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRSs) are improving rapidly, according to
the standard recommendation accuracy metrics. However, it is essential to make
sure that these systems are robust in interacting with users including regular
and malicious users who want to attack the system by feeding the system
modified input data. In this paper, we propose an adversarial evaluation scheme
including four scenarios in two categories and automatically generate
adversarial examples to evaluate the robustness of these systems in the face of
different input data. By executing these adversarial examples we can compare
the ability of different conversational recommender systems to satisfy the
user's preferences. We evaluate three CRSs by the proposed adversarial examples
on two datasets. Our results show that none of these systems are robust and
reliable to the adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Projection Bias in Intertemporal Choices: A Prospect Theory
  Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Li, H. Vicky Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users often face bundle promotions when purchasing, where they have to select
between two options: buy the single item at full price, or buy the bundle at a
discount. In this scenario, users' preferences are usually influenced by the
projection bias, that is, users often believe that their future preferences are
similar to their current preferences, causing them to make irrational and
short-sighted decisions. It is of great significance to analyze the effect of
the projection bias on users' preferences, and this study may help understand
users' decision-making process and provide bundling and pricing strategies for
sellers. Prior works typically use a linear bias model for qualitative
analysis, and they cannot quantitatively calculate users' nonlinear and
personalized bias. In this work, we propose Pobe, a projection bias-embedded
preference model to accurately predict users' choices. The proposed Pobe
introduces the prospect theory to analyze users' irrational decisions, and
utilizes the weight function to handle users' nonlinear and personalized bias.
Based on the proposed Pobe, we also study the impact of items' correlations or
discount prices on users' choices, and provide four bundling strategies.
Experimental results show that the proposed method can achieve better
performance than prior works, especially when only small data is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic neutrality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milo Phillips-Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias infects the algorithms that wield increasing control over our lives.
Predictive policing systems overestimate crime in communities of color; hiring
algorithms dock qualified female candidates; and facial recognition software
struggles to recognize dark-skinned faces. Algorithmic bias has received
significant attention. Algorithmic neutrality, in contrast, has been largely
neglected. Algorithmic neutrality is my topic. I take up three questions. What
is algorithmic neutrality? Is algorithmic neutrality possible? When we have an
eye to algorithmic neutrality, what can we learn about algorithmic bias? To
answer these questions in concrete terms, I work with a case study: search
engines. Drawing on work about neutrality in science, I say that a search
engine is neutral only if certain values, like political ideologies or the
financial interests of the search engine operator, play no role in how the
search engine ranks pages. Search neutrality, I argue, is impossible. Its
impossibility seems to threaten the significance of search bias: if no search
engine is neutral, then every search engine is biased. To defuse this threat, I
distinguish two forms of bias, failing-on-its-own-terms bias and other-values
bias. This distinction allows us to make sense of search bias, and capture its
normative complexion, despite the impossibility of neutrality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Recommendation Systems with User Personality Inferred from
  Product <span class="highlight-title">Review</span>s <span class="chip">WSDM'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Lu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personality is a psychological factor that reflects people's preferences,
which in turn influences their decision-making. We hypothesize that accurate
modeling of users' personalities improves recommendation systems' performance.
However, acquiring such personality profiles is both sensitive and expensive.
We address this problem by introducing a novel method to automatically extract
personality profiles from public product review text. We then design and assess
three context-aware recommendation architectures that leverage the profiles to
test our hypothesis.
  Experiments on our two newly contributed personality datasets --
Amazon-beauty and Amazon-music -- validate our hypothesis, showing performance
boosts of 3--28%.Our analysis uncovers that varying personality types
contribute differently to recommendation performance: open and extroverted
personalities are most helpful in music recommendation, while a conscientious
personality is most helpful in beauty product recommendation. The dataset is
available at https://github.com/XinyuanLu00/IRS-WSDM2023-personality-dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IRS@WSDM'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complex QA and language models hybrid architectures, <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the state-of-the-art of hybrid language models
architectures and strategies for "complex" question-answering (QA, CQA, CPS).
Large Language Models (LLM) are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, methods,
sensitive data protection, explainability, human approval and versatile
feedback... We identify key elements augmenting LLM to solve complex questions
or problems. We extend findings from the robust community edited research
papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and
challenges of LLM in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). Recent projects like
ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential
as well as the equally strong limitations of language models in complex QA.
Hybridizing these models with different components could allow to overcome
these different limits and go much further. We discuss some challenges
associated with complex QA, including domain adaptation, decomposition and
efficient multi-step QA, long form and non-factoid QA, safety and
multi-sensitivity data protection, multimodal search, hallucinations,
explainability and truthfulness, temproal reasoning. Therefore, we analyze
current solutions and promising research trends, using elements such as: hybrid
LLM architectures, active human reinforcement learning supervised with AI,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, iterated decomposition and others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Next Basket Recommendation Reality Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Sami Jullien, Mozhdeh Ariannezhad, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of a next basket recommendation (NBR) system is to recommend items
for the next basket for a user, based on the sequence of their prior baskets.
Recently, a number of methods with complex modules have been proposed that
claim state-of-the-art performance. They rarely look into the predicted basket
and just provide intuitive reasons for the observed improvements, e.g., better
representation, capturing intentions or relations, etc. We provide a novel
angle on the evaluation of next basket recommendation methods, centered on the
distinction between repetition and exploration: the next basket is typically
composed of previously consumed items (i.e., repeat items) and new items (i.e,
explore items). We propose a set of metrics that measure the repeat/explore
ratio and performance of NBR models. Using these new metrics, we analyze
state-of-the-art NBR models. The results of our analysis help to clarify the
extent of the actual progress achieved by existing NBR methods as well as the
underlying reasons for the improvements. Overall, our work sheds light on the
evaluation problem of NBR and provides useful insights into the model design
for this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Federated Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Sun, Yonghui Xu, Yong Liu, Wei He, Lanju Kong, Fangzhao Wu, Yali Jiang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has recently been applied to recommendation systems to
protect user privacy. In federated learning settings, recommendation systems
can train recommendation models only collecting the intermediate parameters
instead of the real user data, which greatly enhances the user privacy. Beside,
federated recommendation systems enable to collaborate with other data
platforms to improve recommended model performance while meeting the regulation
and privacy constraints. However, federated recommendation systems faces many
new challenges such as privacy, security, heterogeneity and communication
costs. While significant research has been conducted in these areas, gaps in
the surveying literature still exist. In this survey, we-(1) summarize some
common privacy mechanisms used in federated recommendation systems and discuss
the advantages and limitations of each mechanism; (2) review some robust
aggregation strategies and several novel attacks against security; (3)
summarize some approaches to address heterogeneity and communication costs
problems; (4)introduce some open source platforms that can be used to build
federated recommendation systems; (5) present some prospective research
directions in the future. This survey can guide researchers and practitioners
understand the research progress in these areas.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Few-Shot Learning for Talking Face System with TTS Data
  Augmentation <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chen, Ziyang Ma, Tao Liu, Xu Tan, Qu Lu, Xie Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face has attracted broad interest from academia and
industry recently. However, data acquisition and labeling in audio-driven
talking face are labor-intensive and costly. The lack of data resource results
in poor synthesis effect. To alleviate this issue, we propose to use TTS
(Text-To-Speech) for data augmentation to improve few-shot ability of the
talking face system. The misalignment problem brought by the TTS audio is
solved with the introduction of soft-DTW, which is first adopted in the talking
face task. Moreover, features extracted by HuBERT are explored to utilize
underlying information of audio, and found to be superior over other features.
The proposed method achieves 17%, 14%, 38% dominance on MSE score, DTW score
and user study preference repectively over the baseline model, which shows the
effectiveness of improving few-shot learning for talking face system with TTS
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Image-in-Audio Deep Ste<span class="highlight-title">gan</span>ography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaume Ros Alonso, Margarita Geleta, Jordi Pons, Xavier Giro-i-Nieto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of steganography has experienced a surge of interest due to the
recent advancements in AI-powered techniques, particularly in the context of
multimodal setups that enable the concealment of signals within signals of a
different nature. The primary objectives of all steganographic methods are to
achieve perceptual transparency, robustness, and large embedding capacity -
which often present conflicting goals that classical methods have struggled to
reconcile. This paper extends and enhances an existing image-in-audio deep
steganography method by focusing on improving its robustness. The proposed
enhancements include modifications to the loss function, utilization of the
Short-Time Fourier Transform (STFT), introduction of redundancy in the encoding
process for error correction, and buffering of additional information in the
pixel subconvolution operation. The results demonstrate that our approach
outperforms the existing method in terms of robustness and perceptual
transparency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud
  Compression <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Sheng Liu, Jia-Fong Yeh, Hao Hsu, Hung-Ting Su, Ming-Sui Lee, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large amount of data collected by LiDAR sensors brings the issue of LiDAR
point cloud compression (PCC). Previous works on LiDAR PCC have used range
image representations and followed the predictive coding paradigm to create a
basic prototype of a coding framework. However, their prediction methods give
an inaccurate result due to the negligence of invalid pixels in range images
and the omission of future frames in the time step. Moreover, their handcrafted
design of residual coding methods could not fully exploit spatial redundancy.
To remedy this, we propose a coding framework BIRD-PCC. Our prediction module
is aware of the coordinates of invalid pixels in range images and takes a
bidirectional scheme. Also, we introduce a deep-learned residual coding module
that can further exploit spatial redundancy within a residual frame.
Experiments conducted on SemanticKITTI and KITTI-360 datasets show that
BIRD-PCC outperforms other methods in most bitrate conditions and generalizes
well to unseen environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-08T00:00:00Z">2023-03-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastFill: Efficient Compatible Model Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Jaeckle, Fartash Faghri, Ali Farhadi, Oncel Tuzel, Hadi Pouransari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many retrieval systems the original high dimensional data (e.g., images)
is mapped to a lower dimensional feature through a learned embedding model. The
task of retrieving the most similar data from a gallery set to a given query
data is performed through a similarity comparison on features. When the
embedding model is updated, it might produce features that are not
comparable/compatible with features already in the gallery computed with the
old model. Subsequently, all features in the gallery need to be re-computed
using the new embedding model -- a computationally expensive process called
backfilling. Recently, compatible representation learning methods have been
proposed to avoid backfilling. Despite their relative success, there is an
inherent trade-off between the new model performance and its compatibility with
the old model. In this work, we introduce FastFill: a compatible model update
process using feature alignment and policy based partial backfilling to
promptly elevate retrieval performance. We show that previous backfilling
strategies suffer from decreased performance and demonstrate the importance of
both the training objective and the ordering in online partial backfilling. We
propose a new training method for feature alignment between old and new
embedding models using uncertainty estimation. Compared to previous works, we
obtain significantly improved backfilling results on a variety of datasets: mAP
on ImageNet (+4.4\%), Places-365 (+2.7\%), and VGG-Face2 (+1.3\%). Further, we
demonstrate that when updating a biased model with FastFill, the minority
subgroup accuracy gap promptly vanishes with a small fraction of partial
backfilling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in The Eleventh International Conference on Learning
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span> Log Analysis of Text-to-Image Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xie, Zhaoying Pan, Jinge Ma, Jie Luo, Qiaozhu Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in diffusion models have unleashed the astonishing
capabilities of text-to-image generation systems to synthesize high-quality
images that are faithful to a given reference text, known as a "prompt." These
systems, once released to the public, have immediately received tons of
attention from researchers, creators, and common users. Despite the plenty of
efforts to improve the underneath generative models, there is limited work on
understanding the information needs of the real users of these systems, e.g.,
by investigating the prompts the users input at scale. In this paper, we take
the initiative to conduct a comprehensive analysis of large-scale prompt logs
collected from multiple text-to-image generation systems. Our work is analogous
to analyzing the query log of Web search engines, a line of work that has made
critical contributions to the glory of the Web search industry and research. We
analyze over two million user-input prompts submitted to three popular
text-to-image systems at scale. Compared to Web search queries, text-to-image
prompts are significantly longer, often organized into unique structures, and
present different categories of information needs. Users tend to make more
edits within creation sessions, showing remarkable exploratory patterns. Our
findings provide concrete implications on how to improve text-to-image
generation systems for creation purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel-CF: Collaborative filtering done right with social network
  analysis and kernel smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is the simplest but oldest machine learning algorithm
in the field of recommender systems. In spite of its long history, it remains a
discussion topic in research venues. Usually people use users/items whose
similarity scores with the target customer greater than 0 to compute the
algorithms. However, this might not be the optimal solution after careful
scrutiny. In this paper, we transform the recommender system input data into a
2-D social network, and apply kernel smoothing to compute preferences for
unknown values in the user item rating matrix. We unifies the theoretical
framework of recommender system and non-parametric statistics and provides an
algorithmic procedure with optimal parameter selection method to achieve the
goal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class Cardinality Comparison as a Fermi Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrestha Ghosh, Simon Razniewski, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questions on class cardinality comparisons are quite tricky to answer and
come with its own challenges. They require some kind of reasoning since web
documents and knowledge bases, indispensable sources of information, rarely
store direct answers to questions, such as, ``Are there more astronauts or
Physics Nobel Laureates?'' We tackle questions on class cardinality comparison
by tapping into three sources for absolute cardinalities as well as the
cardinalities of orthogonal subgroups of the classes. We propose novel
techniques for aggregating signals with partial coverage for more reliable
estimates and evaluate them on a dataset of 4005 class pairs, achieving an
accuracy of 83.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Web Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Learning to Rank with Biased Continuous Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Hongyan Tang, Siwen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a well-known challenge to learn an unbiased ranker with biased
feedback. Unbiased learning-to-rank(LTR) algorithms, which are verified to
model the relative relevance accurately based on noisy feedback, are appealing
candidates and have already been applied in many applications with single
categorical labels, such as user click signals. Nevertheless, the existing
unbiased LTR methods cannot properly handle continuous feedback, which are
essential for many industrial applications, such as content recommender
systems.
  To provide personalized high-quality recommendation results, recommender
systems need model both categorical and continuous biased feedback, such as
click and dwell time. Accordingly, we design a novel unbiased LTR algorithm to
tackle the challenges, which innovatively models position bias in the pairwise
fashion and introduces the pairwise trust bias to separate the position bias,
trust bias, and user relevance explicitly and can work for both continuous and
categorical feedback. Experiment results on public benchmark datasets and
internal live traffic of a large-scale recommender system at Tencent News show
superior results for continuous labels and also competitive performance for
categorical labels of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. arXiv admin note: substantial text overlap with
  arXiv:2111.12929</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Context Pattern Generation for Entity Set Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li, Ruiyang Liu, Yunbo Cao, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various Natural
Language Processing (NLP) and Information Retrieval (IR) downstream
applications have benefited from ESE due to its ability to discover knowledge.
Although existing corpus-based ESE methods have achieved great progress, they
still rely on corpora with high-quality entity information annotated, because
most of them need to obtain the context patterns through the position of the
entity in a sentence. Therefore, the quality of the given corpora and their
entity annotation has become the bottleneck that limits the performance of such
methods. To overcome this dilemma and make the ESE models free from the
dependence on entity annotation, our work aims to explore a new ESE paradigm,
namely corpus-independent ESE. Specifically, we devise a context pattern
generation module that utilizes autoregressive language models (e.g., GPT-2) to
automatically generate high-quality context patterns for entities. In addition,
we propose the GAPA, a novel ESE framework that leverages the aforementioned
GenerAted PAtterns to expand target entities. Extensive experiments and
detailed analyses on three widely used datasets demonstrate the effectiveness
of our method. All the codes of our experiments are available at
https://github.com/geekjuruo/GAPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLCC: A General Framework for Graph-Level Clustering <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11879v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11879v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ju, Yiyang Gu, Binqi Chen, Gongbo Sun, Yifang Qin, Xingyuming Liu, Xiao Luo, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of graph-level clustering, which is a novel
yet challenging task. This problem is critical in a variety of real-world
applications such as protein clustering and genome analysis in bioinformatics.
Recent years have witnessed the success of deep clustering coupled with graph
neural networks (GNNs). However, existing methods focus on clustering among
nodes given a single graph, while exploring clustering on multiple graphs is
still under-explored. In this paper, we propose a general graph-level
clustering framework named Graph-Level Contrastive Clustering (GLCC) given
multiple graphs. Specifically, GLCC first constructs an adaptive affinity graph
to explore instance- and cluster-level contrastive learning (CL).
Instance-level CL leverages graph Laplacian based contrastive loss to learn
clustering-friendly representations while cluster-level CL captures
discriminative cluster representations incorporating neighbor information of
each sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the
optimization of representation learning. The two steps can be alternatively
trained to collaborate and benefit each other. Experiments on a range of
well-known datasets demonstrate the superiority of our proposed GLCC over
competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RETEXO: Scalable Neural Network Training over Distributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashish Kolluri, Sarthak Choudhary, Bryan Hooi, Prateek Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks offer a promising approach to supervised learning over
graph data. Graph data, especially when it is privacy-sensitive or too large to
train on centrally, is often stored partitioned across disparate processing
units (clients) which want to minimize the communication costs during
collaborative training. The fully-distributed setup takes such partitioning to
its extreme, wherein features of only a single node and its adjacent edges are
kept locally with one client processor. Existing GNNs are not architected for
training in such setups and incur prohibitive costs therein. We propose RETEXO,
a novel transformation of existing GNNs that improves the communication
efficiency during training in the fully-distributed setup. We experimentally
confirm that RETEXO offers up to 6 orders of magnitude better communication
efficiency even when training shallow GNNs, with a minimal trade-off in
accuracy for supervised node classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Line Graph Contrastive Learning for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehua Zhang, Shilin Sun, Guixiang Ma, Caiming Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction tasks focus on predicting possible future connections. Most
existing researches measure the likelihood of links by different similarity
scores on node pairs and predict links between nodes. However, the
similarity-based approaches have some challenges in information loss on nodes
and generalization ability on similarity indexes. To address the above issues,
we propose a Line Graph Contrastive Learning(LGCL) method to obtain rich
information with multiple perspectives. LGCL obtains a subgraph view by h-hop
subgraph sampling with target node pairs. After transforming the sampled
subgraph into a line graph, the link prediction task is converted into a node
classification task, which graph convolution progress can learn edge embeddings
from graphs more effectively. Then we design a novel cross-scale contrastive
learning framework on the line graph and the subgraph to maximize the mutual
information of them, so that fuses the structure and feature information. The
experimental results demonstrate that the proposed LGCL outperforms the
state-of-the-art methods and has better performance on generalization and
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-MEC Cooperation Based VR Video Transmission and Cache using
  K-Shortest Paths Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwen Xia, Luyao Chen, Yong Tang, Ting Yang, Wenyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent network architectures, multi-MEC cooperative caching has been
introduced to reduce the transmission latency of VR videos, in which MEC
servers' computing and caching capability are utilized to optimize the
transmission process. However, many solutions that use the computing capability
of MEC servers ignore the additional arithmetic power consumed by the codec
process, thus making them infeasible. Besides, the minimum cache unit is
usually the entire VR video, which makes caching inefficient.
  To address these challenges, we split VR videos into tile files for caching
based on the current popular network architecture and provide a reliable
transmission mechanism and an effective caching strategy. Since the number of
different tile files N is too large, the current cooperative caching algorithms
do not cope with such large-scale input data. We further analyze the problem
and propose an optimized k-shortest paths (OKSP) algorithm with an upper bound
time complexity of O((K * M + N) * M * logN)), and suitable for shortest paths
with restricted number of edges, where K is the total number of tiles that all
M MEC servers can cache in the collaboration domain. And we prove the OKSP
algorithm can compute the caching scheme with the lowest average latency in any
case, which means the solution given is the exact solution. The simulation
results show that the OKSP algorithm has excellent speed for solving
large-scale data and consistently outperforms other caching algorithms in the
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaDM: Codec-aware <span class="highlight-title">Diffusion</span> Modeling for Neural-enhanced Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihua Zhou, Ruibin Li, Song Guo, Peiran Dong, Yi Liu, Jingcai Guo, Zhenda Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the dramatic growth of Internet video traffic,
where the video bitstreams are often compressed and delivered in low quality to
fit the streamer's uplink bandwidth. To alleviate the quality degradation, it
comes the rise of Neural-enhanced Video Streaming (NVS), which shows great
prospects for recovering low-quality videos by mostly deploying neural
super-resolution (SR) on the media server. Despite its benefit, we reveal that
current mainstream works with SR enhancement have not achieved the desired
rate-distortion trade-off between bitrate saving and quality restoration, due
to: (1) overemphasizing the enhancement on the decoder side while omitting the
co-design of encoder, (2) limited generative capacity to recover high-fidelity
perceptual details, and (3) optimizing the compression-and-restoration pipeline
from the resolution perspective solely, without considering color bit-depth.
Aiming at overcoming these limitations, we are the first to conduct an
encoder-decoder (i.e., codec) synergy by leveraging the inherent
visual-generative property of diffusion models. Specifically, we present the
Codec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly
reduce streaming delivery bitrates while holding pretty higher restoration
capacity over existing methods. First, CaDM improves the encoder's compression
efficiency by simultaneously reducing resolution and color bit-depth of video
frames. Second, CaDM empowers the decoder with high-quality enhancement by
making the denoising diffusion restoration aware of encoder's resolution-color
conditions. Evaluation on public cloud services with OpenMMLab benchmarks shows
that CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common
video standards and achieves much better recovery quality (e.g., FID of 0.61)
over state-of-the-art neural-enhancing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Question Answering Using CLIP-Guided Visual-Text Attention <span class="chip">ICIP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Ye, Weikai Kong, Chenglin Yao, Jianfeng Ren, Xudong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal learning of video and text plays a key role in Video Question
Answering (VideoQA). In this paper, we propose a visual-text attention
mechanism to utilize the Contrastive Language-Image Pre-training (CLIP) trained
on lots of general domain language-image pairs to guide the cross-modal
learning for VideoQA. Specifically, we first extract video features using a
TimeSformer and text features using a BERT from the target application domain,
and utilize CLIP to extract a pair of visual-text features from the
general-knowledge domain through the domain-specific learning. We then propose
a Cross-domain Learning to extract the attention information between visual and
linguistic features across the target domain and general domain. The set of
CLIP-guided visual-text features are integrated to predict the answer. The
proposed method is evaluated on MSVD-QA and MSRVTT-QA datasets, and outperforms
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2023 IEEE International Conference on Image
  Processing (ICIP 2023)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-03-07T00:00:00Z">2023-03-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering large 3D volumes: A sampling-based approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Lang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications of X-ray computed tomography, an unsupervised
segmentation of the reconstructed 3D volumes forms an important step in the
image processing chain for further investigation of the digitized object.
Therefore, the goal is to train a clustering algorithm on the volume, which
produces a voxelwise classification by assigning a cluster index to each voxel.
However, clustering methods, e.g., K-Means, typically have an asymptotic
polynomial runtime with respect to the dataset size, and thus, these techniques
are rarely applicable to large volumes. In this work, we introduce a novel
clustering technique based on random sampling, which allows for the voxelwise
classification of arbitrarily large volumes. The presented method conducts
efficient linear passes over the data to extract a representative random sample
of a fixed size on which the classifier can be trained. Then, a final linear
pass performs the segmentation and assigns a cluster index to each individual
voxel. Quantitative and qualitative evaluations show that excellent results can
be achieved even with a very small sample size. Consequently, the unsupervised
segmentation by means of clustering becomes feasible for arbitrarily large
volumes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Summarizing Evidence from Clinical Trials: A Prototype
  Highlighting Current Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjana Ramprasad, Denis Jered McInerney, Iain J. Marshal, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TrialsSummarizer, a system that aims to automatically summarize
evidence presented in the set of randomized controlled trials most relevant to
a given query. Building on prior work, the system retrieves trial publications
matching a query specifying a combination of condition, intervention(s), and
outcome(s), and ranks these according to sample size and estimated study
quality. The top-k such studies are passed through a neural multi-document
summarization system, yielding a synopsis of these trials. We consider two
architectures: A standard sequence-to-sequence model based on BART and a
multi-headed architecture intended to provide greater transparency to
end-users. Both models produce fluent and relevant summaries of evidence
retrieved for queries, but their tendency to introduce unsupported statements
render them inappropriate for use in this domain at present. The proposed
architecture may help users verify outputs allowing users to trace generated
tokens back to inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Privacy Preserving System for Movie Recommendations using Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Neumann, Andreas Lutz, Karsten Müller, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are employed by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Accordingly, we present a complete recommender system for movie
recommendations, which provides privacy and thus trustworthiness on two levels:
First, it is trained using federated learning and thus is, by its very nature,
privacy-preserving, while still enabling individual users to benefit from
global insights. And second, a novel federated learning scheme, FedQ, is
employed, which not only addresses the problem of non-i.i.d. and small local
datasets, but also prevents input data reconstruction attacks by aggregating
client models early. To reduce the communication overhead, compression is
applied, which significantly reduces the exchanged neural network updates to a
fraction of their original data. We conjecture that it may also improve data
privacy through its lossy quantization stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the ACM TORS Special Issue on Trustworthy Recommender
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Remote Sensing Image Retrieval and Classification with Robust
  Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13392v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13392v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitri Gominski, Valérie Gouet-Brunet, Liming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high resolution remote sensing image analysis are currently
hampered by the difficulty of gathering enough annotated data for training deep
learning methods, giving rise to a variety of small datasets and associated
dataset-specific methods. Moreover, typical tasks such as classification and
retrieval lack a systematic evaluation on standard benchmarks and training
datasets, which make it hard to identify durable and generalizable scientific
contributions. We aim at unifying remote sensing image retrieval and
classification with a new large-scale training and testing dataset, SF300,
including both vertical and oblique aerial images and made available to the
research community, and an associated fine-tuning method. We additionally
propose a new adversarial fine-tuning method for global descriptors. We show
that our framework systematically achieves a boost of retrieval and
classification performance on nine different datasets compared to an ImageNet
pretrained baseline, with currently no other method to compare to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Performance margin with the proposed method is not statistically
  significant. Please refer to http://alegoria.ign.fr/en/SF300_dataset if you
  are interested in the dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LambdaKG: A Library for <span class="highlight-title">Pre-train</span>ed Language Model-Based Knowledge Graph
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xie, Zhoubo Li, Xiaohan Wang, Yuqi Zhu, Ningyu Zhang, Jintian Zhang, Siyuan Cheng, Bozhong Tian, Shumin Deng, Feiyu Xiong, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress and the project website is
  https://zjunlp.github.io/project/promptkg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, Shirui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale e-commercial platforms in the real-world usually contain various
recommendation scenarios (domains) to meet demands of diverse customer groups.
Multi-Domain Recommendation (MDR), which aims to jointly improve
recommendations on all domains and easily scales to thousands of domains, has
attracted increasing attention from practitioners and researchers. Existing MDR
methods usually employ a shared structure and several specific components to
respectively leverage reusable features and domain-specific information.
However, data distribution differs across domains, making it challenging to
develop a general model that can be applied to all circumstances. Additionally,
during training, shared parameters often suffer from the domain conflict while
specific parameters are inclined to overfitting on data sparsity domains. we
first present a scalable MDR platform served in Taobao that enables to provide
services for thousands of domains without specialists involved. To address the
problems of MDR methods, we propose a novel model agnostic learning framework,
namely MAMDR, for the multi-domain recommendation. Specifically, we first
propose a Domain Negotiation (DN) strategy to alleviate the conflict between
domains. Then, we develop a Domain Regularization (DR) to improve the
generalizability of specific parameters by learning from other domains. We
integrate these components into a unified framework and present MAMDR, which
can be applied to any model structure to perform multi-domain recommendation.
Finally, we present a large-scale implementation of MAMDR in the Taobao
application and construct various public MDR benchmark datasets which can be
used for following studies. Extensive experiments on both benchmark datasets
and industry datasets demonstrate the effectiveness and generalizability of
MAMDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICDE 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FSVVD: A Dataset of Full Scene Volumetric Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyuan Hu, Yili Jin, Haowen Yang, Junhua Liu, Fangxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a rapid development of immersive multimedia which
bridges the gap between the real world and virtual space. Volumetric videos, as
an emerging representative 3D video paradigm that empowers extended reality,
stand out to provide unprecedented immersive and interactive video watching
experience. Despite the tremendous potential, the research towards 3D
volumetric video is still in its infancy, relying on sufficient and complete
datasets for further exploration. However, existing related volumetric video
datasets mostly only include a single object, lacking details about the scene
and the interaction between them. In this paper, we focus on the current most
widely used data format, point cloud, and for the first time release a
full-scene volumetric video dataset that includes multiple people and their
daily activities interacting with the external environments. Comprehensive
dataset description and analysis are conducted, with potential usage of this
dataset. The dataset and additional tools can be accessed via the following
website: https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MMSys'23 Open Dataset and Software Track, A preliminary
  version. The dataset and additional tools can be accessed via
  https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approach to Learning Generalized Audio Representation Through Batch
  Embedding Covariance Regularization and Constant-Q Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, Bhiksha Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose embedding is highly desirable for few-shot even zero-shot
learning in many application scenarios, including audio tasks. In order to
understand representations better, we conducted a thorough error analysis and
visualization of HEAR 2021 submission results. Inspired by the analysis, this
work experiments with different front-end audio preprocessing methods,
including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT),
and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover
a more holistic simulation of the frequency information received by the human
auditory system. We tested the models on the suite of HEAR 2021 tasks, which
encompass a broad category of tasks. Preliminary results show (1) the proposed
BECR can incur a more dispersed embedding on the test set, (2) BECR improves
the PaSST model without extra computation complexity, and (3) STFT
preprocessing outperforms CQT in all tasks we tested.
Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compose & Embellish: Well-Structured Piano Performance Generation via A
  Two-Stage Approach <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08212v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08212v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Lun Wu, Yi-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even with strong sequence models like Transformers, generating expressive
piano performances with long-range musical structures remains challenging.
Meanwhile, methods to compose well-structured melodies or lead sheets (melody +
chords), i.e., simpler forms of music, gained more success. Observing the
above, we devise a two-stage Transformer-based framework that Composes a lead
sheet first, and then Embellishes it with accompaniment and expressive touches.
Such a factorization also enables pretraining on non-piano data. Our objective
and subjective experiments show that Compose & Embellish shrinks the gap in
structureness between a current state of the art and real performances by half,
and improves other musical aspects such as richness and coherence as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Predictive Analytics in Reversible Ste<span class="highlight-title">gan</span>ography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang, Xu Wang, Sisheng Chen, Isao Echizen, Victor Sanchez, Chang-Tsun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is regarded as a promising solution for reversible
steganography. There is an accelerating trend of representing a reversible
steo-system by monolithic neural networks, which bypass intermediate operations
in traditional pipelines of reversible steganography. This end-to-end paradigm,
however, suffers from imperfect reversibility. By contrast, the modular
paradigm that incorporates neural networks into modules of traditional
pipelines can stably guarantee reversibility with mathematical explainability.
Prediction-error modulation is a well-established reversible steganography
pipeline for digital images. It consists of a predictive analytics module and a
reversible coding module. Given that reversibility is governed independently by
the coding module, we narrow our focus to the incorporation of neural networks
into the analytics module, which serves the purpose of predicting pixel
intensities and a pivotal role in determining capacity and imperceptibility.
The objective of this study is to evaluate the impacts of different training
configurations upon predictive accuracy of neural networks and provide
practical insights. In particular, we investigate how different initialisation
strategies for input images may affect the learning process and how different
training strategies for dual-layer prediction respond to the problem of
distributional shift. Furthermore, we compare steganographic performance of
various model architectures with different loss functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Neural Networks for Reversible Ste<span class="highlight-title">gan</span>ography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have led to a paradigm shift in the field of
reversible steganography. A fundamental pillar of reversible steganography is
predictive modelling which can be realised via deep neural networks. However,
non-trivial errors exist in inferences about some out-of-distribution and noisy
data. In view of this issue, we propose to consider uncertainty in predictive
models based upon a theoretical framework of Bayesian deep learning, thereby
creating an adaptive steganographic system. Most modern deep-learning models
are regarded as deterministic because they only offer predictions while failing
to provide uncertainty measurement. Bayesian neural networks bring a
probabilistic perspective to deep learning and can be regarded as self-aware
intelligent machinery; that is, a machine that knows its own limitations. To
quantify uncertainty, we apply Bayesian statistics to model the predictive
distribution and approximate it through Monte Carlo sampling with stochastic
forward passes. We further show that predictive uncertainty can be disentangled
into aleatoric and epistemic uncertainties and these quantities can be learnt
unsupervised. Experimental results demonstrate an improvement delivered by
Bayesian uncertainty analysis upon steganographic rate-distortion performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the predictability in reversible ste<span class="highlight-title">gan</span>ography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Chun Chang, Xu Wang, Sisheng Chen, Hitoshi Kiya, Isao Echizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks have advanced the frontiers of reversible
steganography. The core strength of neural networks is the ability to render
accurate predictions for a bewildering variety of data. Residual modulation is
recognised as the most advanced reversible steganographic algorithm for digital
images. The pivot of this algorithm is predictive analytics in which pixel
intensities are predicted given some pixel-wise contextual information. This
task can be perceived as a low-level vision problem and hence neural networks
for addressing a similar class of problems can be deployed. On top of the prior
art, this paper investigates predictability of pixel intensities based on
supervised and unsupervised learning frameworks. Predictability analysis
enables adaptive data embedding, which in turn leads to a better trade-off
between capacity and imperceptibility. While conventional methods estimate
predictability by the statistics of local image patterns, learning-based
frameworks consider further the degree to which correct predictions can be made
by a designated predictor. Not only should the image patterns be taken into
account but also the predictor in use. Experimental results show that
steganographic performance can be significantly improved by incorporating the
learning-based predictability analysers into a reversible steganographic
system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Video Quality Assessment on User Generated Contents from
  Aesthetic and Technical Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in user-generated-content (UGC) videos calls for the
development of effective video quality assessment (VQA) algorithms. However,
the objective of the UGC-VQA problem is still ambiguous and can be viewed from
two perspectives: the technical perspective, measuring the perception of
distortions; and the aesthetic perspective, which relates to preference and
recommendation on contents. To understand how these two perspectives affect
overall subjective opinions in UGC-VQA, we conduct a large-scale subjective
study to collect human quality opinions on overall quality of videos as well as
perceptions from aesthetic and technical perspectives. The collected
Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality
opinions on UGC videos are universally and inevitably affected by both
aesthetic and technical perspectives. In light of this, we propose the
Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of
UGC videos based on the two perspectives. The DOVER proves state-of-the-art
performance in UGC-VQA under very high efficiency. With perspective opinions in
DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable
clear-cut quality evaluations from a single aesthetic or technical perspective.
Code at https://github.com/VQAssessment/DOVER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action-GPT: Leveraging Large-scale Language Models for Improved and
  Generalized Action Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Action-GPT, a plug-and-play framework for incorporating Large
Language Models (LLMs) into text-based action generation models. Action phrases
in current motion capture datasets contain minimal and to-the-point
information. By carefully crafting prompts for LLMs, we generate richer and
fine-grained descriptions of the action. We show that utilizing these detailed
descriptions instead of the original action phrases leads to better alignment
of text and motion spaces. We introduce a generic approach compatible with
stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion
models. In addition, the approach enables multiple text descriptions to be
utilized. Our experiments show (i) noticeable qualitative and quantitative
improvement in the quality of synthesized motions, (ii) benefits of utilizing
multiple LLM-generated descriptions, (iii) suitability of the prompt function,
and (iv) zero-shot generation capabilities of the proposed approach. Project
page: https://actiongpt.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, pretrained models and sample videos will be made available at
  \url{https://actiongpt.github.io}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-based Event-centric Online Video Question Answering on a
  Newly Constructed ATBS Dataset <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikai Kong, Shuhong Ye, Chenglin Yao, Jianfeng Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks facilitate video question answering (VideoQA), but the
real-world applications on video streams such as CCTV and live cast place
higher demands on the solver. To address the challenges of VideoQA on long
videos of unknown length, we define a new set of problems called Online
Open-ended Video Question Answering (O^2VQA). It requires an online
state-updating mechanism for the solver to decide if the collected information
is sufficient to conclude an answer. We then propose a Confidence-based
Event-centric Online Video Question Answering (CEO-VQA) model to solve this
problem. Furthermore, a dataset called Answer Target in Background Stream
(ATBS) is constructed to evaluate this newly developed online VideoQA
application. Compared to the baseline VideoQA method that watches the whole
video, the experimental results show that the proposed method achieves a
significant performance gain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2023 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-frequency Network for Robust Speaker Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiguo Li, Tianzi Zhang, Xiaobin Liu, Lirong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wide deployment of speech-based biometric systems usually demands
high-performance speaker recognition algorithms. However, most of the prior
works for speaker recognition either process the speech in the frequency domain
or time domain, which may produce suboptimal results because both time and
frequency domains are important for speaker recognition. In this paper, we
attempt to analyze the speech signal in both time and frequency domains and
propose the time-frequency network~(TFN) for speaker recognition by extracting
and fusing the features in the two domains. Based on the recent advance of deep
neural networks, we propose a convolution neural network to encode the raw
speech waveform and the frequency spectrum into domain-specific features, which
are then fused and transformed into a classification feature space for speaker
recognition. Experimental results on the publicly available datasets TIMIT and
LibriSpeech show that our framework is effective to combine the information in
the two domains and performs better than the state-of-the-art methods for
speaker recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-03-15T05:25:51.606131264Z">
            2023-03-15 05:25:51 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
